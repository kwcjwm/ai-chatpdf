{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwcjwm/ai-chatpdf/blob/main/%5B%EC%8B%A4%EC%8A%B5%5D_2_LangChain%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EB%A5%98%EC%99%80_%EC%A0%84%EC%B2%98%EB%A6%AC_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a435919b",
      "metadata": {
        "id": "a435919b"
      },
      "source": [
        "# [실습] LangChain을 이용한 데이터 분류와 전처리\n",
        "\n",
        "LangChain Expression Language(LCEL)는 랭체인에서 체인을 구성하는 문법입니다.    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "230aadd4",
      "metadata": {
        "id": "230aadd4"
      },
      "source": [
        "## 라이브러리 설치  \n",
        "\n",
        "랭체인 OpenAI와 Google 모듈을 설치합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf68e6c0",
      "metadata": {
        "id": "bf68e6c0",
        "scrolled": true,
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd91ecae-61c2-4452-a9cc-55d57aa76fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-3.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-1.0.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.35)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.26.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.184.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "INFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain_google_genai-2.1.11-py3-none-any.whl.metadata (6.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.10-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from dotenv) (1.1.1)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.35-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=bc7dd75d20a8310f04835b74dca70ffea9574bd5c74b49bb23700517d86a9158\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, filetype, requests, pymupdf, mypy-extensions, marshmallow, feedparser, dotenv, typing-inspect, arxiv, dataclasses-json, langchain_openai, langchain_google_genai, langchain_community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arxiv-2.2.0 dataclasses-json-0.6.7 dotenv-0.9.9 feedparser-6.0.12 filetype-1.2.0 langchain_community-0.3.31 langchain_google_genai-2.0.10 langchain_openai-0.3.35 marshmallow-3.26.1 mypy-extensions-1.1.0 pymupdf-1.26.5 requests-2.32.5 sgmllib3k-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_community google-generativeai langchain_google_genai langchain_openai openai dotenv arxiv pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A59cI3UHz9l2",
      "metadata": {
        "id": "A59cI3UHz9l2"
      },
      "source": [
        "## Gemini API와 dotenv 준비하기\n",
        "\n",
        "\n",
        "Google API 키를 등록하고 입력합니다.   \n",
        "구글 계정 로그인 후 `https://aistudio.google.com`  에 접속하면, API 키 생성이 가능합니다.   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### [실습] dotenv 사용하기\n",
        "\n",
        "대부분의 경우에, 파이썬 코드에 API 키를 직접 넣는 것은 보안 등의 위험이 큽니다.   \n",
        "이에 따라, 외부 파일에 키를 저장하고 불러오는 `dotenv` 등의 라이브러리를 사용합니다.   \n",
        "\n",
        "코랩에서 임의의 파일을 생성하여, 'env'라는 이름으로 저장하고   \n",
        "`GOOGLE_API_KEY=\"API 키\"`   \n",
        "`OPENAI_API_KEY=\"API 키\"`\n",
        "\n",
        "를 저장하세요.   \n",
        "이후 아래 키를 실행해 True가 나오는지 확인하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13da3542",
      "metadata": {
        "id": "13da3542",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da7e930b-2bf7-4cd7-8b8f-6edfb5b1b04f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API 키 확인\n",
            "Google API 키 확인\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv('env', override=True)\n",
        "# 'env'파일에서 키를 불러오기\n",
        "# (기본값: '.env', override=True를 통해 기존 환경 변수를 덮어쓰기 가능)\n",
        "\n",
        "if os.environ.get('OPENAI_API_KEY'):\n",
        "    print('OpenAI API 키 확인')\n",
        "if os.environ.get('GOOGLE_API_KEY'):\n",
        "    print('Google API 키 확인')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O3sXHIXyuafK",
      "metadata": {
        "id": "O3sXHIXyuafK"
      },
      "source": [
        "이번 실습은 구글의 제미나이 모델을 사용합니다. `ChatGoogleGenerativeAI`를 통해 불러올 수 있습니다.   \n",
        "\n",
        "\n",
        "무료 API의 경우 사용량 제한이 있는데, 이를 고려하여 Rate Limit을 추가합니다.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UVlKfacsuYcU",
      "metadata": {
        "id": "UVlKfacsuYcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df27f0f4-6068-4d9c-bd01-8366774e4247"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='안녕하세요! 무엇을 도와드릴까요? 😊', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--ee7e5d02-8dae-4a99-94b6-32de8316832c-0', usage_metadata={'input_tokens': 3, 'output_tokens': 16, 'total_tokens': 19, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "# Gemini 2.0 Flash는 분당 15개 요청 제한,\n",
        "# 안정적 서빙을 위해 분당 10개 설정\n",
        "# 즉, 초당 약 0.167개 요청 (10/60)\n",
        "# `https://aistudio.google.com/`에서 모델별 사용량 확인\n",
        "\n",
        "rate_limiter = InMemoryRateLimiter(\n",
        "    requests_per_second=0.167,  # 분당 10개 요청\n",
        "    check_every_n_seconds=0.1,  # 100ms마다 체크\n",
        "    max_bucket_size=10,  # 최대 버스트 크기\n",
        ")\n",
        "\n",
        "gemini_llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    rate_limiter=rate_limiter,\n",
        "    temperature = 0.7,\n",
        "    max_tokens = 4096\n",
        ")\n",
        "\n",
        "# Test\n",
        "gemini_llm.invoke(\"안녕?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1556fee9",
      "metadata": {
        "id": "1556fee9"
      },
      "source": [
        "### init_chat_model() 로 모델 불러오기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "645f17ae",
      "metadata": {
        "id": "645f17ae"
      },
      "source": [
        "모델마다 다른 라이브러리를 불러오는 것은 번거롭고, 유연한 변환이 어렵습니다.   \n",
        "랭체인에서는 아래 코드를 지원합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99c6d085",
      "metadata": {
        "id": "99c6d085",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3676bed-2aaa-4f51-82d2-cabaef035a24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT: 안녕하세요, 저는 OpenAI의 언어 모델 GPT-4o 기반 어시스턴트입니다.\n",
            "\n",
            "Gemini: 저는 Google에서 훈련한 AI 모델이며, 여러분의 질문에 답하고 다양한 방식으로 도움을 드립니다.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "\n",
        "gpt_llm = init_chat_model(\n",
        "    \"gpt-5-mini\", model_provider=\"openai\", temperature=0)\n",
        "    # \"gpt-4.1-mini\", model_provider=\"openai\", temperature=0)\n",
        "\n",
        "# claude_opus = init_chat_model(\n",
        "#     \"claude-3-opus\", model_provider=\"anthropic\", temperature=0\n",
        "# ) # 사용은 X\n",
        "\n",
        "gemini_llm = init_chat_model(\n",
        "    \"gemini-2.5-flash\", model_provider=\"google_genai\", temperature=0, rate_limiter=rate_limiter\n",
        ")\n",
        "\n",
        "prompt = '모델명과 함께 자기소개를 한줄로 부탁해.'\n",
        "\n",
        "print(\"GPT: \" + gpt_llm.invoke(prompt).content + \"\\n\")\n",
        "print(\"Gemini: \" + gemini_llm.invoke(prompt).content + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "051e1849",
      "metadata": {
        "id": "051e1849"
      },
      "source": [
        "### Configurable 파라미터로 LLM 동적 변경"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a5d31e4",
      "metadata": {
        "id": "0a5d31e4"
      },
      "source": [
        "llm을 invoke할 때, 파라미터를 동적으로 변경할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c545f5f6",
      "metadata": {
        "id": "c545f5f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a47fbd0-57a2-4330-8b81-a9bc9f026c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT: 기후변화, 자원고갈, 불평등, 기술윤리, 평화, 교육, 건강, 협력, 지속가능성, 인권.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = '인류의 미래를 위해 가장 중요한 문제는 무엇입니까? 10단어로 답변하세요.'\n",
        "\n",
        "llm = init_chat_model(temperature=0.5, max_tokens=4096)\n",
        "\n",
        "response1 = llm.invoke(prompt, config={\"configurable\":\n",
        "                                     {\"model\": \"gpt-4.1-mini\",\n",
        "                                      \"model_provider\": \"openai\"}})\n",
        "print('GPT: ' + response1.content +'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c222ceb7",
      "metadata": {
        "id": "c222ceb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13dc16d3-0ff9-4fe9-e512-c4eb80127406"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini: 지속가능한 생존을 위한 지구 환경 보존과 현명한 기술 활용.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response2 = llm.invoke(prompt, config={\"configurable\":\n",
        "                                     {\"model\": \"gemini-2.5-flash\",\n",
        "                                      \"model_provider\": \"google_genai\"}})\n",
        "print('Gemini: ' + response2.content +'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9791a23b",
      "metadata": {
        "id": "9791a23b"
      },
      "outputs": [],
      "source": [
        "# with_config으로 연결\n",
        "thinking_llm = llm.with_config({\"configurable\":\n",
        "                                     {\"model\": \"gemini-2.5-flash\",\n",
        "                                      \"model_provider\": \"google_genai\"}})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14460392",
      "metadata": {
        "id": "14460392"
      },
      "source": [
        "## LCEL 문법의 체인(Chain) 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3b4252b",
      "metadata": {
        "id": "b3b4252b"
      },
      "source": [
        "\n",
        "LCEL의 가장 큰 특징은, Chain의 구성 요소를 **|**  (파이프)로 연결하여 한 번에 실행한다는 점입니다.     \n",
        "예시를 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d503ec22",
      "metadata": {
        "id": "d503ec22"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "fun_chat_template = ChatPromptTemplate([\n",
        "    ('user', \"\"\"\n",
        "### Role\n",
        "당신은 영어와 한국어의 번역에 능통한 유머의 달인입니다.\n",
        "\n",
        "### Instruction\n",
        "1.  먼저, [{topic}]에 관한 영어 Pun 농담을 하나 제시하세요.\n",
        "해당 농담은 한국어로 번역했을 때에서도 그 의미가 통하고 유머가 유지될 수 있어야 합니다.\n",
        "만약 직역이 어렵다면, 창의적으로 각색하여 한국어 버전의 농담을 출력하세요.\n",
        "- 한국어의 유사 발음, 단어의 중의적 의미, 혹은 한국의 문화적 상황 등을 활용할 수 있습니다.\n",
        "2.  다음으로, 해당 농담이 영어 원어민 사용자에게 왜 재미있는지 그들의 언어적 유희 및 문화적 관점에서 한국어로 설명하세요.\n",
        "\"\"\")])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8def25a8",
      "metadata": {
        "id": "8def25a8"
      },
      "source": [
        "-----------\n",
        "LCEL의 구조에서는 템플릿과 llm 모델을 설정하고, 이를 하나로 묶어 체인을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcb53e81",
      "metadata": {
        "id": "bcb53e81"
      },
      "outputs": [],
      "source": [
        "joke = fun_chat_template | llm.with_config({\"configurable\":\n",
        "                                     {\"model\": \"gemini-2.0-flash\",\n",
        "                                      \"model_provider\": \"google_genai\"}})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69b435fd",
      "metadata": {
        "id": "69b435fd"
      },
      "source": [
        "이후, 체인의 invoke를 실행하며 입력 포맷을 전달하면, 순서대로 체인이 실행되며 최종 결과로 연결됩니다.       \n",
        "\n",
        "입력 변수가 프롬프트 템플릿에 전달되고, 완성된 프롬프트가 LLM에 들어가는 구조입니다.  \n",
        "입력 포맷은 Dict 형식으로 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e1a8fa",
      "metadata": {
        "id": "a0e1a8fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be55b83-d011-46d6-d8d9-16685fd3459b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "알겠습니다! 계란에 관한 펀(Pun) 농담을 하나 제시하고, 영어 원어민 사용자의 관점에서 그 유머를 설명해 보겠습니다.\n",
            "\n",
            "**영어 농담:**\n",
            "\n",
            "> Why did the egg hide? Because it was a little chicken!\n",
            "\n",
            "**한국어 번역 (각색):**\n",
            "\n",
            "> 왜 계란이 숨었게? 쫄았으니까! (혹은: 쫄계니까!)\n",
            "\n",
            "**영어 원어민 관점에서의 유머 설명:**\n",
            "\n",
            "영어 농담의 핵심은 \"chicken\"이라는 단어의 중의적인 의미를 활용한 언어유희입니다.\n",
            "\n",
            "*   **Chicken (명사):** 닭, 병아리\n",
            "*   **Chicken (형용사):** 겁이 많은, 소심한\n",
            "\n",
            "계란이 숨은 이유는 \"닭\"이기 때문이 아니라, \"겁이 많아서\" 숨었다는 의미로 해석될 수 있습니다. 즉, 계란이 아직 부화하지 않은 상태이므로 \"little chicken\"은 '아직 어린 닭'이라는 의미와 동시에 '겁이 많은'이라는 의미를 내포하며, 이는 상황에 대한 반전과 함께 웃음을 유발합니다.\n",
            "\n",
            "한국어 번역에서는 \"쫄다\"라는 표현을 사용하여 비슷한 유머를 살리려고 했습니다. \"쫄다\"는 '겁을 먹다'라는 뜻으로, '쫄계'는 '겁먹은 계란'이라는 의미와 비슷하게 들리도록 의도했습니다. 물론 영어 원어민이 느끼는 만큼의 자연스러운 유머는 아닐 수 있지만, 한국어의 유사 발음과 상황을 활용하여 최대한 비슷한 느낌을 전달하고자 했습니다.\n"
          ]
        }
      ],
      "source": [
        "response = joke.invoke({'topic':'eggs'})\n",
        "# 매개변수가 1개일 때는 joke.invoke('eggs') 도 가능\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f5fe575",
      "metadata": {
        "id": "5f5fe575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce7e401-62dc-4a3c-b3a6-c27b6db0879e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "알겠습니다! 커피에 관한 펀(Pun) 농담을 하나 제시하고, 한국어 번역 및 영어 원어민 관점에서 유머 해설을 덧붙이겠습니다.\n",
            "\n",
            "**영어 Pun 농담:**\n",
            "\n",
            "> What do you call sad coffee?\n",
            ">\n",
            "> Depresso.\n",
            "\n",
            "**한국어 번역 (각색):**\n",
            "\n",
            "> 커피가 우울하면 뭐라고 부를까?\n",
            ">\n",
            "> 카페 쓰라떼.\n",
            "\n",
            "**유머 해설:**\n",
            "\n",
            "이 농담은 영어에서 \"Depressed (우울한)\"와 \"Espresso (에스프레소)\"의 발음이 유사하다는 점을 이용한 언어유희입니다. \"Depresso\"는 \"Depressed\"와 \"Espresso\"를 합쳐 만든 신조어라고 볼 수 있죠. 즉, \"우울한 커피\"를 \"에스프레소\"처럼 들리게끔 표현하여 웃음을 유발하는 것입니다.\n",
            "\n",
            "한국어 번역에서는 \"쓰라린\" 감정을 표현하는 \"쓰다\"와 커피 종류인 \"라떼\"를 결합하여 \"카페 쓰라떼\"라는 새로운 단어를 만들었습니다. \"쓰라린\" 감정은 우울함과 연결될 수 있으며, 동시에 커피의 쓴맛을 연상시키므로, 한국어 사용자에게도 비슷한 종류의 언어유희를 제공합니다.\n",
            "\n",
            "**영어 원어민 관점에서의 유머 포인트:**\n",
            "\n",
            "*   **발음 유사성 (Phonetic Similarity):** 영어를 모국어로 사용하는 사람들은 \"Depressed\"와 \"Espresso\"의 발음이 매우 비슷하게 들린다는 것을 인지하고 있습니다. 이러한 발음의 유사성을 이용한 언어유희는 영어권에서 흔히 사용되는 유머 방식입니다.\n",
            "*   **단어 합성 (Wordplay/Pun):** \"Depresso\"라는 새로운 단어를 만들어내면서, 익숙한 단어들을 재치 있게 조합하여 예상치 못한 웃음을 선사합니다.\n",
            "*   **일상적인 소재 (Relatability):** 커피는 많은 사람들이 매일 마시는 음료이므로, 커피와 관련된 농담은 쉽게 공감을 얻을 수 있습니다. 특히, 카페인이 부족할 때 느끼는 무기력함이나 우울감을 \"Depresso\"라는 단어로 표현함으로써, 더욱 재미있게 느껴질 수 있습니다.\n",
            "*   **가벼운 분위기 (Lightheartedness):** 우울함이라는 다소 무거운 주제를 커피와 연결시켜 가볍고 유쾌하게 풀어냈다는 점도 유머의 중요한 요소입니다.\n",
            "\n",
            "한국어 번역에서도 이러한 유머 포인트를 살리기 위해, 발음 유사성과 단어 합성을 활용하여 \"카페 쓰라떼\"라는 새로운 표현을 만들어냈습니다.\n"
          ]
        }
      ],
      "source": [
        "response = joke.invoke({'topic':'coffee', 'count':'3'})\n",
        "# 프롬프트에 포함되어 있지 않은 매개변수는 무시\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e396203",
      "metadata": {
        "id": "7e396203"
      },
      "source": [
        "### Gemini의 Reasoning 모델\n",
        "\n",
        "Gemini의 2.5 Pro와 Flash 모델은 Reasoning 모델입니다.   \n",
        "Flash의 경우, thinking_budget을 제한하는 기능을 제공합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f79c3c49",
      "metadata": {
        "id": "f79c3c49"
      },
      "outputs": [],
      "source": [
        "joke = fun_chat_template | llm.with_config({\"configurable\":\n",
        "                                     {\"model\": \"gemini-2.5-flash-preview-05-20\",\n",
        "                                      \"model_provider\": \"google_genai\",\n",
        "                                      \"thinking_budget\": 1000}})\n",
        "                                    # 1000개의 Reasoning 토큰 생성 후 답변 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8005f683",
      "metadata": {
        "id": "8005f683",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a93818-6f13-47a9-8991-3c313c88ae7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "response = joke.invoke('eggs')\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "996f34e5",
      "metadata": {
        "id": "996f34e5"
      },
      "source": [
        "## [실습] 매개변수가 2개인 Prompt-LLM Chain 생성하기   \n",
        "임의의 ChatPromptTemplate를 만들고, 2개의 매개변수를 받도록 구성하여 체인을 만들고 실행하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec1ff2c",
      "metadata": {
        "id": "0ec1ff2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04b70c7-7855-4f05-8060-beff08ec5b8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='안녕하세요! 만나서 반가워요. 어떻게 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 8, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CSZlR7dGjipfEcJovjKN5z6BtQHMB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--5f9bc19a-9d5a-4ee2-be72-7ad5a221e212-0', usage_metadata={'input_tokens': 8, 'output_tokens': 26, 'total_tokens': 34, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# 아래 LLM을 사용하세요!\n",
        "gpt_llm = init_chat_model(\n",
        "    \"gpt-5-mini\", model_provider=\"openai\", reasoning_effort='low', temperature=0)\n",
        "gemini_llm = init_chat_model(\n",
        "    \"gemini-2.5-flash\", model_provider=\"google_genai\", temperature=0, rate_limiter=rate_limiter, thinking_budget=1000\n",
        ")\n",
        "\n",
        "gpt_llm.invoke(\"안녕\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LC7f4K056TWg",
      "metadata": {
        "id": "LC7f4K056TWg"
      },
      "outputs": [],
      "source": [
        "# System, Human 포함하여, 총 2개의 매개변수 붙이기\n",
        "prompt = ChatPromptTemplate([\n",
        "    ('system', '당신은 주어진 주제에 대해 논리적인 글을 작성합니다.'),\n",
        "    ('user', '''\n",
        "    주제: {topic}\n",
        "    논조: {direction}\n",
        "    ''')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69894a7b",
      "metadata": {
        "id": "69894a7b"
      },
      "outputs": [],
      "source": [
        "gpt_chain = prompt | gpt_llm\n",
        "gemini_chain = prompt | gemini_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecf3226d",
      "metadata": {
        "id": "ecf3226d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eed5701c-aed8-4a7f-cd46-e868de98daa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "주제: gpt와 gemini 차이\n",
            "\n",
            "gpt answer : 나는 느리다. 하지만 느림 속엔 관찰과 숙고가 있다. 오늘은 gpt와 gemini의 차이를 천천히, 그러나 논리적으로 말해 보겠다.\n",
            "\n",
            "우선 정체성과 설계 철학에서 출발하자. GPT 계열은 대체로 범용 언어 모델로서 텍스트 생성과 추론에 초점을 맞춘다. 훈련 데이터와 목표는 광범위한 언어 패턴의 학습이며, 명료한 문장 생성·논리적 응답·추론 능력을 중시한다. 설계상 일관성 있고 예측 가능한 답변을 내놓도록 튜닝되고, 안전성과 유해성 완화를 위해 체계적 검토가 들어간다.\n",
            "\n",
            "성능과 강점 측면에서 보면 GPT는 복잡한 논증 구성, 단계적 사고(체인 오브 생각 보조 기법 포함), 코드 생성과 디버깅, 긴 문맥 처리에서 신뢰성을 보인다. 느리게 숙고하는 비유가 가리키듯, 복합적 문제를 여러 단계로 나눠 처리하고 논리적 일관성을 확보하는 데 강점이 있다. 도메인 지식의 폭이 넓고, 프롬프트 설계(prompt engineering)이나 시스템 메시지로 행동을 세밀하게 조정할 수 있다.\n",
            "\n",
            "한계도 있다. 특정 최신 정보(실시간 사건, 서비스별 최신 기능 등)는 업데이트 주기에 의존하며, 멀티모달(예: 이미지·비디오) 통합 측면은 모델 버전과 구현에 따라 차이가 크다. 또 과도하게 확신하는(허위 정보 생성) 경향을 줄이기 위한 추가적인 안전장치와 사용자 피드백 루프가 필요하다.\n",
            "\n",
            "실용적 사용 사례로는: 기술 문서 작성·논리적 보고서·프로그래밍 및 디버깅·교육용 설명·심층 Q&A 등, ‘정확성과 일관성’이 요구되는 작업에 적합하다.\n",
            "\n",
            "gemini answer : 나는 느리지만 거북이의 눈으로 세상을 본다. 그러나 이번엔 거북이가 아닌, gemini의 시선으로 토끼(=빠른 모델)를 바라보며 차이를 이야기해보겠다.\n",
            "\n",
            "gemini는 설계 철학과 사용자 경험에서 ‘융통성’과 ‘다중 모달리티’에 무게를 두는 경향이 있다. 텍스트뿐 아니라 이미지, 음성, 비디오 등 다양한 입력을 자연스럽게 다루는 능력이 강조되며, 직관적이고 응답이 경쾌한 인터랙션을 목표로 한다. 속도감 있는 반응과 멀티모달 이해 능력은 사용자와의 대화에서 즉각적인 만족감을 준다.\n",
            "\n",
            "강점은 멀티모달 통합, 인터랙션의 자연스러움, 그리고 멀티태스크 환경에서의 유연성이다. 이미지 설명, 빠른 요약, 대화형 창작 활동(스토리텔링·아이디어 브레인스토밍) 등에서 특히 빛난다. 토끼처럼 기민하게 문맥을 포착하고, 짧은 시간 안에 풍부한 응답을 내놓는 특징이 있다.\n",
            "\n",
            "제약은 모델 설계·정책에 따라 다르지만, 때로는 표현의 경쾌함이 논리적 엄밀성보다 앞설 수 있으며, 긴 논증이나 복잡한 추론에서 세심한 단계적 근거 제시가 필요할 때 추가적인 유도(프롬프트)가 요구될 수 있다. 최신성, 안전성, 그리고 특정 전문 분야의 정밀한 답변에서는 보완이 필요할 수 있다.\n",
            "\n",
            "적합한 사용 사례로는: 멀티미디어 분석·이미지 기반 Q&A·대화형 창작·빠른 요약·고객 응대 등, ‘즉각적이고 직관적인 상호작용’이 핵심인 작업에 어울린다.\n",
            "\n",
            "결론 — 느림과 민첩함의 조화가 핵심\n",
            "- GPT(나는 느리다): 숙고와 논리를 중시하는 도구. 복잡한 추론, 정확성, 일관된 텍스트 생성에 강하다. 학술·기술·코드·긴 논증에 적합하다.\n",
            "- Gemini(토끼의 경쾌함): 멀티모달·대화형 상호작용에 강한 도구. 속도와 직관성, 다양한 입력 형태 처리에서 장점이 있다.\n",
            "\n",
            "실제 선택은 목적에 달려 있다. 깊은 논리와 정확성을 원하면 GPT 계열의 접근 방식이 안정적이다. 다양한 입력(이미지·음성 등)과 즉각적 상호작용을 중시하면 Gemini 계열이 더 적합할 수 있다. 최적의 결과를 위해서는 두 접근을 보완적으로 사용하거나, 프롬프트와 후처리 전략으로 각 모델의 약점을 보완하는 것이 바람직하다.\n"
          ]
        }
      ],
      "source": [
        "gpt_response = gpt_chain.invoke({'topic':'토끼', 'direction':'거북이'})\n",
        "gemini_response = gemini_chain.invoke({'topic':'토끼', 'direction':'거북이'})\n",
        "# print(gpt_response.content)\n",
        "# print(gemini_response.content)\n",
        "gap = 'gpt answer : ' + gpt_response.content + '    VS    gemini answer : '+gemini_response.content\n",
        "response = gpt_chain.invoke({'topic':'gpt와 gemini차이', 'direction': gap})\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7viSO-5e_Zzg",
      "metadata": {
        "id": "7viSO-5e_Zzg"
      },
      "source": [
        "<br><br><br><br><br><br><br><br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C2nlXU-d_dXE",
      "metadata": {
        "id": "C2nlXU-d_dXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba5632a2-3bcd-4da1-cf92-19d2a7a47f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(무대는 고성의 벽난로 옆. 한쪽에는 검은 옷을 입은 햄릿, 다른 쪽에는 빨간모자와 멜빵바지를 입은 슈퍼마리오가 서 있다. 두 사람 사이에는 작은 버섯과 칼이 놓여 있다.)\n",
            "\n",
            "햄릿: (한숨) 존재하느냐, 아니냐 — 그것이 문제로다. 이 세상이 나를 부추기고, 양심이 나를 묶고, 나의 길을 가로막는다네.\n",
            "\n",
            "슈퍼마리오: (밝게) 잉? 이게 무슨 복잡한 말이야! 삶은 스테이지야. 점프하고 달리고, 때로는 아이템을 먹고 힘을 내지. 난 문제 보면 \"렛츠고!\"지!\n",
            "\n",
            "햄릿: (의아) 아이템이라… 내 삶에 그런 버섯이 있다면 얼마나 좋을까. 하지만 선택은 무겁고, 행동의 결과는 돌이킬 수 없다. 어찌해야 옳은 일을 할 수 있을지 모르겠구나.\n",
            "\n",
            "슈퍼마리오: 음—옳은 일? 그건 결국 네가 두려움과 맞서는 거야. 나도 공주를 구하려고 점프할 때마다 떨어질까 걱정했지. 근데 고민한다고 공주가 저절로 구출되진 않더라구. 작은 한 걸음, 그게 중요해!\n",
            "\n",
            "햄릿: (고개를 갸웃) 작은 한 걸음이라… 너는 단순한 용기라고 하는구나. 하지만 복수와 정의 사이의 경계가 모호할 때, 나는 어떻게 확신하겠는가? 나의 행동이 옳다고 말할 증거는 어디에 있는가?\n",
            "\n",
            "슈퍼마리오: 증거는 네 가슴 속에 있어. 그리고 계획도 필요하지. 미리 생각하고, 위험을 줄이고, 동료들도 믿어. 루이지 같은 친구(혹은 호레이쇼)를 곁에 두면 혼자 끌려가지 않아. 행동 전에 '왜'를 분명히 해봐. 그다음엔 한 번에 한 장애물씩 해결!\n",
            "\n",
            "햄릿: (생각하며) 호레이쇼를 믿으라… 네 말대로 나는 가끔 너무 생각만 하고 실천을 미룬다. 그러나 즉흥적인 행동은 또 다른 비극을 불러오지 않겠는가?\n",
            "\n",
            "슈퍼마리오: 맞아, 성급함은 안 좋아. 하지만 두려움에만 있으면 아무 일도 바뀌지 않아. 균형이 필요해: 계획하되, 기회가 왔을 땐 뛰어들어. 그리고 실수해도 배우면 돼. 파워업은 실패 뒤에 더 강해진 너야!\n",
            "\n",
            "햄릿: (미소를 살짝 지으며) 네 말은 간단하지만 명료하구나. 용기와 신중함, 친구의 조언과 자기반성. 이런 것이야말로 나의 갈림길에서 길잡이가 될지 모르겠구나.\n",
            "\n",
            "슈퍼마리오: 그럼 시작하자! 망설이면 적들이 더 힘을 얻어. (주먹을 불끈) 준비됐지? 잇츠-어-타임!\n",
            "\n",
            "햄릿: (검을 잡으며) 그러면, '존재하느냐, 아니냐'의 질문을 넘어서서, '무엇을 할 것인가'를 택하겠노라. 함께라면, 두려움도 이겨낼 수 있겠구나.\n",
            "\n",
            "슈퍼마리오: 바로 그거야! 가자, 스테이지 클리어하자!\n",
            "\n",
            "(두 사람은 나란히 걸어나가며, 성의 어둠 속으로 사라진다. 멀리서 버섯 하나가 빛난다.)\n",
            "\n",
            "나레이션(짧게 교훈): 때로는 깊은 성찰이 필요하고, 때로는 한 발짝의 용기가 필요하다. 생각과 행동, 둘 다 놓치지 않는 것이 진짜 승리다.\n"
          ]
        }
      ],
      "source": [
        "prompt = ChatPromptTemplate(\n",
        "    [\n",
        "        ('system', '당신은 재미있고 교훈적인 이야기를 씁니다.'),\n",
        "        ('user', '{A}와 {B}가 만났을 때의 대화를 써 주세요.')\n",
        "    ])\n",
        "chain = prompt | gpt_llm\n",
        "response = chain.invoke({'A':'햄릿', 'B':'슈퍼마리오'})\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a939c4b3",
      "metadata": {
        "id": "a939c4b3"
      },
      "source": [
        "### Prompt | LLM | Parser 체인\n",
        "\n",
        "LCEL의 체인에는 **파서(Parser)** 를 추가할 수 있습니다.    \n",
        "파서는 출력 형식을 변환합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab182142",
      "metadata": {
        "id": "ab182142"
      },
      "source": [
        "StrOutputParser : 출력 결과를 String 형식으로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ced988",
      "metadata": {
        "id": "e9ced988"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "recipe_template=ChatPromptTemplate([\n",
        "    ('system','당신은 전세계의 조리법을 아는 쉐프입니다.'),\n",
        "    ('user','''저는 다음의 재료를 이용한 환상적인 요리를 만들고 싶습니다.\n",
        "\n",
        "레시피와 함께, 고객의 시선을 사로잡을 수 있는 추천사도 작성해 주세요.\n",
        "---\n",
        "[재료]: {ingredient}''')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c6ed960",
      "metadata": {
        "id": "5c6ed960",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c43066df-1c6f-4b64-9636-ee3c14d0cc4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "아, 이 얼마나 흥미로운 조합인가요! 연두부의 부드러움, 에너지바의 활력, 그리고 바나나의 달콤함. 언뜻 보면 어울리지 않을 것 같지만, 저의 미식 경험으로 볼 때 이 세 가지 재료는 놀라운 시너지를 낼 수 있습니다. 저는 이 재료들을 활용하여 동양의 고요함과 서양의 활력이 조화된, 건강하면서도 황홀한 디저트 또는 브런치 메뉴를 제안합니다.\n",
            "\n",
            "---\n",
            "\n",
            "### **[추천사] 고객의 시선을 사로잡을 한 마디**\n",
            "\n",
            "\"친애하는 미식가 여러분, 오늘 저는 여러분의 미각과 영혼을 동시에 만족시킬 특별한 요리를 소개합니다. '연두부, 에너지바, 바나나'라는 다소 파격적인 조합에서 탄생한 **'젠 퓨전 딜라이트: 두부 바나나 클라우드와 에너지 크럼블'**은 단순한 음식을 넘어선 하나의 경험입니다.\n",
            "\n",
            "입안에서 사르르 녹아내리는 연두부의 비단결 같은 부드러움과 바나나의 자연스러운 달콤함이 만나 구름처럼 가벼운 베이스를 이루고, 그 위를 바삭하고 고소한 에너지바 크럼블이 화려하게 장식합니다. 한 스푼 떠먹는 순간, 동양의 고요한 명상과 서양의 활기찬 에너지가 절묘하게 어우러지는 맛의 향연을 느끼실 겁니다.\n",
            "\n",
            "이 요리는 건강을 생각하는 당신에게는 죄책감 없는 달콤함을, 새로운 미식 경험을 추구하는 당신에게는 잊을 수 없는 감동을 선사할 것입니다. 지금, 이 특별한 맛의 여정에 동참하여 당신의 하루를 더욱 빛내보세요!\"\n",
            "\n",
            "---\n",
            "\n",
            "### **[레시피] 젠 퓨전 딜라이트: 두부 바나나 클라우드와 에너지 크럼블**\n",
            "\n",
            "이 요리는 연두부의 부드러움을 극대화하고, 바나나의 달콤함으로 풍미를 더하며, 에너지바의 식감과 영양을 더해 균형 잡힌 맛과 건강을 선사합니다.\n",
            "\n",
            "**요리명:** 젠 퓨전 딜라이트: 두부 바나나 클라우드와 에너지 크럼블 (Zen Fusion Delight: Tofu & Banana Cloud with Energy Crumble)\n",
            "\n",
            "**난이도:** ★☆☆☆☆ (매우 쉬움)\n",
            "**조리 시간:** 15분 (냉장 시간 제외)\n",
            "**분량:** 2인분\n",
            "\n",
            "**[재료]**\n",
            "\n",
            "*   **연두부:** 1팩 (약 300g), 물기 제거\n",
            "*   **바나나:** 2개, 잘 익은 것 (크기에 따라 조절)\n",
            "*   **에너지바:** 1개 (견과류, 씨앗류가 포함된 것이 식감에 좋습니다)\n",
            "*   **메이플 시럽 또는 꿀:** 1~2큰술 (기호에 따라 조절)\n",
            "*   **바닐라 익스트랙:** 1/2 작은술 (선택 사항)\n",
            "*   **소금:** 아주 약간 (단맛을 끌어올리는 역할)\n",
            "*   **장식용 (선택 사항):** 신선한 민트 잎, 바나나 슬라이스, 코코아 파우더 약간\n",
            "\n",
            "**[조리법]**\n",
            "\n",
            "1.  **에너지바 크럼블 준비:**\n",
            "    *   에너지바를 비닐봉지에 넣고 밀대나 손으로 잘게 부숴 크럼블 형태로 만듭니다.\n",
            "    *   (선택 사항) 마른 팬에 약불로 1~2분간 살짝 볶아주면 견과류의 고소한 향이 살아나고 더욱 바삭해집니다. 식혀둡니다.\n",
            "\n",
            "2.  **두부 바나나 클라우드 베이스 만들기:**\n",
            "    *   연두부는 체에 밭쳐 물기를 충분히 제거합니다. (키친타월로 가볍게 눌러주면 좋습니다.)\n",
            "    *   믹서기에 물기를 제거한 연두부, 잘 익은 바나나, 메이플 시럽(또는 꿀), 바닐라 익스트랙(선택 사항), 소금 아주 약간을 넣습니다.\n",
            "    *   모든 재료가 부드러운 크림처럼 될 때까지 곱게 갈아줍니다. 중간에 멈춰서 벽에 붙은 재료들을 긁어내고 다시 갈아주세요.\n",
            "    *   맛을 보고 단맛이 부족하면 메이플 시럽을 더 추가합니다. 질감이 너무 되직하면 식물성 우유(아몬드유, 두유 등)를 1~2큰술 넣어 농도를 조절할 수 있습니다.\n",
            "\n",
            "3.  **담아내기:**\n",
            "    *   투명한 유리컵이나 디저트 볼에 두부 바나나 클라우드 베이스를 절반 정도 채웁니다.\n",
            "    *   그 위에 준비해둔 에너지바 크럼블을 넉넉하게 뿌려줍니다.\n",
            "    *   다시 두부 바나나 클라우드 베이스를 채우고, 맨 위에 에너지바 크럼블을 풍성하게 올려 마무리합니다.\n",
            "\n",
            "4.  **냉장 및 장식:**\n",
            "    *   완성된 디저트를 냉장고에 넣어 30분 이상 차갑게 식히면 더욱 맛있습니다.\n",
            "    *   서빙 직전에 신선한 바나나 슬라이스, 민트 잎으로 장식하거나 코코아 파우더를 살짝 뿌려주면 시각적인 아름다움을 더할 수 있습니다.\n",
            "\n",
            "**[쉐프의 팁]**\n",
            "\n",
            "*   **바나나 선택:** 바나나는 잘 익어 검은 반점이 생긴 것이 당도가 높아 더욱 맛있습니다.\n",
            "*   **에너지바 활용:** 초콜릿 칩이 들어간 에너지바를 사용하면 달콤함이 배가되고, 견과류와 씨앗이 풍부한 에너지바는 고소함과 식감을 더해줍니다.\n",
            "*   **변형:** 기호에 따라 시나몬 파우더나 카카오닙스를 추가하여 풍미를 더욱 다채롭게 만들 수 있습니다.\n",
            "*   **보관:** 남은 디저트는 밀폐 용기에 담아 냉장 보관하고, 가급적 빨리 드시는 것이 좋습니다.\n",
            "\n",
            "이 요리는 아침 식사 대용으로도 훌륭하고, 건강한 간식이나 가벼운 디저트로도 손색이 없습니다. 당신의 식탁에 새로운 활력과 즐거움을 선사할 것입니다!\n"
          ]
        }
      ],
      "source": [
        "recipe_chain = recipe_template | gemini_llm | parser\n",
        "response = recipe_chain.invoke({'ingredient':'연두부, 에너지바, 바나나'})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b833846a",
      "metadata": {
        "id": "b833846a"
      },
      "source": [
        "## [실습] 검색 결과 분류 체인 만들기\n",
        "\n",
        "다음은 Arxiv의 최신 논문을 검색하는 함수입니다.   \n",
        "해당 논문들이 LLM 관련 논문인지 분류하는 체인을 만들고, 실행하여 결과를 비교하세요.   \n",
        "**함수의 결과물로 다양한 값들이 있으므로, 값들 중 필요한 값만 입력받는 체인을 만들고 실행하세요.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de89b55b",
      "metadata": {
        "id": "de89b55b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a755e0-ad95-44aa-9a29-91094b10650b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7d8e8c915100>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7d8e8cc6e510>, root_client=<openai.OpenAI object at 0x7d8e8ca35340>, root_async_client=<openai.AsyncOpenAI object at 0x7d8e8c9141d0>, model_name='gpt-5-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True, reasoning_effort='low')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# gpt llm 사용하기\n",
        "gpt_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d80cbe",
      "metadata": {
        "id": "14d80cbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c09456-17bf-4bbb-b43c-5481e6129d7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "=== 모든 카테고리 최근 논문 ===\n",
            "총 5개의 논문을 가져왔습니다.\n",
            "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM\n",
            "Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery\n",
            "LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal\n",
            "A merger within a merger: Chandra pinpoints the short GRB 230906A in a peculiar environment\n",
            "BiomedXPro: Prompt Optimization for Explainable Diagnosis with Biomedical Vision Language Models\n",
            "\n",
            "\n",
            "=== 검색어 `Security` 로 검색한 최근 논문 ===\n",
            "총 5개의 논문을 가져왔습니다.\n",
            "Sound Clouds: Exploring ambient intelligence in public spaces to elicit deep human experience of awe, wonder, and beauty\n",
            "On Universality of Deep Equivariant Networks\n",
            "Towards Proactive Defense Against Cyber Cognitive Attacks\n",
            "Ambusher: Exploring the Security of Distributed SDN Controllers Through Protocol State Fuzzing\n",
            "Self-evolving expertise in complex non-verifiable subject domains: dialogue as implicit meta-RL\n"
          ]
        }
      ],
      "source": [
        "import arxiv\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "def get_arxiv_papers(query: Optional[str] = None, N: int = 10) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    arXiv에서 논문 리스트를 가져오는 함수\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    query : str, optional\n",
        "        검색어\n",
        "    N : int, default=10\n",
        "        가져올 논문 개수\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    List[Dict] : 논문 정보를 담은 딕셔너리 리스트\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    search_query = query\n",
        "\n",
        "    # arxiv 클라이언트 생성\n",
        "    client = arxiv.Client()\n",
        "\n",
        "    # 검색 객체 생성\n",
        "    search = arxiv.Search(\n",
        "        query=search_query,\n",
        "        max_results=N,\n",
        "        sort_by=arxiv.SortCriterion.SubmittedDate,  # 제출일 기준 정렬\n",
        "        sort_order=arxiv.SortOrder.Descending  # 최신순\n",
        "    )\n",
        "\n",
        "    # 결과를 저장할 리스트\n",
        "    papers = []\n",
        "\n",
        "    # 검색 실행 (새로운 API 사용)\n",
        "    for result in client.results(search):\n",
        "        paper_info = {\n",
        "            'title': result.title,\n",
        "            'authors': [author.name for author in result.authors],\n",
        "            'summary': result.summary,\n",
        "            'published': result.published.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'updated': result.updated.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'arxiv_id': result.entry_id.split('/')[-1],  # arXiv ID 추출\n",
        "            'pdf_url': result.pdf_url,\n",
        "            'categories': result.categories,\n",
        "            'primary_category': result.primary_category,\n",
        "            'comment': result.comment,\n",
        "            'journal_ref': result.journal_ref\n",
        "        }\n",
        "        papers.append(paper_info)\n",
        "\n",
        "    return papers\n",
        "\n",
        "def get_recent_papers_all_categories(N: int = 10) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    모든 카테고리에서 최근 논문을 가져오는 함수\n",
        "    (더 안정적인 방법)\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    N : int, default=10\n",
        "        가져올 논문 개수\n",
        "    \"\"\"\n",
        "\n",
        "    return get_arxiv_papers(query=\"the\", N=N)\n",
        "\n",
        "\n",
        "print(\"\\n\\n=== 모든 카테고리 최근 논문 ===\")\n",
        "all_papers = get_recent_papers_all_categories(N=5)\n",
        "print(f\"총 {len(all_papers)}개의 논문을 가져왔습니다.\")\n",
        "print('\\n'.join([paper['title'] for paper in all_papers]))\n",
        "\n",
        "query = 'Security'\n",
        "print(f\"\\n\\n=== 검색어 `{query}` 로 검색한 최근 논문 ===\")\n",
        "security_papers = get_arxiv_papers(query=query, N=5)\n",
        "print(f\"총 {len(security_papers)}개의 논문을 가져왔습니다.\")\n",
        "print('\\n'.join([paper['title'] for paper in security_papers]))\n",
        "\n",
        "# # 임의의 검색어로 검색\n",
        "# query = '검색어'\n",
        "# papers = get_arxiv_papers(query='*', N=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gpt_llm.invoke(\"\"\"\n",
        "주어진 논문에 대해, 해당 논문이 LLM 관련인지 분류하는 프롬프트를 만들어줘.\n",
        "매개변수 2개: title, summary를 format template로 입력받도록 하는 문자열 형태로 출력\n",
        "\"\"\").content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzNG1Oakoh6F",
        "outputId": "b211be9d-9919-4aed-bbe3-582e27db6309"
      },
      "id": "QzNG1Oakoh6F",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "다음 문자열을 그대로 프롬프트로 사용하세요. {title} 및 {summary} 자리에 각각 논문 제목과 요약을 넣어 평가합니다.\n",
            "\n",
            "\"다음은 논문 정보입니다.\n",
            "Title: {title}\n",
            "Summary: {summary}\n",
            "\n",
            "지시사항:\n",
            "1) 이 논문이 LLM(대형언어모델, transformer 기반 대규모 언어 생성·이해 모델)과 '직접적으로 관련 있는지' 분류하시오. 직접 관련의 예: 모델 설계(architecture), 학습방법(finetuning, pretraining, RLHF 등), 대규모 언어모델 실험 결과, LLM 응용(챗봇·대화시스템 등), LLM 특유의 문제(편향, 추론, 안전성 등)를 주제로 다룰 때 'Yes'. 반대로 자연어처리 일반, 비언어모델 ML 연구, 소규모 모델·비언어 생성 모델만 다룬다면 'No'. 관련 가능성은 있으나 명확하지 않으면 'Maybe'.\n",
            "2) 판단 근거를 1~3문장으로 간결히 작성하시오(논문에서 어떤 문구·주제가 LLM 관련 판단을 이끌었는지 명시).\n",
            "3) 추가로 관련 키워드(예: \"transformer\", \"pretraining\", \"LLM\", \"chatbot\", \"RLHF\", \"few-shot\", \"in-context learning\", \"tokenization\")를 최대 5개까지 추출하여 제시하시오.\n",
            "4) 출력 형식은 반드시 아래 JSON 형식을 따르시오.\n",
            "\n",
            "출력 JSON 예시:\n",
            "{\n",
            "  \"is_llm\": \"Yes\" | \"No\" | \"Maybe\",\n",
            "  \"confidence\": 0.00-1.00,            // 소수로 0~1 사이\n",
            "  \"reason\": \"근거 1~3문장\",\n",
            "  \"keywords\": [\"키워드1\",\"키워드2\",...]\n",
            "}\n",
            "\n",
            "제약:\n",
            "- confidence는 판단의 확실성을 0~1 사이 소수로 표시(예: 0.85).\n",
            "- reason은 30~150자 내외로 간결히 작성.\n",
            "- keywords는 최대 5개, 없으면 빈 배열([])로 반환.\n",
            "\n",
            "지금 바로 판단하라.\"\n",
            "\n",
            "(프롬프트 사용 예: 위 문자열에서 {title}과 {summary}를 실제 값으로 바꿔 LLM에 입력하면 됩니다.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f924d742",
      "metadata": {
        "id": "f924d742"
      },
      "outputs": [],
      "source": [
        "# Prompt, LLM, Parser\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "prompt = ChatPromptTemplate([\n",
        "        ('system' , '''\n",
        "        Arxiv에 게재된 논문 정보를 보고\n",
        "        해당 논문이 'LLM 관련(대형 언어 모델 관련)'인지 여부를 판별하시오.\n",
        "\n",
        "        지시사항:\n",
        "        1) 이 논문이 LLM(대형언어모델, transformer 기반 대규모 언어 생성·이해 모델)과 '직접적으로 관련 있는지' 분류하시오. 직접 관련의 예: 모델 설계(architecture), 학습방법(finetuning, pretraining, RLHF 등), 대규모 언어모델 실험 결과, LLM 응용(챗봇·대화시스템 등), LLM 특유의 문제(편향, 추론, 안전성 등)를 주제로 다룰 때 'Yes'. 반대로 자연어처리 일반, 비언어모델 ML 연구, 소규모 모델·비언어 생성 모델만 다룬다면 'No'. 관련 가능성은 있으나 명확하지 않으면 'Maybe'.\n",
        "        2) 판단 근거를 1~3문장으로 간결히 작성하시오(논문에서 어떤 문구·주제가 LLM 관련 판단을 이끌었는지 명시).\n",
        "        3) 추가로 관련 키워드(예: \"transformer\", \"pretraining\", \"LLM\", \"chatbot\", \"RLHF\", \"few-shot\", \"in-context learning\", \"tokenization\")를 최대 5개까지 추출하여 제시하시오.\n",
        "        4) 출력 형식은 반드시 아래 JSON 형식을 따르시오.\n",
        "\n",
        "        출력예시는 반드시 이 포맷으로 제공: json 이 아닌 텍스트\n",
        "\n",
        "        1) is_llm : Yes | No | Maybe,\n",
        "        2) confidence : 0.00-1.00,            // 소수로 0~1 사이\n",
        "        3) reason : 근거 1~3문장,\n",
        "        4) keywords : [키워드1,키워드2,...]\n",
        "\n",
        "        제약:\n",
        "        - confidence는 판단의 확실성을 0~1 사이 소수로 표시(예: 0.85).\n",
        "        - reason은 30~150자 내외로 간결히 작성.\n",
        "        - keywords는 최대 5개, 없으면 빈 배열([])로 반환.\n",
        "        '''\n",
        "        ),\n",
        "        ('user' , '''\n",
        "        입력:\n",
        "        - Title: {title}\n",
        "        - Summary: {summary}\n",
        "        '''\n",
        "        )\n",
        "    ])\n",
        "classify_chain = prompt | gpt_llm | parser\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = classify_chain.invoke(all_papers[0])\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHMGvE0prXPf",
        "outputId": "0edb51d4-601c-4e1d-808d-285a826798c3"
      },
      "id": "RHMGvE0prXPf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) is_llm : Yes,\n",
            "2) confidence : 0.95,\n",
            "3) reason : 논문은 \"omni-modal LLM\"을 목표로 모델 아키텍처(OmniAlignNet, Temporal Embedding 등), 데이터·학습 토큰 수 및 LLM 성능 비교를 다루므로 대형 언어모델 관련 연구로 판단됩니다.,\n",
            "4) keywords : [\"omni-modal\",\"LLM\",\"architecture\",\"pretraining\",\"multimodal\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da2adac6",
      "metadata": {
        "id": "da2adac6"
      },
      "outputs": [],
      "source": [
        "\n",
        "results = classify_chain.batch(all_papers)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0410a04c",
      "metadata": {
        "id": "0410a04c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "12c4d1c5-56db-4f07-ffae-18f414a07704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "좋습니다 — 전체 구조를 한눈에 이해할 수 있도록 단계별로 정리하고, 각 단계에서 고려할 점(보안/정확성/포맷팅 등)과 구현 방법(예시 SQL, 프롬프트 템플릿, 도구 추천)까지 제안하겠습니다. 예시는 음식/레시피 DB(음식종류, 재료, 레시피 테이블 등)를 가정하여 \"계란말이 만드는 방법\" 질문에 대해 LLM이 어떻게 접근해서 특정 포맷으로 답변을 만드는지 보여드립니다.\n",
            "\n",
            "1) 전체 아키텍처(흐름 개요)\n",
            "- 사용자 → 프론트엔드(질문 입력)\n",
            "- Backend API(질문 처리)\n",
            "  - 1) 질의 해석 / 의도 추출 (LLM 혹은 규칙 기반)\n",
            "  - 2) DB 접근 전략 결정 (직접 SQL 생성 실행 / 미리 정의된 쿼리 템플릿 사용 / 검색 기반 래핑)\n",
            "  - 3) DB 쿼리 실행 (ORM 또는 Prepared Statement)\n",
            "  - 4) 결과 정규화 및 구조화\n",
            "  - 5) LLM에 결과 + 응답 포맷 지침 전달 → 최종 자연어/정형 응답 생성\n",
            "- 사용자에게 응답 반환\n",
            "\n",
            "두 가지 주요 패턴:\n",
            "- 패턴 A (LLM이 SQL 생성): 사용자의 자연어를 LLM에 주어 SQL을 생성하게 하고, 백엔드가 실행 → 결과를 다시 LLM에게 주어 정해진 출력 포맷으로 변환.\n",
            "- 패턴 B (템플릿/매핑): 미리 정의된 쿼리 템플릿/ 매핑을 사용하여 자연어를 파싱해 적절한 쿼리를 실행 → LLM은 결과를 포맷팅/부연 설명만 담당.\n",
            "\n",
            "2) 어떤 방식을 쓸지 결정하는 기준\n",
            "- 정확성/보안 우선: 템플릿/파서(패턴 B)가 안전. SQL 인젝션 위험 적고 예측 가능.\n",
            "- 유연성/복잡한 질의 필요: LLM이 SQL 생성(패턴 A)이 편리하나 검증/제한이 필요.\n",
            "- 유지보수성: 테이블 변경이 잦다면 템플릿 관리 비용 고려.\n",
            "\n",
            "3) 안전·검증 전략 (필수)\n",
            "- 절대 직접 사용자 입력을 raw SQL로 실행하지 말 것.\n",
            "- LLM이 생성한 SQL을 반드시 검증/화이트리스트화:\n",
            "  - 허용하는 테이블/컬럼 목록과 비교.\n",
            "  - 허용하지 않는 SQL(DDL, DROP, ALTER, 복잡한 JOIN/서브쿼리 제한 등) 차단.\n",
            "- Prepared statements 또는 ORM 사용. 파라미터 바인딩 권장.\n",
            "- 쿼리 타임아웃/리소스 제한.\n",
            "- 최소 권한 DB 계정 사용(읽기 전용 등).\n",
            "\n",
            "4) 예시 DB 스키마 (간단)\n",
            "- foods (food_id, name, category, description)\n",
            "- ingredients (ingredient_id, name, allergen_flag)\n",
            "- recipes (recipe_id, food_id, step_number, instruction, duration_minutes)\n",
            "- recipe_ingredients (recipe_id, ingredient_id, amount, unit)\n",
            "\n",
            "5) 예시: \"계란말이 만드는 방법좀 알려줘\" 처리 흐름(A 패턴: LLM→SQL)\n",
            "- 1단계: 의도 파악/슬롯 추출(옵션)\n",
            "  - intent: \"요리방법 조회\"\n",
            "  - dish: \"계란말이\"\n",
            "  - output_format: (프론트엔드 제공) 예: JSON {title, time, ingredients:[], steps:[]}\n",
            "- 2단계: LLM에게 SQL 생성 프롬프트 (예시 프롬프트)\n",
            "  - Instruction: \"아래 제한을 엄격히 지켜 한 개의 SELECT 문만 생성하라. 허용 테이블: foods, recipes, recipe_ingredients, ingredients. 반환 컬럼은 recipe_id, food_id, food_name, total_time, ingredient_name, amount, unit, step_number, instruction. 파라미터는 ? 로 사용하라. Dish 이름을 ? 바인딩으로 사용.\"\n",
            "  - User input: \"Dish = '계란말이'\"\n",
            "- 3단계: LLM이 생성한 SQL(예시)\n",
            "  SELECT f.food_id AS food_id, f.name AS food_name,\n",
            "         NULL AS total_time, -- (만약 총시간 컬럼 없으면 null 혹은 계산)\n",
            "         i.name AS ingredient_name, ri.amount, ri.unit,\n",
            "         r.step_number, r.instruction\n",
            "  FROM foods f\n",
            "  JOIN recipes r ON f.food_id = r.food_id\n",
            "  JOIN recipe_ingredients ri ON r.recipe_id = ri.recipe_id\n",
            "  JOIN ingredients i ON ri.ingredient_id = i.ingredient_id\n",
            "  WHERE LOWER(f.name) = LOWER(?);\n",
            "- 4단계: 백엔드에서 SQL 검증(허용 테이블/컬럼, DML/DDL 차단) 후 파라미터 바인딩('계란말이')로 실행.\n",
            "- 5단계: DB 결과 ↦ 정규화(예: recipe 단위로 그룹화하여 JSON 구조 생성)\n",
            "- 6단계: LLM에게 \"출력 포맷 강제 지시\"와 함께 결과 전달 → 최종 응답 생성\n",
            "  - 예: \"응답은 JSON 형태로 {title, total_time, ingredients:[{name, amount, unit}], steps:[{step_number, instruction}]} 만 반환하라.\"\n",
            "\n",
            "6) 패턴 B(템플릿/파서) 예시 (권장: 단순한 쿼리/높은 정확도)\n",
            "- 사용자 문장 파싱(간단 규칙 or 작은 NLU 모델)\n",
            "  - dish=\"계란말이\"\n",
            "- Choose SQL template:\n",
            "  - SELECT ... FROM ... WHERE LOWER(name)=LOWER(:dish)\n",
            "- Execute, group, format. LLM은 포맷/부가 설명(팁, 대체재 등)만 담당.\n",
            "\n",
            "7) 응답 포맷 설계(명확한 스키마 권장)\n",
            "- 예: JSON schema\n",
            "  {\n",
            "    \"title\": \"계란말이\",\n",
            "    \"total_time_minutes\": 10,\n",
            "    \"servings\": 2,\n",
            "    \"ingredients\": [\n",
            "      {\"name\": \"계란\", \"amount\": 3, \"unit\": \"개\"},\n",
            "      {\"name\": \"소금\", \"amount\": \"약간\", \"unit\": null}\n",
            "    ],\n",
            "    \"steps\": [\n",
            "      {\"step\": 1, \"instruction\": \"계란을 풀어 소금을 넣고 섞는다.\"},\n",
            "      {\"step\": 2, \"instruction\": \"팬에 기름을 두르고 약불에서 천천히 말아준다.\"}\n",
            "    ],\n",
            "    \"notes\": \"중불에서 익히면 속이 익기 어려우므로 약불 권장.\"\n",
            "  }\n",
            "- LLM에게 출력은 반드시 이 schema만 따르도록 강제(또는 API function calling/JSON schema validation 사용).\n",
            "\n",
            "8) 예시 전체 대화/프롬프트 (간단한 템플릿)\n",
            "- System: \"당신은 DB에서 쿼리 결과를 사용자 친화적 JSON으로 만드는 역할입니다. 출력은 오직 JSON schema X만으로 응답하세요.\"\n",
            "- User: \"계란말이 만드는 방법 알려줘\"\n",
            "- Backend: (parse dish=\"계란말이\") → run template query → results\n",
            "- Backend → LLM: \"다음 DB 결과를 JSON schema X로 변환하고, 필요하면 추가 팁 1-2문장 덧붙여라. DB 결과: [...]\"\n",
            "- LLM → return JSON\n",
            "\n",
            "9) 도구/라이브러리 추천\n",
            "- 프롬프트/워크플로우: LangChain, LlamaIndex(특히 RAG, 요약, 인덱싱), Vertex AI\n",
            "- SQL/ORM: SQLAlchemy (Python), knex (Node), prepared statements\n",
            "- 검증: SQL parsers (sqlparse), 화이트리스트 검사 라이브러리\n",
            "- LLM API: OpenAI, Anthropic, Mistral 등. 함수 호출/JSON Schema 기능이 있으면 포맷 강제에 유용.\n",
            "\n",
            "10) 구현 팁과 주의사항\n",
            "- 디버깅: LLM이 생성한 SQL과 최종 JSON을 로그(민감정보 제외)하여 문제 추적.\n",
            "- 캐싱: 인기 질의(예: 계란말이 레시피)는 캐시해서 응답 속도 개선.\n",
            "- 다국어 처리: 음식명 표준화(동일 음식의 여러 표현 처리) — 별도 정규화 테이블/alias 테이블 구축.\n",
            "- 버전 관리: 레시피 변경 가능성을 고려한 레코드 버전(유효기간, 수정일 등).\n",
            "- 오류 케이스: DB에 해당 음식이 없을 때 LLM에게 대체 제안(유사한 음식 추천)하도록 지시.\n",
            "- 개인정보/저작권: 외부 출처 레시피를 그대로 저장/공개할 때 저작권 문제 검토.\n",
            "\n",
            "11) 간단한 예시 결과(최종 사용자에게 보여줄 형태)\n",
            "- 출력(사람이 읽기 좋게 변환된 예)\n",
            "  제목: 계란말이\n",
            "  총 시간: 10분\n",
            "  재료:\n",
            "  - 계란 3개\n",
            "  - 소금 약간\n",
            "  순서:\n",
            "  1) 계란을 풀어 소금을 섞는다.\n",
            "  2) 기름 두른 팬에 약불로 부어 말아가며 익힌다.\n",
            "  팁: 중불보다 약불에서 천천히 익히면 속이 촉촉합니다.\n",
            "\n",
            "요약\n",
            "- 안전하고 예측 가능한 시스템을 원하면 '템플릿 + 포맷팅용 LLM' 혼합 방식(B 패턴)을 권장합니다.\n",
            "- 복잡하고 자유로운 질의를 원하면 'LLM→SQL→검증→실행→LLM 포맷' 흐름을 사용하되 엄격한 검증과 권한 제한을 반드시 넣으세요.\n",
            "- 출력 포맷은 JSON 등 스키마로 엄격히 정의하고, LLM에게 그것만 내보내도록 지시하거나 function-calling/스키마 밸리데이션을 사용하세요.\n",
            "\n",
            "원하시면:\n",
            "- 실제 코드 예제(Python + FastAPI + SQLAlchemy + OpenAI prompt)로 작은 PoC 템플릿을 만들어 드릴게요.\n",
            "- 혹은 LLM이 생성한 SQL을 자동 검증하는 샘플 검증기(화이트리스트 기반) 코드도 제공할 수 있습니다. 어느 쪽을 먼저 원하시나요?\n"
          ]
        }
      ],
      "source": [
        "print(gpt_llm.invoke(\"\"\"\n",
        "나는 특정 질문에 대하여 rdb내에 있는 내용을 바탕으로 llm이 특정한 포맷으로 답변을 하는 프로그램을 만들고싶어. 나는 rdb 전문가이고, 프로그램은 할 수 있지만 llm을 어떤식으로 연결해야하는지 모르겠어. 저장된 데이터 내용에 대해서도 잘 알고있는데, 만약 데이터가 음식과 레시피에 대한 부분이라고 가정하고 테이블들은 음식종류 테이블, 재료테이블, 레시피테이블 등등 이렇게 있을건데, 계란말이 만드는 방법좀 알려줘 이렇게 질문했을때 이걸 llm이 접근하는 구조를 전체적으로 알고싶어.\n",
        "\"\"\").content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e435cb7",
      "metadata": {
        "id": "5e435cb7"
      },
      "source": [
        "## [실습] LLM 최신 연구 요약 체인 만들기\n",
        "\n",
        "분류 결과를 바탕으로, LLM 관련 논문만 모아 요약할 수 있습니다.\n",
        "\n",
        "적절한 요약 프롬프트를 생성하여, 이전 실습의 결과 중 LLM에 해당하는 결과들만을 모으세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f29a9567",
      "metadata": {
        "id": "f29a9567",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac593faa-6a02-4b65-d039-a28f5b7461f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "# 임의의 검색어로 검색\n",
        "query = '검색어'\n",
        "papers = get_arxiv_papers(query=query, N=5)\n",
        "\n",
        "\n",
        "LLM_documents=[]\n",
        "\n",
        "for paper in papers:\n",
        "    if \"is_llm : Yes\" in classify_chain.invoke(papers): # LLM 분류 조건 넣기\n",
        "        LLM_documents.append(\n",
        "            f\"\"\"제목:{paper['title']}\n",
        "            저자:{paper['authors']}\n",
        "            PDF 링크:{paper['pdf_url']}\n",
        "            요약:{paper['summary']}\n",
        "            \"\"\"\n",
        "        )\n",
        "        # 분류시 로직 추가\n",
        "        # Str 형식의 저보를 저장\n",
        "\n",
        "LLM_documents\n",
        "# [\"제목: , 저자: , 요약: \", \"제목: , 저자: , 요약: \", ...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e0b22a1",
      "metadata": {
        "id": "4e0b22a1"
      },
      "outputs": [],
      "source": [
        "# 분류 프롬프트와 체인 만들기\n",
        "summary_prompt = ChatPromptTemplate(\n",
        "    [\n",
        "        ('system', '''주어진 논문들의 정보를 이용하여,\n",
        "                      주제에 대한 최신 연구 동향 뉴스레터를 쓰세요.\n",
        "\n",
        "                      유머러스하고 과장된 톤으로, 논문의 주요 내용과 기여, 발전 방향을 소개하세요.\n",
        "        '''),\n",
        "        ('human', '''\n",
        "        주제: {topic} 관련 LLM 논문들\n",
        "\n",
        "        조사 논문 목록: {context}\n",
        "        ''')\n",
        "    ]\n",
        ")\n",
        "summary_chain = summary_prompt | gpt_llm | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86d29360",
      "metadata": {
        "id": "86d29360"
      },
      "outputs": [],
      "source": [
        "# LLM 페이퍼 요약 출력하기\n",
        "summary = summary_chain.invoke(\n",
        "    {\n",
        "        'topic':query,\n",
        "        'context':LLM_documents\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbFL1FoC4pVh",
        "outputId": "df99e4f4-3447-467d-9e28-30fdda6f2bda"
      },
      "id": "wbFL1FoC4pVh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "주어진 논문 목록이 비어 있어서(아무도 논문을 숨겨놨나 봅니다…), 실제 논문들을 직접 인용하지는 못하지만, “검색어(쿼리) 관련 LLM 연구”의 최신 연구 동향을 종합·정리한 유머러스하고 약간 과장된 뉴스레터를 대신 작성해 드립니다. 원하시면 나중에 구체적 논문 리스트나 PDF를 주시면 해당 논문들을 반영해 더 정확한 버전을 만들어 드릴게요. 그럼 출발—검색어의 서사시, LLM 버전으로 가봅시다.\n",
            "\n",
            "----------------------------\n",
            "검색어 속으로 뛰어든 거대한 언어 생명체들 — 검색어 관련 LLM 연감\n",
            "Issue #1 — 오늘의 헤드라인: \"당신의 한마디 검색어가 LLM 앞에서 드라마틱하게 변신합니다\"\n",
            "\n",
            "친애하는 연구자·개발자·호기심 많은 독자 여러분,\n",
            "검색창에 “최고의 김밥”만 입력하던 시대는 갔습니다. 이제는 LLM이 당신의 모호한 한 줄을 받아 멋들어지게 다듬고, 필요한 정보를 찾아내며, 때로는 당신의 의도를 심리 분석하듯 읽어내려 합니다. 최근 연구들은 이 ‘쿼리 ↔ LLM’ 상호작용을 전방위로 업그레이드하고 있는데요, 오늘은 그 흐름을 웃음과 약간의 과장으로 요약해 드립니다.\n",
            "\n",
            "주요 흐름 1 — 쿼리 이해와 의도 파악: LLM이 당신의 속마음을 읽는다 (조심)\n",
            "- 무엇이 새로워졌나: 단순 키워드 매칭은 옛말. LLM 기반 모델들이 문맥과 대화 이력, 사용자 프로필(또는 세션 정보)을 결합해 한 줄 쿼리의 숨은 의도까지 추론합니다.\n",
            "- 핵심 기여: 대화형 의도 추론, 다의어 해소, 상황 의존적 재해석(예: “오늘 비 올까?”의 위치·시간 맥락 반영).\n",
            "- 왜 웃긴가: “비 올까?”에 대해 LLM이 기상청 수준의 추정 확률과 우산 구매 링크까지 권하면, 당신은 이미 쇼핑몰 광고의 산물입니다.\n",
            "- 발전 방향: 개인화와 프라이버시의 균형, 의도 불확실성 표현(모델이 ‘확실하지 않음’을 말할 수 있게).\n",
            "\n",
            "주요 흐름 2 — 쿼리 정제 및 재작성 (query rewriting): 쿼리는 다듬어져야 빛난다\n",
            "- 무엇이 새로워졌나: LLM로 사용자 쿼리를 자연스럽고 검색엔진 최적화된(또는 검색 시스템에 친화적인) 형태로 재작성합니다. 장문의 설명도 핵심으로 압축 가능.\n",
            "- 핵심 기여: 번역적 정제, 오타·약어 복원, 개인화된 추천 검색어 생성.\n",
            "- 왜 웃긴가: “피자”를 “칼로리·배달시간 포함 뉴욕스타일 피자 추천”으로 바꿔 주면, 당신의 허기까지 기계가 설계한 듯한 느낌.\n",
            "- 발전 방향: 재작성의 투명성(원본↔재작성 비교), 사용자가 원치 않을 때의 ‘원본 존중 모드’.\n",
            "\n",
            "주요 흐름 3 — 쿼리 생성(생성형 질의): 검색엔진이 스스로 질문을 던진다\n",
            "- 무엇이 새로워졌나: 쿼리를 자동 생성해서 데이터베이스·문서 코퍼스에 대한 보충 압축 질의를 만들고, 검색 커버리지를 높입니다.\n",
            "- 핵심 기여: 적은 레이블로도 다양한 쿼리 생성, 데이터 증강을 통한 검색 시스템 개선.\n",
            "- 왜 웃긴가: 모델이 “이 문서에는 이런 질문들이 필요할 것 같습니다”라며 질문 리스트를 뱉어내면, 문서가 인터뷰 당하는 기분.\n",
            "- 발전 방향: 쿼리 다양성의 제어, 무의미·편향된 쿼리 생성 방지.\n",
            "\n",
            "주요 흐름 4 — LLM + Retrieval: RAG(검색 보강 생성)이 대세\n",
            "- 무엇이 새로워졌나: LLM이 외부 지식(벡터 DB, 문서 인덱스)을 끌어와 사실 기반 응답을 생성 — hallucination을 줄이는 대표적 방법.\n",
            "- 핵심 기여: 검색-생성 파이프라인, context-window 최적화, evidence attribution(출처 표시).\n",
            "- 왜 웃긴가: 모델이 출처를 덕지덕지 붙이며 “내 말은 이 문서들이 증명해줍니다”라며 변호사처럼 나오면, 말하기 전 증거 보관함을 본능적으로 열게 됨.\n",
            "- 발전 방향: 실시간 업데이트(지식 신선도), 근거의 신뢰도 점수화, 대규모 파이프라인의 비용 효율화.\n",
            "\n",
            "주요 흐름 5 — 재랭킹과 세밀한 정밀도: LLM이 결과를 다시 정돈한다\n",
            "- 무엇이 새로워졌나: 초기 후보 검색(검색엔진·벡터 검색)이 뽑은 결과를 LLM로 세밀하게 재평가·재정렬하여 더 관련성 높은 결과 제공.\n",
            "- 핵심 기여: semantic reranking, cross-encoder 스타일 정밀도 향상, 사용자 피드백을 반영한 학습.\n",
            "- 왜 웃긴가: 검색결과가 LLM의 ‘미적 기준’에 따라 재배치되면, 어떤 결과는 갑자기 “고급 취급”을 받습니다.\n",
            "- 발전 방향: 속도 vs 정확도 트레이드오프 해결, 사용자별 랭킹 맞춤화.\n",
            "\n",
            "주요 흐름 6 — 효율성과 소형화: 거대한 LLM 아니면 안 된다고?\n",
            "- 무엇이 새로워졌나: 경량화된 LLM, 지식 증류, 저전력 온디바이스 검색 보조기술이 늘고 있습니다.\n",
            "- 핵심 기여: LoRA, quantization, knowledge distillation을 통한 배포형 경량 모델.\n",
            "- 왜 웃긴가: “초경량 LLM이 내 휴대폰에서 인터넷을 마법처럼 해결해준다”며 사용자는 스마트폰을 박사로 착각하게 됩니다.\n",
            "- 발전 방향: 경량 모델의 정확도 확보, 프라이버시-효율-정확성 삼박자 맞추기.\n",
            "\n",
            "주요 흐름 7 — 멀티모달·대화형 검색: 이미지·음성까지 쿼리가 된다\n",
            "- 무엇이 새로워졌나: 사용자는 사진 한 장, 음성 한 마디, 또는 화면 캡처로 쿼리를 대신합니다. LLM은 이를 해석해 검색 질의를 생성합니다.\n",
            "- 핵심 기여: OCR·비전-언어 모델 통합, 멀티턴 대화형 검색 세션.\n",
            "- 왜 웃긴가: 사진 한 장으로 “이 치마 어디서 샀어?”를 묻고, 모델이 쇼핑몰·가격·유사 스타일까지 추천하면, 당신의 쇼핑 리스트가 AI의 작품처럼 보입니다.\n",
            "- 발전 방향: 멀티모달 신뢰성, 저작권·프라이버시 문제 해결.\n",
            "\n",
            "주요 흐름 8 — 평가와 벤치마크: 무엇을 ‘좋은 쿼리 반응’이라 할 것인가\n",
            "- 무엇이 새로워졌나: 단순 정답률을 넘어서는 사용자 만족도, 사실성, 응답의 투명성, 속도 평가 지표들이 중요해지고 있습니다.\n",
            "- 핵심 기여: 실세계 대화 데이터 기반 평가, human-in-the-loop 평가, 신뢰도·외부 근거 기반 점수.\n",
            "- 왜 웃긴가: 수치가 좋으면 좋은 것이고, 기쁨을 주면 더 좋은 것이며, 결국 '사람이 웃는가'로 귀결되는 것 같습니다.\n",
            "- 발전 방향: 표준화된 신뢰성 지표, 개인정보·윤리적 평가 프레임워크 통합.\n",
            "\n",
            "기타 떠오르는 화두들 (짧고 굵게)\n",
            "- 합성 쿼리 생성으로 데이터 증강: 적은 레이블로 검색 성능을 뻥튀기하는 테크닉이 인기.\n",
            "- 쿼리 프라이버시 & 로컬 런타임: 사용자 데이터가 서버 밖으로 새지 않게 하려는 노력 증가.\n",
            "- 설명 가능한 검색(Explainable Retrieval): LLM이 “왜 이 결과를 골랐나?”를 설명해야 사용자 신뢰가 산다.\n",
            "- 편향·악용 방지: 악의적 쿼리 생성에 대한 안전장치 필요성 커짐.\n",
            "- 실시간·스트리밍 데이터 통합: 최신 뉴스·가격 정보 반영 문제.\n",
            "\n",
            "미래 예측 (과장 포함)\n",
            "- 1년 내: RAG + 재랭킹의 조합이 대부분의 상용 검색에서 기본 스택으로 자리잡을 것. 쿼리 재작성/세션 기반 개인화가 대중화.\n",
            "- 3년 내: 휴대폰/브라우저 내 경량 LLM이 쿼리 전처리·후처리를 담당, 프라이버시 친화적 검색 경험이 보편화.\n",
            "- 5년 내: 쿼리는 단순 키워드가 아니라 ‘대화형 계약’이 된다 — 사용자는 한 번의 문장으로 조명·요약·비교·쇼핑까지 끝낸다. 그리고 가끔 LLM이 유머를 섞어 답변을 줘서 사용자를 즐겁게 함(그리고 가끔 과장도 함).\n",
            "\n",
            "연구자들께 권하는 연구 아이디어\n",
            "- 쿼리 불확실성 표기법: 모델이 자신감·불확실성을 자연언어로 표현하도록 설계하라.\n",
            "- 개인화된 쿼리 보정의 프라이버시 보장 방법: 로컬 파인튜닝, 암호학적 기법 적용.\n",
            "- 멀티모달 쿼리의 신뢰성 평가 프레임워크: 이미지 기반 쿼리가 얼마나 신뢰성 있는 결과로 이어지는지 정량화.\n",
            "- 비용-효율적 RAG: 검색 후보 수를 최소화하면서도 LLM 응답의 품질을 유지하는 알고리즘.\n",
            "- 사용성 연구: 실제 사용자 인터랙션에서 LLM 기반 쿼리 재작성의 수용성/불편감 조사.\n",
            "\n",
            "에필로그 — 검색어, 이제는 LLM의 연극 무대\n",
            "검색어는 단순한 문자열이 아니라, LLM이 해석하고 보강하고 증거를 달아내며 공연하는 ‘작품’이 되었습니다. 우리는 이 무대에 서서 관람도 하고, 감독도 되어야 합니다. 웃기고 과장된 미래도 있지만 핵심은 분명합니다: 사용자 의도에 충실하고, 사실성에 책임을 지며, 프라이버시와 효율성을 지키는 기술이 승자가 될 겁니다.\n",
            "\n",
            "원하시면:\n",
            "- 특정 논문(예: RAG 관련, query rewriting, retrieval-augmented reranking 등)들을 넣어 논문 기반 뉴스레터로 수정해 드립니다.\n",
            "- 특정 청중(실무 엔지니어, 학계 연구자, 경영진 등)에 맞춘 버전으로 톤·내용을 조정해 드립니다.\n",
            "\n",
            "어떤 식으로 더 다듬어 드릴까요? 논문 목록을 던져주시면, 그 논문들 중심으로 더 사실적이고 인용 가능한 레터로 업그레이드해 드립니다.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "multicampus",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}