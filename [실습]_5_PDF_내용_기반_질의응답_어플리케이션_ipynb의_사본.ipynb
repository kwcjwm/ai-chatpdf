{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKEsZgnVB-CW"
      },
      "source": [
        "# [실습] PDF 내용 기반 질의응답 어플리케이션\n",
        "\n",
        "이번에는 PDF의 내용을 이용해 질의응답을 수행해 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxjH2i5GVdwY"
      },
      "source": [
        "### 라이브러리 설치  \n",
        "\n",
        "랭체인 관련 라이브러리와 벡터 데이터베이스 라이브러리를 설치합니다.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHgrhVbLnM6I",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0089b030-a43e-4d13-e4b9-399732e108cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.0.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.2.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-1.0.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.35)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.37.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.23.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.19.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "INFO: pip is looking at multiple versions of langchain-chroma to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-0.2.6-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.35.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading langchain_openai-0.3.35-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.2.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.4/20.4 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_chroma-0.2.6-py3-none-any.whl (12 kB)\n",
            "Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=ac4c3238180cc208bc395287e6d626ff5aea8af9dc5a3cb0ae7b7793c11a5cab\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, urllib3, pymupdf, pybase64, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, humanfriendly, httptools, bcrypt, backoff, watchfiles, typing-inspect, requests, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, posthog, opentelemetry-semantic-conventions, onnxruntime, dataclasses-json, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, langchain-openai, chromadb, langchain_chroma, langchain-community\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.16.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.16.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 chromadb-1.2.0 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 langchain-community-0.3.31 langchain-openai-0.3.35 langchain_chroma-0.2.6 marshmallow-3.26.1 mmh3-5.2.0 mypy-extensions-1.1.0 onnxruntime-1.23.1 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 posthog-5.4.0 pybase64-1.4.2 pymupdf-1.26.5 pypika-0.48.9 requests-2.32.5 typing-inspect-0.9.0 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai langchain langchain-openai langchain-community chromadb tiktoken langchain_chroma pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5uYZQknXLDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc53d484-dbf4-49f5-88f1-3077da2c6190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API 키 확인\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv('env', override=True)\n",
        "\n",
        "if os.environ.get('OPENAI_API_KEY'):\n",
        "    print('OpenAI API 키 확인')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kZxXmSjQRHy"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature = 0.7, max_tokens = 4096)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p6HLjhjB-Cb"
      },
      "source": [
        "## Indexing : 데이터 불러오기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9hsE11FB-Cb"
      },
      "source": [
        "PyMuPDFLoader를 이용해 목표 문서를 불러옵니다.    \n",
        "실습 시트에 포함된 교재 PDF 파일을 업로드해 주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HthIQomXB-Cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ba6fd3-f11b-4716-fda2-611327b560c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./2505.09388v1.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# 현재 위치의 모든 pdf 불러오기\n",
        "from glob import glob\n",
        "import os\n",
        "\n",
        "pdfs = glob(os.path.join('./', '*.pdf'))\n",
        "pdfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFMUxNT5B-Cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82fa3198-f8f1-41aa-f4cd-534141a3cd0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./2505.09388v1.pdf\n",
            "# Number of documents: 35\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "documents = []\n",
        "\n",
        "for path_material in pdfs:\n",
        "    print(path_material)\n",
        "    loader = PyMuPDFLoader(path_material)\n",
        "    # 페이지별로 저장\n",
        "    pages = loader.load()\n",
        "    documents.append(pages)\n",
        "    print(\"# Number of documents:\", len(pages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JApDOWz3LcXJ"
      },
      "source": [
        "PDF Loader로 불러온 데이터는 페이지 단위로 저장됩니다.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJaToVloLmmb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "392e25ac-792d-49a6-f595-e69718ea6f6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table 9: Examples of SFT data for thinking and non-thinking modes during the thinking mode fusion\n",
            "stage. For the thinking mode, the /think flag can be omitted since it represents the default behavior. This\n",
            "feature has been implemented in the chat template1 supported by the Hugging Face’s tokenizer, where\n",
            "the thinking mode can be disabled using an additional parameter enable thinking=False.\n",
            "Thinking Mode\n",
            "Non-Thinking Mode\n",
            "<|im start|>user\n",
            "{query} /think<|im end|>\n",
            "<|im start|>assistant\n",
            "<think>\n",
            "{thinking content}\n",
            "</think>\n",
            "{response}<|im end|>\n",
            "<|im start|>user\n",
            "{query} /no think<|im end|>\n",
            "<|im start|>assistant\n",
            "<think>\n",
            "</think>\n",
            "{response}<|im end|>\n",
            "stable, which is crucial for maintaining stable training. As a result, we achieve consistent improvements\n",
            "in both training reward and validation performance over the course of a single RL run, without any\n",
            "manual intervention on hyperparameters. For instance, the AIME’24 score of the Qwen3-235B-A22B\n",
            "model increases from 70.1 to 85.1 over a total of 170 RL training steps.\n",
            "4.3\n",
            "Thinking Mode Fusion\n",
            "The goal of the Thinking Mode Fusion stage is to integrate the “non-thinking” capabilities into the\n",
            "previously developed “thinking” model. This approach allows developers to manage and control\n",
            "reasoning behaviors, while also reducing the cost and complexity of deploying separate models for\n",
            "thinking and non-thinking tasks. To achieve this, we conduct continual supervised fine-tuning (SFT)\n",
            "on the Reasoning RL model and design a chat template to fuse the two modes. Moreover, we find that\n",
            "models capable of handling both modes proficiently perform consistently well under different thinking\n",
            "budgets.\n",
            "Construction of SFT data.\n",
            "The SFT dataset combines both the “thinking” and “non-thinking” data.\n",
            "To ensure that the performance of the Stage 2 model is not compromised by the additional SFT, the\n",
            "“thinking” data is generated via rejection sampling on Stage 1 queries using the Stage 2 model itself. The\n",
            "“non-thinking” data, on the other hand, is carefully curated to cover a diverse range of tasks, including\n",
            "coding, mathematics, instruction-following, multilingual tasks, creative writing, question answering,\n",
            "and role-playing. Additionally, we employ automatically generated checklists for assessing the response\n",
            "quality of “non-thinking” data. To enhance the performance on tasks with low-resource languages, we\n",
            "particularly increase the proportion of translation tasks.\n",
            "Chat Template Design.\n",
            "To better integrate the two modes and enable users to dynamically switch the\n",
            "model’s thinking process, we design chat templates for Qwen3, as shown in Table 9. Specifically, for\n",
            "samples in thinking mode and non-thinking mode, we introduce /think and /no think flags in the user\n",
            "query or system message, respectively. This allows the model to follow the user’s input and select the\n",
            "appropriate thinking mode accordingly. For non-thinking mode samples, we retain an empty thinking\n",
            "block in the assistant’s response. This design ensures internal format consistency within the model and\n",
            "allows developers to prevent the model from engaging in thinking behavior by concatenating an empty\n",
            "think block in the chat template. By default, the model operates in thinking mode; therefore, we add\n",
            "some thinking mode training samples where the user queries do not include /think flags. For more\n",
            "complex multi-turn dialogs, we randomly insert multiple /think and /no think flags into users’ queries,\n",
            "with the model response adhering to the last flag encountered.\n",
            "Thinking Budget.\n",
            "An additional advantage of Thinking Mode Fusion is that, once the model learns to\n",
            "respond in both non-thinking and thinking modes, it naturally develops the ability to handle intermediate\n",
            "cases—generating responses based on incomplete thinking. This capability lays the foundation for\n",
            "implementing budget control over the model’s thinking process. Specifically, when the length of the\n",
            "model’s thinking reaches a user-defined threshold, we manually halt the thinking process and insert\n",
            "the stop-thinking instruction: “Considering the limited time by the user, I have to give the\n",
            "solution based on the thinking directly now.\\n</think>.\\n\\n”. After this instruction is inserted,\n",
            "the model proceeds to generate a final response based on its accumulated reasoning up to that point. It\n",
            "is worth noting that this ability is not explicitly trained but emerges naturally as a result of applying\n",
            "Thinking Mode Fusion.\n",
            "11\n",
            "----------\n",
            "4.4\n",
            "General RL\n",
            "The General RL stage aims to broadly enhance the models’ capabilities and stability across diverse\n",
            "scenarios. To facilitate this, we have established a sophisticated reward system covering over 20 distinct\n",
            "tasks, each with customized scoring criteria. These tasks specifically target enhancements in the following\n",
            "core capabilities:\n",
            "• Instruction Following: This capability ensures that models accurately interpret and follow user\n",
            "instructions, including requirements related to content, format, length, and the use of structured\n",
            "output, delivering responses that align with user expectations.\n",
            "• Format Following: In addition to explicit instructions, we expect the model to adhere to specific\n",
            "formatting conventions. For instance, it should respond appropriately to the /think and /\n",
            "no think flags by switching between thinking and non-thinking modes, and consistently use\n",
            "designated tokens (e.g., <think> and </think>) to separate the thinking and response parts in\n",
            "the final output.\n",
            "• Preference Alignment: For open-ended queries, preference alignment focuses on improving the\n",
            "model’s helpfulness, engagement, and style, ultimately delivering a more natural and satisfying\n",
            "user experience.\n",
            "• Agent Ability: This involves training the model to correctly invoke tools via designated interfaces.\n",
            "During the RL rollout, the model is allowed to perform complete multi-turn interaction cycles\n",
            "with real environment execution feedback, thereby improving its performance and stability in\n",
            "long-horizon decision-making tasks.\n",
            "• Abilities for Specialized Scenarios: In more specialized scenarios, we design tasks tailored to the\n",
            "specific context. For example, in Retrieval-Augmented Generation (RAG) tasks, we incorporate\n",
            "reward signals to guide the model toward generating accurate and contextually appropriate\n",
            "responses, thereby minimizing the risk of hallucination.\n",
            "To provide feedback for the aforementioned tasks, we utilized three distinct types of rewards:\n",
            "(1) Rule-based Reward: The rule-based reward has been widely used in the reasoning RL stage,\n",
            "and is also useful for general tasks such as instruction following (Lambert et al., 2024) and format\n",
            "adherence. Well-designed rule-based rewards can assess the correctness of model outputs with\n",
            "high precision, preventing issues like reward hacking.\n",
            "(2) Model-based Reward with Reference Answer: In this approach, we provide a reference answer\n",
            "for each query and prompt Qwen2.5-72B-Instruct to score the model’s response based on this\n",
            "reference. This method allows for more flexible handling of diverse tasks without requiring strict\n",
            "formatting, avoiding false negatives that can occur with purely rule-based rewards.\n",
            "(3) Model-based Reward without Reference Answer: Leveraging human preference data, we train\n",
            "a reward model to assign scalar scores to model responses. This approach, which does not\n",
            "depend on a reference answer, can handle a broader range of queries while effectively enhancing\n",
            "the model’s engagement and helpfulness.\n",
            "4.5\n",
            "Strong-to-Weak Distillation\n",
            "The Strong-to-Weak Distillation pipeline is specifically designed to optimize lightweight models, encom-\n",
            "passing 5 dense models (Qwen3-0.6B, 1.7B, 4B, 8B, and 14B) and one MoE model (Qwen3-30B-A3B). This\n",
            "approach enhances model performance while effectively imparting robust mode-switching capabilities.\n",
            "The distillation process is divided into two primary phases:\n",
            "(1) Off-policy Distillation: At this initial phase, we combine the outputs of teacher models generated\n",
            "with both /think and /no think modes for response distillation. This helps lightweight student\n",
            "models develop basic reasoning skills and the ability to switch between different modes of\n",
            "thinking, laying a solid foundation for the next on-policy training phase.\n",
            "(2) On-policy Distillation: In this phase, the student model generates on-policy sequences for\n",
            "fine-tuning. Specifically, prompts are sampled, and the student model produces responses in\n",
            "either /think or /no think mode. The student model is then fine-tuned by aligning its logits\n",
            "with those of a teacher model (Qwen3-32B or Qwen3-235B-A22B) to minimize the KL divergence.\n",
            "4.6\n",
            "Post-training Evaluation\n",
            "To comprehensively evaluate the quality of instruction-tuned models, we adopted automatic benchmarks\n",
            "to assess model performance under both thinking and non-thinking modes. These benchmarks are\n",
            "12\n",
            "----------\n",
            "Table 10: Multilingual benchmarks and the included languages. The languages are identified in IETF\n",
            "language tags.\n",
            "Benchmark\n",
            "# Langs\n",
            "Languages\n",
            "Multi-IF\n",
            "8\n",
            "en, es, fr, hi, it, pt, ru, zh\n",
            "INCLUDE\n",
            "44\n",
            "ar, az, be, bg, bn, de, el, es, et, eu, fa, fi, fr, he, hi, hr, hu, hy, id, it, ja, ka,\n",
            "kk, ko, lt, mk, ml, ms, ne, nl, pl, pt, ru, sq, sr, ta, te, tl, tr, uk, ur, uz, vi, zh\n",
            "MMMLU\n",
            "14\n",
            "ar, bn, de, en, es, fr, hi, id, it, ja, ko, pt, sw, zh\n",
            "MT-AIME2024\n",
            "55\n",
            "af, ar, bg, bn, ca, cs, cy, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id,\n",
            "it, ja, kn, ko, lt, lv, mk, ml, mr, ne, nl, no, pa, pl, pt, ro, ru, sk, sl, so, sq, sv,\n",
            "sw, ta, te, th, tl, tr, uk, ur, vi, zh-Hans, zh-Hant\n",
            "PolyMath\n",
            "18\n",
            "ar, bn, de, en, es, fr, id, it, ja, ko, ms, pt, ru, sw, te, th, vi, zh\n",
            "MLogiQA\n",
            "10\n",
            "ar, en, es, fr, ja, ko, pt, th, vi, zh\n",
            "categorized into several dimensions:\n",
            "• General Tasks: We utilize benchmarks including MMLU-Redux (Gema et al., 2024), GPQA-\n",
            "Diamond (Rein et al., 2023), C-Eval (Huang et al., 2023), and LiveBench (2024-11-25) (White et al.,\n",
            "2024). For GPQA-Diamond, we sample 10 times for each query and report the averaged accuracy.\n",
            "• Alignment Tasks: To evaluate how well the model aligns with human preferences, we employ\n",
            "a suite of specialized benchmarks. For instruction-following performance, we report the strict-\n",
            "prompt accuracy of IFEval (Zhou et al., 2023). To assess alignment with human preferences on\n",
            "general topics, we utilize Arena-Hard (Li et al., 2024) and AlignBench v1.1 (Liu et al., 2023b). For\n",
            "writing tasks, we rely on Creative Writing V3 (Paech, 2024) and WritingBench (Wu et al., 2025) to\n",
            "evaluate the model’s proficiency and creativity.\n",
            "• Math & Text Reasoning: For evaluating mathematical and logical reasoning skills, we employ\n",
            "high-level math benchmarks including MATH-500 (Lightman et al., 2023), AIME’24 and AIME’25\n",
            "(AIME, 2025), and text reasoning tasks including ZebraLogic (Lin et al., 2025) and AutoLogi\n",
            "(Zhu et al., 2025). For AIME problems, each year’s questions include Part I and Part II, totaling\n",
            "30 questions. For each question, we sample 64 times and take the average accuracy as the final\n",
            "score.\n",
            "• Agent & Coding: To test the model’s proficiency in coding and agent-based tasks, we use BFCL\n",
            "v3 (Yan et al., 2024), LiveCodeBench (v5, 2024.10-2025.02) (Jain et al., 2024), and Codeforces\n",
            "Ratings from CodeElo (Quan et al., 2025). For BFCL, all Qwen3 models are evaluated using the\n",
            "FC format, and yarn was used to deploy the models to a context length of 64k for Multi-Turn\n",
            "evaluation. Some baselines are derived from the BFCL leaderboard, taking the higher scores\n",
            "between FC and Prompt formats. For models not reported on the leaderboard, the Prompt\n",
            "formats are evaluated. For LiveCodeBench, for the non-thinking mode, we use the officially\n",
            "recommended prompt, while for the thinking mode, we adjust the prompt template to allow\n",
            "the model to think more freely, by removing the restriction You will not return anything\n",
            "except for the program. To evaluate the performance gap between models and competitive\n",
            "programming experts, we use CodeForces to calculate Elo ratings. In our benchmark, each\n",
            "problem is solved by generating up to eight independent reasoning attempts.\n",
            "• Multilingual Tasks: For multilingual capabilities, we evaluate four kinds of tasks: instruction\n",
            "following, knowledge, mathematics, and logical reasoning. Instruction following is assessed\n",
            "using Multi-IF (He et al., 2024), which focuses on 8 key languages. Knowledge assessment\n",
            "consisted of two types: regional knowledge evaluated through INCLUDE (Romanou et al.,\n",
            "2024), covering 44 languages, and general knowledge assessed with MMMLU (OpenAI, 2024)\n",
            "across 14 languages, excluding the unoptimized Yoruba language; for these two benchmarks,\n",
            "we sample only 10% of the original data to improve evaluation efficiency. The mathematics task\n",
            "employ MT-AIME2024 (Son et al., 2025), encompassing 55 languages, and PolyMath (Wang et al.,\n",
            "2025), which includes 18 languages. Logical reasoning is evaluated using MlogiQA, covering 10\n",
            "languages, sourced from Zhang et al. (2024).\n",
            "For all Qwen3 models in the thinking mode, we utilize a sampling temperature of 0.6, a top-p value\n",
            "of 0.95, and a top-k value of 20. Additionally, for Creative Writing v3 and WritingBench, we apply a\n",
            "presence penalty of 1.5 to encourage the generation of more diverse content. For Qwen3 models in the\n",
            "non-thinking mode, we configure the sampling hyperparameters with temperature = 0.7, top-p = 0.8,\n",
            "top-k = 20, and presence penalty = 1.5. For both the thinking and non-thinking modes, we set the max\n",
            "output length to 32,768 tokens, except AIME’24 and AIME’25 where we extend this length to 38,912\n",
            "tokens to provide sufficient thinking space.\n",
            "13\n",
            "----------\n",
            "Table 11: Comparison among Qwen3-235B-A22B (Thinking) and other reasoning baselines. The\n",
            "highest and second-best scores are shown in bold and underlined, respectively.\n",
            "OpenAI-o1\n",
            "DeepSeek-R1 Grok-3-Beta\n",
            "(Think)\n",
            "Gemini2.5-Pro Qwen3-235B-A22B\n",
            "Architecture\n",
            "-\n",
            "MoE\n",
            "-\n",
            "-\n",
            "MoE\n",
            "# Activated Params\n",
            "-\n",
            "37B\n",
            "-\n",
            "-\n",
            "22B\n",
            "# Total Params\n",
            "-\n",
            "671B\n",
            "-\n",
            "-\n",
            "235B\n",
            "General\n",
            "Tasks\n",
            "MMLU-Redux\n",
            "92.8\n",
            "92.9\n",
            "-\n",
            "93.7\n",
            "92.7\n",
            "GPQA-Diamond\n",
            "78.0\n",
            "71.5\n",
            "80.2\n",
            "84.0\n",
            "71.1\n",
            "C-Eval\n",
            "85.5\n",
            "91.8\n",
            "-\n",
            "82.9\n",
            "89.6\n",
            "LiveBench 2024-11-25\n",
            "75.7\n",
            "71.6\n",
            "-\n",
            "82.4\n",
            "77.1\n",
            "Alignment\n",
            "Tasks\n",
            "IFEval strict prompt\n",
            "92.6\n",
            "83.3\n",
            "-\n",
            "89.5\n",
            "83.4\n",
            "Arena-Hard\n",
            "92.1\n",
            "92.3\n",
            "-\n",
            "96.4\n",
            "95.6\n",
            "AlignBench v1.1\n",
            "8.86\n",
            "8.76\n",
            "-\n",
            "9.03\n",
            "8.94\n",
            "Creative Writing v3\n",
            "81.7\n",
            "85.5\n",
            "-\n",
            "86.0\n",
            "84.6\n",
            "WritingBench\n",
            "7.69\n",
            "7.71\n",
            "-\n",
            "8.09\n",
            "8.03\n",
            "Math & Text\n",
            "Reasoning\n",
            "MATH-500\n",
            "96.4\n",
            "97.3\n",
            "98.8\n",
            "98.0\n",
            "AIME’24\n",
            "74.3\n",
            "79.8\n",
            "83.9\n",
            "92.0\n",
            "85.7\n",
            "AIME’25\n",
            "79.2\n",
            "70.0\n",
            "77.3\n",
            "86.7\n",
            "81.5\n",
            "ZebraLogic\n",
            "81.0\n",
            "78.7\n",
            "-\n",
            "87.4\n",
            "80.3\n",
            "AutoLogi\n",
            "79.8\n",
            "86.1\n",
            "-\n",
            "85.4\n",
            "89.0\n",
            "Agent &\n",
            "Coding\n",
            "BFCL v3\n",
            "67.8\n",
            "56.9\n",
            "-\n",
            "62.9\n",
            "70.8\n",
            "LiveCodeBench v5\n",
            "63.9\n",
            "64.3\n",
            "70.6\n",
            "70.4\n",
            "70.7\n",
            "CodeForces (Rating / Percentile) 1891 / 96.7%\n",
            "2029 / 98.1%\n",
            "-\n",
            "2001 / 97.9%\n",
            "2056 / 98.2%\n",
            "Multilingual\n",
            "Tasks\n",
            "Multi-IF\n",
            "48.8\n",
            "67.7\n",
            "-\n",
            "77.8\n",
            "71.9\n",
            "INCLUDE\n",
            "84.6\n",
            "82.7\n",
            "-\n",
            "85.1\n",
            "78.7\n",
            "MMMLU 14 languages\n",
            "88.4\n",
            "86.4\n",
            "-\n",
            "86.9\n",
            "84.3\n",
            "MT-AIME2024\n",
            "67.4\n",
            "73.5\n",
            "-\n",
            "76.9\n",
            "80.8\n",
            "PolyMath\n",
            "38.9\n",
            "47.1\n",
            "-\n",
            "52.2\n",
            "54.7\n",
            "MLogiQA\n",
            "75.5\n",
            "73.8\n",
            "-\n",
            "75.6\n",
            "77.1\n",
            "Table 12: Comparison among Qwen3-235B-A22B (Non-thinking) and other non-reasoning baselines.\n",
            "The highest and second-best scores are shown in bold and underlined, respectively.\n",
            "GPT-4o\n",
            "-2024-11-20 DeepSeek-V3 Qwen2.5-72B\n",
            "-Instruct\n",
            "LLaMA-4\n",
            "-Maverick\n",
            "Qwen3-235B-A22B\n",
            "Architecture\n",
            "-\n",
            "MoE\n",
            "Dense\n",
            "MoE\n",
            "MoE\n",
            "# Activated Params\n",
            "-\n",
            "37B\n",
            "72B\n",
            "17B\n",
            "22B\n",
            "# Total Params\n",
            "-\n",
            "671B\n",
            "72B\n",
            "402B\n",
            "235B\n",
            "General\n",
            "Tasks\n",
            "MMLU-Redux\n",
            "87.0\n",
            "89.1\n",
            "86.8\n",
            "91.8\n",
            "89.2\n",
            "GPQA-Diamond\n",
            "46.0\n",
            "59.1\n",
            "49.0\n",
            "69.8\n",
            "62.9\n",
            "C-Eval\n",
            "75.5\n",
            "86.5\n",
            "84.7\n",
            "83.5\n",
            "86.1\n",
            "LiveBench 2024-11-25\n",
            "52.2\n",
            "60.5\n",
            "51.4\n",
            "59.5\n",
            "62.5\n",
            "Alignment\n",
            "Tasks\n",
            "IFEval strict prompt\n",
            "86.5\n",
            "86.1\n",
            "84.1\n",
            "86.7\n",
            "83.2\n",
            "Arena-Hard\n",
            "85.3\n",
            "85.5\n",
            "81.2\n",
            "82.7\n",
            "96.1\n",
            "AlignBench v1.1\n",
            "8.42\n",
            "8.64\n",
            "7.89\n",
            "7.97\n",
            "8.91\n",
            "Creative Writing v3\n",
            "81.1\n",
            "74.0\n",
            "61.8\n",
            "61.3\n",
            "80.4\n",
            "WritingBench\n",
            "7.11\n",
            "6.49\n",
            "7.06\n",
            "5.46\n",
            "7.70\n",
            "Math & Text\n",
            "Reasoning\n",
            "MATH-500\n",
            "77.2\n",
            "90.2\n",
            "83.6\n",
            "90.6\n",
            "91.2\n",
            "AIME’24\n",
            "11.1\n",
            "39.2\n",
            "18.9\n",
            "38.5\n",
            "40.1\n",
            "AIME’25\n",
            "7.6\n",
            "28.8\n",
            "15.0\n",
            "15.9\n",
            "24.7\n",
            "ZebraLogic\n",
            "27.4\n",
            "42.1\n",
            "26.6\n",
            "40.0\n",
            "37.7\n",
            "AutoLogi\n",
            "65.9\n",
            "76.1\n",
            "66.1\n",
            "75.2\n",
            "83.3\n",
            "Agent &\n",
            "Coding\n",
            "BFCL v3\n",
            "72.5\n",
            "57.6\n",
            "63.4\n",
            "52.9\n",
            "68.0\n",
            "LiveCodeBench v5\n",
            "32.7\n",
            "33.1\n",
            "30.7\n",
            "37.2\n",
            "35.3\n",
            "CodeForces (Rating / Percentile) 864 / 35.4%\n",
            "1134 / 54.1%\n",
            "859 / 35.0%\n",
            "712 / 24.3%\n",
            "1387 / 75.7%\n",
            "Multilingual\n",
            "Tasks\n",
            "Multi-IF\n",
            "65.6\n",
            "55.6\n",
            "65.3\n",
            "75.5\n",
            "70.2\n",
            "INCLUDE\n",
            "78.8\n",
            "76.7\n",
            "69.6\n",
            "80.9\n",
            "75.6\n",
            "MMMLU 14 languages\n",
            "80.3\n",
            "81.1\n",
            "76.9\n",
            "82.5\n",
            "79.8\n",
            "MT-AIME2024\n",
            "9.2\n",
            "20.9\n",
            "12.7\n",
            "27.0\n",
            "32.4\n",
            "PolyMath\n",
            "13.7\n",
            "20.4\n",
            "16.9\n",
            "26.1\n",
            "27.0\n",
            "MLogiQA\n",
            "57.4\n",
            "58.9\n",
            "59.3\n",
            "59.9\n",
            "67.6\n",
            "14\n",
            "----------\n",
            "Summary of Evaluation Results\n",
            "From the evaluation results, we summarize several key conclusions of\n",
            "the finalized Qwen3 models as follows:\n",
            "(1) Our flagship model, Qwen3-235B-A22B, demonstrates the state-of-the-art overall performance\n",
            "among open-source models in both the thinking and non-thinking modes, surpassing strong\n",
            "baselines such as DeepSeek-R1 and DeepSeek-V3. Qwen3-235B-A22B is also highly competitive\n",
            "to closed-source leading models, such as OpenAI-o1, Gemini2.5-Pro, and GPT-4o, showcasing its\n",
            "profound reasoning capabilities and comprehensive general abilities.\n",
            "(2) Our flagship dense model, Qwen3-32B, outperforms our previous strongest reasoning model,\n",
            "QwQ-32B, in most of the benchmarks, and performs comparably to the closed-source OpenAI-o3-\n",
            "mini, indicating its compelling reasoning capabilities. Qwen3-32B is also remarkably performant\n",
            "in the non-thinking mode and surpasses our previous flagship non-reasoning dense model,\n",
            "Qwen2.5-72B-Instruct.\n",
            "(3) Our lightweight models, including Qwen3-30B-A3B, Qwen3-14B, and other smaller dense ones,\n",
            "possess consistently superior performance to the open-source models with a close or larger\n",
            "amount of parameters, proving the success of our Strong-to-Weak Distillation approach.\n",
            "The detailed results are as follows.\n",
            "Qwen3-235B-A22B\n",
            "For our flagship model Qwen3-235B-A22B, we compare it with the leading reason-\n",
            "ing and non-reasoning models. For the thinking mode, we take OpenAI-o1 (OpenAI, 2024), DeepSeek-R1\n",
            "(Guo et al., 2025), Grok-3-Beta (Think) (xAI, 2025), and Gemini2.5-Pro (DeepMind, 2025) as the reasoning\n",
            "baselines. For the non-thinking mode, we take GPT-4o-2024-11-20 (OpenAI, 2024), DeepSeek-V3 (Liu\n",
            "et al., 2024a), Qwen2.5-72B-Instruct (Yang et al., 2024b), and LLaMA-4-Maverick (Meta-AI, 2025) as the\n",
            "non-reasoning baselines. We present the evaluation results in Table 11 and 12.\n",
            "(1) From Table 11, with only 60% activated and 35% total parameters, Qwen3-235B-A22B (Thinking)\n",
            "outperforms DeepSeek-R1 on 17/23 the benchmarks, particularly on the reasoning-demanded\n",
            "tasks (e.g., mathematics, agent, and coding), demonstrating the state-of-the-art reasoning capabil-\n",
            "ities of Qwen3-235B-A22B among open-source models. Moreover, Qwen3-235B-A22B (Thinking)\n",
            "is also highly competitive to the closed-source OpenAI-o1, Grok-3-Beta (Think), and Gemini2.5-\n",
            "Pro, substantially narrowing the gap in the reasoning capabilities between open-source and\n",
            "close-source models.\n",
            "(2) From Table 12, Qwen3-235B-A22B (Non-thinking) exceeds the other leading open-source models,\n",
            "including DeepSeek-V3, LLaMA-4-Maverick, and our previous flagship model Qwen2.5-72B-\n",
            "Instruct, and also surpasses the closed-source GPT-4o-2024-11-20 in 18/23 the benchmarks,\n",
            "indicating its inherent strong capabilities even when not enhanced with the deliberate thinking\n",
            "process.\n",
            "Qwen3-32B\n",
            "For our flagship dense model, Qwen3-32B, we take DeepSeek-R1-Distill-Llama-70B, OpenAI-\n",
            "o3-mini (medium), and our previous strongest reasoning model, QwQ-32B (Qwen Team, 2025), as the\n",
            "baselines in the thinking mode. We also take GPT-4o-mini-2024-07-18, LLaMA-4-Scout, and our previ-\n",
            "ous flagship model, Qwen2.5-72B-Instruct, as the baselines in the non-thinking mode. We present the\n",
            "evaluation results in Table 13 and 14.\n",
            "(1) From Table 13, Qwen3-32B (Thinking) outperforms QwQ-32B on 17/23 the benchmarks, making\n",
            "it the new state-of-the-art reasoning model at the sweet size of 32B. Moreover, Qwen3-32B (Think-\n",
            "ing) also competes with the closed-source OpenAI-o3-mini (medium) with better alignment and\n",
            "multilingual performance.\n",
            "(2) From Table 14, Qwen3-32B (Non-thinking) exhibits superior performance to all the baselines\n",
            "on almost all the benchmarks. Particularly, Qwen3-32B (Non-thinking) performs on par with\n",
            "Qwen2.5-72B-Instruct on the general tasks with significant advantages on the alignment, multi-\n",
            "lingual, and reasoning-related tasks, again proving the fundamental improvements of Qwen3\n",
            "over our previous Qwen2.5 series models.\n",
            "Qwen3-30B-A3B & Qwen3-14B\n",
            "For Qwen3-30B-A3B and Qwen3-14B, we compare them with DeepSeek-\n",
            "R1-Distill-Qwen-32B and QwQ-32B in the thinking mode, and Phi-4 (Abdin et al., 2024), Gemma-3-27B-IT\n",
            "(Team et al., 2025), and Qwen2.5-32B-Instruct in the non-thinking mode, respectively. We present the\n",
            "evaluation results in Table 15 and 16.\n",
            "(1) From Table 15, Qwen3-30B-A3B and Qwen3-14B (Thinking) are both highly competitive to\n",
            "QwQ-32B, especially on the reasoning-related benchmarks. It is noteworthy that Qwen3-30B-\n",
            "A3B achieves comparable performance to QwQ-32B with a smaller model size and less than\n",
            "15\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "for i in range(10,15):\n",
        "    print(pages[i].page_content)\n",
        "    print('----------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of0NCi61Ln6A"
      },
      "source": [
        "각각의 Document를 하나로 합쳐, 하나의 큰 Document를 만들고 청킹을 수행하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydv93CT1LeYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0de62493-0eb7-4e22-a7fd-f7bafc708b1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "116009"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "# Document 클래스 만들기\n",
        "\n",
        "corpus = Document(page_content='', metadata={'source':'./교재.pdf'})\n",
        "for page in pages:\n",
        "    corpus.page_content += page.page_content + '\\n'\n",
        "\n",
        "corpus.page_content = corpus.page_content.replace('\\n\\n','\\n')\n",
        "len(corpus.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHLX6_DENX3x"
      },
      "source": [
        "Text Splitter를 이용해 청크로 분리합니다.   \n",
        "이번에는 토큰 단위로 분리해 보겠습니다.    \n",
        "TextSplitter의 `.from_tiktoken_encoder`를 이용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dJNuoxLNZtQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d8318a-9bd4-47b0-ec90-e3a7fdb9931f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import tiktoken\n",
        "\n",
        "token_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    # GPT-4o (= GPT 4.1 = GPT 5) 기준 1000/200 토큰 청킹\n",
        ")\n",
        "\n",
        "token_chunks = token_splitter.split_documents([corpus])\n",
        "print(len(token_chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_Ji6HSwDDrP"
      },
      "source": [
        "전체 데이터를 Chroma에 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7D0khLSB-Cc"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-3-large')\n",
        "Chroma().delete_collection()\n",
        "db = Chroma.from_documents(token_chunks, embeddings)\n",
        "\n",
        "\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 5})\n",
        "# Top K search 옵션 정하기\n",
        "\n",
        "\n",
        "# filter 옵션을 통해 특정 메타데이터를 가진 벡터만 검색 가능\n",
        "# Ex) author가 Hyungho Byun인 Document만 검색\n",
        "# retriever = db.as_retriever(search_kwargs={\"k\": 5,\"filter\":{'author':'Hyungho Byun'}})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqEIdYfhB-Cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a6f4570-844d-4611-e35e-8068c435b2f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(id='88c55bbd-0719-45e0-b628-e92a4f4047c5', metadata={'source': './교재.pdf'}, page_content='Qwen2.5-14B-Instruct\\n91.4\\n97.7\\n96.8\\n95.9\\n93.4\\n86.7\\n78.1\\nQwen2.5-32B-Instruct\\n92.9\\n96.9\\n97.1\\n95.5\\n95.5\\n90.3\\n82.0\\nQwen2.5-72B-Instruct\\n95.1\\n97.7\\n97.2\\n97.7\\n96.5\\n93.0\\n88.4\\nNon-thinking\\nMode\\nQwen3-4B\\n85.2\\n95.1\\n93.6\\n91.0\\n87.8\\n77.8\\n66.0\\nQwen3-8B\\n89.1\\n96.3\\n96.0\\n91.8\\n91.2\\n82.1\\n77.4\\nQwen3-14B\\n94.6\\n98.0\\n97.8\\n96.4\\n96.1\\n94.0\\n85.1\\nQwen3-32B\\n93.7\\n98.4\\n96.0\\n96.2\\n94.4\\n91.8\\n85.6\\nQwen3-30B-A3B\\n91.6\\n96.5\\n97.0\\n95.3\\n92.4\\n89.1\\n79.2\\nQwen3-235B-A22B\\n95.0\\n97.7\\n97.2\\n96.4\\n95.1\\n93.3\\n90.6\\nThinking\\nMode\\nQwen3-4B\\n83.5\\n92.7\\n88.7\\n86.5\\n83.2\\n83.0\\n67.2\\nQwen3-8B\\n84.4\\n94.7\\n94.4\\n86.1\\n80.8\\n78.3\\n72.0\\nQwen3-14B\\n90.1\\n95.4\\n93.6\\n89.8\\n91.9\\n90.6\\n79.0\\nQwen3-32B\\n91.0\\n94.7\\n93.7\\n91.6\\n92.5\\n90.0\\n83.5\\nQwen3-30B-A3B\\n86.6\\n94.1\\n92.7\\n89.0\\n86.6\\n82.1\\n75.0\\nQwen3-235B-A22B\\n92.2\\n95.1\\n94.8\\n93.0\\n92.3\\n92.0\\n86.0\\nFor evaluating long-context processing capabilities, we report the results on the RULER benchmark (Hsieh\\net al., 2024) in Table 23. To enable length extrapolation, we utilize YARN (Peng et al., 2023) with a\\nscaling factor=4. In thinking mode, we set the thinking budget to 8192 tokens to mitigate overly\\nverbose reasoning on the extremely long inputs.\\nThe results show that:\\n1. In non-thinking mode, Qwen3 outperforms Qwen2.5 models of a similar size in long-context\\nprocessing tasks.\\n2. In thinking mode, the model’s performance slightly degrades. We hypothesize that the thinking\\ncontent does not provide significant benefits for these retrieval tasks, which do not rely on\\nreasoning and may instead interfere with the retrieval process. We are committed to enhancing\\nthe long-context capability in the thinking mode in future versions.\\nA.1.2\\nMultilingual Ability\\nTable 24-35 presents the detailed benchmark scores across various languages, including Spanish, French,\\nPortuguese, Italian, Arabic, Japanese, Korean, Indonesian, Russian, Vietnamese, German, and Thai. The\\nresults of these tables demonstrate that the Qwen3 series models achieve competitive performance across\\nall evaluated benchmarks, showcasing their strong multilingual capabilities.\\nTo evaluate the performance of Qwen3 across a broader range of languages, we utilize Belebele (Bandarkar\\net al., 2023), a benchmark for natural language understanding. We conduct evaluations on 80 supported\\nlanguages from the benchmark, excluding 42 unoptimized languages, as shown in Table 36 (organized\\nby language family). The performance comparison between Qwen3 and other baseline models on\\nthe Belebele benchmark is presented in Table 37. The results show that Qwen3 achieves comparable\\nperformance to similarly-sized Gemma models while outperforming Qwen2.5 significantly.\\n23\\nTable 24: Benchmark scores for language: Spanish (es). The highest and second-best scores are shown\\nin bold and underlined, respectively.\\nModel'), Document(id='3a7cc126-be43-416a-8c0d-777c62a38201', metadata={'source': './교재.pdf'}, page_content='16.7\\n19.3\\n45.0\\nQwen3-235B-A22B\\n71.4\\n81.7\\n40.0\\n25.9\\n54.8\\nQwen3-32B\\n57.1\\n77.2\\n30.0\\n21.9\\n46.6\\nQwen3-30B-A3B\\n57.1\\n77.7\\n23.3\\n25.2\\n45.8\\nQwen3-14B\\n57.1\\n76.0\\n30.0\\n24.5\\n46.9\\nQwen3-8B\\n64.3\\n70.8\\n20.0\\n19.9\\n43.8\\nQwen3-4B\\n64.3\\n66.0\\n26.7\\n16.4\\n43.4\\nQwen3-1.7B\\n42.9\\n53.2\\n10.0\\n10.6\\n29.2\\nQwen3-0.6B\\n42.9\\n37.8\\n3.3\\n5.7\\n22.4\\nTable 35: Benchmark scores for language: Thai (th). The highest and second-best scores are shown in\\nbold and underlined, respectively.\\nModel\\nMLogiQA MT-AIME24 PolyMath Average\\nThinking\\nMode\\nGemini2.5-Pro\\n73.8\\n80.0\\n50.7\\n68.2\\nQwQ-32B\\n75.0\\n60.0\\n41.3\\n58.8\\nQwen3-235B-A22B\\n73.8\\n86.7\\n53.6\\n71.4\\nQwen3-32B\\n73.8\\n76.7\\n46.9\\n65.8\\nQwen3-30B-A3B\\n63.7\\n80.0\\n45.2\\n63.0\\nQwen3-14B\\n65.0\\n76.7\\n44.4\\n62.0\\nQwen3-8B\\n68.8\\n70.0\\n41.3\\n60.0\\nQwen3-4B\\n60.0\\n60.0\\n39.4\\n53.1\\nQwen3-1.7B\\n48.8\\n33.3\\n23.7\\n35.3\\nQwen3-0.6B\\n33.8\\n13.3\\n11.4\\n19.5\\nNon-thinking\\nMode\\nGPT-4o-2024-1120\\n52.5\\n10.0\\n11.9\\n24.8\\nGemma-3-27b-IT\\n50.0\\n16.7\\n19.0\\n28.6\\nQwen2.5-72B-Instruct\\n58.8\\n6.7\\n17.4\\n27.6\\nQwen3-235B-A22B\\n61.3\\n23.3\\n27.6\\n37.4\\nQwen3-32B\\n61.3\\n13.3\\n22.2\\n32.3\\nQwen3-30B-A3B\\n50.0\\n30.0\\n22.3\\n34.1\\nQwen3-14B\\n47.5\\n23.3\\n22.1\\n31.0\\nQwen3-8B\\n42.5\\n10.0\\n17.2\\n23.2\\nQwen3-4B\\n43.8\\n13.3\\n16.1\\n24.4\\nQwen3-1.7B\\n42.5\\n6.7\\n9.5\\n19.6\\nQwen3-0.6B\\n37.5\\n0.0\\n3.6\\n13.7\\n29\\nTable 36: Language families and language codes supported by Qwen3 in Belebele Benchmark\\nLanguage family # Langs Language code (ISO 639-3 ISO 15924)\\nIndo-European\\n40\\npor Latn, deu Latn, tgk Cyrl, ces Latn, nob Latn, dan Latn, snd Arab, spa Latn,\\nisl Latn, slv Latn, eng Latn, ory Orya, hrv Latn, ell Grek, ukr Cyrl, pan Guru,\\nsrp Cyrl, npi Deva, mkd Cyrl, guj Gujr, nld Latn, swe Latn, hin Deva, rus Cyrl,\\nasm Beng, cat Latn, als Latn, sin Sinh, urd Arab, mar Deva, lit Latn, slk Latn,'), Document(id='5e43f43e-46f4-4691-afda-6ec8a8450aa2', metadata={'source': './교재.pdf'}, page_content='(Meta-AI, 2025), Qwen2.5-72B-Base (Yang et al., 2024b), DeepSeek-V3 Base (Liu et al., 2024a). From\\nthe results in Table 3, the Qwen3-235B-A22B-Base model attains the highest performance scores across\\nmost of the evaluated benchmarks. We further compare Qwen3-235B-A22B-Base with other baselines\\nseparately for the detailed analysis.\\n(1) Compared with the recently open-source model Llama-4-Maverick-Base, which has about twice\\nthe number of parameters, Qwen3-235B-A22B-Base still performs better on most benchmarks.\\n(2) Compared with the previously state-of-the-art open-source model DeepSeek-V3-Base, Qwen3-\\n235B-A22B-Base outperforms DeepSeek-V3-Base on 14 out of 15 evaluation benchmarks with\\nonly about 1/3 the total number of parameters and 2/3 activated parameters, demonstrating the\\npowerful and cost-effectiveness of our models.\\n(3) Compared with our previous MoE Qwen2.5-Plus of similar size, Qwen3-235B-A22B-Base sig-\\nnificantly outperforms it with fewer parameters and activated parameters, which shows the\\nremarkable advantages of Qwen3 in pre-training data, training strategy, and model architecture.\\n(4) Compared with our previous flagship open-source dense model Qwen2.5-72B-Base, Qwen3-\\n235B-A22B-Base surpasses the latter in all benchmarks and uses fewer than 1/3 of the activated\\nparameters. Meanwhile, due to the advantage of the model architecture, the inference costs and\\ntraining costs on each trillion tokens of Qwen3-235B-A22B-Base are much cheaper than those of\\nQwen2.5-72B-Base.\\nQwen3-32B-Base\\nQwen3-32B-Base is our largest dense model among the Qwen3 series. We compare\\nit to the baselines of similar sizes, including Gemma-3-27B (Team et al., 2025) and Qwen2.5-32B (Yang\\net al., 2024b). In addition, we introduce two strong baselines: the recently open-source MoE model Llama-\\n4-Scout, which has three times the parameters of Qwen3-32B-Base but half the activated parameters;\\n5\\nTable 3: Comparison among Qwen3-235B-A22B-Base and other representative strong open-source\\nbaselines. The highest, the second-best scores are shown in bold and underlined, respectively.\\nQwen2.5-72B Qwen2.5-Plus Llama-4-Maverick DeepSeek-V3 Qwen3-235B-A22B\\nBase\\nBase\\nBase\\nBase\\nBase\\nArchitecture\\nDense\\nMoE\\nMoE\\nMoE\\nMoE\\n# Total Params\\n72B\\n271B\\n402B\\n671B\\n235B\\n# Activated Params\\n72B\\n37B\\n17B\\n37B\\n22B\\nGeneral Tasks\\nMMLU\\n86.06\\n85.02\\n85.16\\n87.19\\n87.81\\nMMLU-Redux\\n83.91\\n82.69\\n84.05\\n86.14\\n87.40\\nMMLU-Pro\\n58.07\\n63.52\\n63.91\\n59.84\\n68.18\\nSuperGPQA\\n36.20\\n37.18\\n40.85\\n41.53\\n44.06\\nBBH\\n86.30\\n85.60\\n83.62\\n86.22\\n88.87\\nMath & STEM Tasks\\nGPQA\\n45.88\\n41.92\\n43.94\\n41.92\\n47.47\\nGSM8K\\n91.50\\n91.89\\n87.72\\n87.57\\n94.39\\nMATH\\n62.12\\n62.78\\n63.32\\n62.62\\n71.84\\nCoding Tasks\\nEvalPlus\\n65.93\\n61.43\\n68.38\\n63.75\\n77.60\\nMultiPL-E\\n58.70\\n62.16\\n57.28\\n62.26\\n65.94\\nMBPP\\n76.00\\n74.60\\n75.40\\n74.20\\n81.40\\nCRUX-O\\n66.20\\n68.50\\n77.00\\n76.60\\n79.00\\nMultilingual Tasks'), Document(id='e319bd34-c573-4bc2-adb1-591587c9ba09', metadata={'source': './교재.pdf'}, page_content='These detailed annotations support more effective data filtering and combination. Unlike previous\\nstudies (Xie et al., 2023; Fan et al., 2023; Liu et al., 2024b) that optimize the data mixture at the data source\\nor domain level, our method optimizes the data mixture at the instance-level through extensive ablation\\nexperiments on small proxy models with the fine-grained data labels.\\n3.2\\nPre-training Stage\\nThe Qwen3 models are pre-trained through a three-stage process:\\n(1) General Stage (S1): At the first pre-training stage, all Qwen3 models are trained on over 30\\ntrillion tokens using a sequence length of 4,096 tokens. At this stage, the models have been fully\\npre-trained on language proficiency and general world knowledge, with training data covering\\n119 languages and dialects.\\n(2) Reasoning Stage (S2): To further improve the reasoning ability, we optimize the pre-training\\ncorpus of this stage by increasing the proportion of STEM, coding, reasoning, and synthetic data.\\nThe models are further pre-trained with about 5T higher-quality tokens at a sequence length of\\n4,096 tokens. We also accelerate the learning rate decay during this stage.\\n(3) Long Context Stage: In the final pre-training stage, we collect high-quality long context corpora\\nto extend the context length of Qwen3 models. All models are pre-trained on hundreds of billions\\nof tokens with a sequence length of 32,768 tokens. The long context corpus includes 75% of text\\nbetween 16,384 to 32,768 tokens in length, and 25% of text between 4,096 to 16,384 in length.\\nFollowing Qwen2.5 (Yang et al., 2024b), we increase the base frequency of RoPE from 10,000 to\\n1,000,000 using the ABF technique (Xiong et al., 2023). Meanwhile, we introduce YARN (Peng\\net al., 2023) and Dual Chunk Attention (DCA, An et al., 2024) to achieve a four-fold increase in\\nsequence length capacity during inference.\\nSimilar to Qwen2.5 (Yang et al., 2024b), we develop scaling laws for optimal hyper-parameters (e.g.,\\nlearning rate scheduler, and batch size) predictions based on three pre-training stages mentioned above.\\nThrough extensive experiments, we systematically study the relationship between model architecture,\\ntraining data, training stage, and optimal training hyper-parameters. Finally, we set the predicted optimal\\nlearning rate and batch size strategy for each dense or MoE model.\\n3.3\\nPre-training Evaluation\\nWe conduct comprehensive evaluations of the base language models of the Qwen3 series. The evaluation\\nof base models mainly focuses on their performance in general knowledge, reasoning, mathematics,\\nscientific knowledge, coding, and multilingual capabilities. The evaluation datasets for pre-trained base\\nmodels include 15 benchmarks:\\n• General Tasks: MMLU (Hendrycks et al., 2021a) (5-shot), MMLU-Pro (Wang et al., 2024) (5-\\nshot, CoT), MMLU-redux (Gema et al., 2024) (5-shot), BBH (Suzgun et al., 2023) (3-shot, CoT),\\nSuperGPQA (Du et al., 2025)(5-shot, CoT).\\n• Math & STEM Tasks: GPQA (Rein et al., 2023) (5-shot, CoT), GSM8K (Cobbe et al., 2021) (4-shot,\\nCoT), MATH (Hendrycks et al., 2021b) (4-shot, CoT).\\n4\\n• Coding Tasks: EvalPlus (Liu et al., 2023a) (0-shot) (Average of HumanEval (Chen et al., 2021),\\nMBPP (Austin et al., 2021), Humaneval+, MBPP+) (Liu et al., 2023a), MultiPL-E (Cassano et al.,\\n2023) (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript), MBPP-3shot (Austin\\net al., 2021), CRUX-O of CRUXEval (1-shot) (Gu et al., 2024).\\n• Multilingual Tasks: MGSM (Shi et al., 2023) (8-shot, CoT), MMMLU (OpenAI, 2024) (5-shot),'), Document(id='1a47fa73-4f85-4fcc-844c-5510f909d29d', metadata={'source': './교재.pdf'}, page_content='a collection of open-weight large language models (LLMs) that achieve state-of-the-art performance\\nacross a wide variety of tasks and domains. We release both dense and Mixture-of-Experts (MoE) models,\\nwith the number of parameters ranging from 0.6 billion to 235 billion, to meet the needs of different\\ndownstream applications. Notably, the flagship model, Qwen3-235B-A22B, is an MoE model with a\\ntotal of 235 billion parameters and 22 billion activated ones per token. This design ensures both high\\nperformance and efficient inference.\\nQwen3 introduces several key advancements to enhance its functionality and usability. First, it integrates\\ntwo distinct operating modes, thinking mode and non-thinking mode, into a single model. This allows\\nusers to switch between these modes without alternating between different models, e.g., switching from\\nQwen2.5 to QwQ (Qwen Team, 2024). This flexibility ensures that developers and users can adapt the\\nmodel’s behavior to suit specific tasks efficiently. Additionally, Qwen3 incorporates thinking budgets, pro-\\nviding users with fine-grained control over the level of reasoning effort applied by the model during task\\nexecution. This capability is crucial to the optimization of computational resources and performance, tai-\\nloring the model’s thinking behavior to meet varying complexity in real-world applications. Furthermore,\\nQwen3 has been pre-trained on 36 trillion tokens covering up to 119 languages and dialects, effectively\\nenhancing its multilingual capabilities. This broadened language support amplifies its potential for\\ndeployment in global use cases and international applications. These advancements together establish\\nQwen3 as a cutting-edge open-source large language model family, capable of effectively addressing\\ncomplex tasks across various domains and languages.\\nThe pre-training process for Qwen3 utilizes a large-scale dataset consisting of approximately 36 trillion\\ntokens, curated to ensure linguistic and domain diversity. To efficiently expand the training data, we\\nemploy a multi-modal approach: Qwen2.5-VL (Bai et al., 2025) is finetuned to extract text from extensive\\nPDF documents. We also generate synthetic data using domain-specific models: Qwen2.5-Math (Yang\\net al., 2024c) for mathematical content and Qwen2.5-Coder (Hui et al., 2024) for code-related data. The\\npre-training process follows a three-stage strategy. In the first stage, the model is trained on about 30\\ntrillion tokens to build a strong foundation of general knowledge. In the second stage, it is further trained\\non knowledge-intensive data to enhance reasoning abilities in areas like science, technology, engineering,\\nand mathematics (STEM) and coding. Finally, in the third stage, the model is trained on long-context\\ndata to increase its maximum context length from 4,096 to 32,768 tokens.\\nTo better align foundation models with human preferences and downstream applications, we employ a\\nmulti-stage post-training approach that empowers both thinking (reasoning) and non-thinking modes. In\\nthe first two stages, we focus on developing strong reasoning abilities through long chain-of-thought\\n(CoT) cold-start finetuning and reinforcement learning focusing on mathematics and coding tasks. In the\\nfinal two stages, we combine data with and without reasoning paths into a unified dataset for further\\nfine-tuning, enabling the model to handle both types of input effectively, and we then apply general-\\ndomain reinforcement learning to improve performance across a wide range of downstream tasks. For\\nsmaller models, we use strong-to-weak distillation, leveraging both off-policy and on-policy knowledge\\ntransfer from larger models to enhance their capabilities. Distillation from advanced teacher models\\nsignificantly outperforms reinforcement learning in performance and training efficiency.\\nWe evaluate both pre-trained and post-trained versions of our models across a comprehensive set of\\nbenchmarks spanning multiple tasks and domains. Experimental results show that our base pre-trained\\nmodels achieve state-of-the-art performance. The post-trained models, whether in thinking or non-\\nthinking mode, perform competitively against leading proprietary models and large mixture-of-experts\\n(MoE) models such as o1, o3-mini, and DeepSeek-V3. Notably, our models excel in coding, mathematics,\\nand agent-related tasks. For example, the flagship model Qwen3-235B-A22B achieves 85.7 on AIME’24\\n2\\nand 81.5 on AIME’25 (AIME, 2025), 70.7 on LiveCodeBench v5 (Jain et al., 2024), 2,056 on CodeForces,')]\n"
          ]
        }
      ],
      "source": [
        "# Query 검색\n",
        "unique_docs = retriever.invoke(\"LangChain의 장점은?\")\n",
        "\n",
        "print(unique_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK5H6ZOGVdwa"
      },
      "source": [
        "## [실습] PDF 요약 체인 만들기    \n",
        "\n",
        "Prompt와 Chain을 구성하여, 강의 자료에 대한 질의응답을 수행하는 RAG 체인을 만들고 실행하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_euTplXDVdwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25bf339c-e07f-4810-830b-3579ca955009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "당신은 Qwen 3 Technical Report에 대해 QA(Question-Answer)을 수행하는 Assistant입니다.\n",
            "다음의 Context를 이용하여 Question에 답변하세요.\n",
            "정확한 답변을 제공하세요.\n",
            "만약 모든 Context를 다 확인해도 정보가 없다면,\n",
            "\"정보가 부족하여 답변할 수 없습니다.\"를 출력하세요.\n",
            "---\n",
            "Context: \u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
            "---\n",
            "Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate([\n",
        "    (\"user\", '''당신은 Qwen 3 Technical Report에 대해 QA(Question-Answer)을 수행하는 Assistant입니다.\n",
        "다음의 Context를 이용하여 Question에 답변하세요.\n",
        "정확한 답변을 제공하세요.\n",
        "만약 모든 Context를 다 확인해도 정보가 없다면,\n",
        "\"정보가 부족하여 답변할 수 없습니다.\"를 출력하세요.\n",
        "---\n",
        "Context: {context}\n",
        "---\n",
        "Question: {question}''')])\n",
        "\n",
        "prompt.pretty_print()\n",
        "# Prompt : 한국어 질문 전달하"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "translate_prompt = ChatPromptTemplate(\n",
        "    [\"다음 질문을 그대로 영어로 번역하세요: {query}\"]\n",
        ")\n",
        "translate_chain = translate_prompt | llm | StrOutputParser()\n",
        "\n",
        "translate_chain.invoke(\"큐웬 3 컨텍스트 윈도우 어떻게 돼?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BYJOaIOgxHD_",
        "outputId": "8dc5023e-b94a-4120-f50e-047671a1da0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How many context windows does Qwen 3 have?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPIUr6gPVdwa"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# retriever의 결과물은 List[Document] 이므로 이를 ---로 구분하는 함수\n",
        "# metadata의 source를 보존하여 추가\n",
        "def format_docs(docs):\n",
        "    return \" \\n---\\n \".join(['Content: '+ doc.page_content+ '\\n' for doc in docs])\n",
        "    # join : 구분자를 기준으로 스트링 리스트를 하나의 스트링으로 연결\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": translate_chain | retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    # retriever : question을 받아서 context 검색: document 반환\n",
        "    # format_docs : document 형태를 받아서 텍스트로 변환\n",
        "    # RunnablePassthrough(): 체인의 입력을 그대로 저장\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq2FsUBJVdwa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "23204b28-6fbf-4a55-f48f-5ae6fbb67023"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Qwen3 모델의 컨텍스트 길이는 모델에 따라 다르며, 다음과 같습니다.\\n\\n- Qwen3-0.6B, Qwen3-1.7B 모델: 32K (32,768 토큰)\\n- Qwen3-4B, Qwen3-8B, Qwen3-14B, Qwen3-32B 모델: 128K (131,072 토큰)\\n- Qwen3 MoE 모델 (Qwen3-30B-A3B, Qwen3-235B-A22B): 128K (131,072 토큰)\\n\\n즉, Qwen3 모델들은 최대 128K 토큰의 긴 컨텍스트를 지원합니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# 테스트\n",
        "\n",
        "rag_chain.invoke(\"Qwen 3의 컨텍스트 길이는 어떻게 되니?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV5W9UK_Vdwb"
      },
      "source": [
        "Top-K 기반의 RAG도 문서의 내용을 바탕으로 답변하지만,    \n",
        "특정 문제에 대해서는 단일 검색이 아닌 전체 문서를 확인해야 하는 경우가 존재합니다.   \n",
        "\n",
        "Chunking을 활용하여, PDF 파일을 요약해 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSOqIBzXU6f_",
        "outputId": "ff32dcb4-54ef-4887-8724-3166f4f4ac3f"
      },
      "source": [
        "## 요약(Summarization)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7HEI2ggB-Cd"
      },
      "source": [
        "### 1. Stuff : 전체 문서를 다 넣고 요약하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPvtexxwB-Cd"
      },
      "source": [
        "가장 간단한 요약 방법입니다.   \n",
        "문서의 길이가 Context 길이보다 큰 경우에는 실행이 어렵습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXz-zX9rB-Cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8819024-bc64-4313-b11d-e255322046f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qwen3는 Qwen 모델 시리즈의 최신 버전으로, 성능, 효율성, 다국어 지원 능력을 크게 향상시킨 대규모 언어 모델(LLM)이다. Qwen3는 0.6억에서 235억 파라미터 규모의 조밀(dense) 및 전문가 혼합(MoE) 아키텍처 모델을 포함하며, 복잡한 다단계 추론을 위한 '생각 모드'와 신속한 응답을 위한 '비생각 모드'를 하나의 통합 프레임워크로 제공한다. 사용자는 작업의 복잡도에 따라 계산 자원을 조절하는 '생각 예산' 기능을 활용할 수 있으며, 이전 모델 대비 29개에서 119개 언어로 다국어 지원 범위를 확대하여 글로벌 활용성을 높였다. 모든 Qwen3 모델은 Apache 2.0 라이선스로 공개되어 연구와 개발의 투명성과 재현성을 보장한다.\n",
            "\n",
            "Qwen3의 학습에는 36조 개의 토큰 규모의 대규모 다국어 및 다분야 데이터셋이 사용되었으며, PDF 문서에서 텍스트를 추출하는 멀티모달 기법과 수학·코딩 전문 모델을 활용한 합성 데이터 생성으로 데이터 다양성과 품질을 크게 확장하였다. 사전학습은 일반 지식 학습, 추론 능력 강화, 그리고 최대 32,768 토큰의 장문 처리 능력 향상을 위한 3단계로 진행되었고, 모델 구조는 안정적인 학습을 위한 QK-Norm 도입과 고성능을 위한 세부 개선이 포함되었다. 또한, Qwen3는 다양한 벤치마크에서 기존 오픈소스 및 폐쇄형 모델과 경쟁하거나 이를 능가하는 최첨단 성능을 입증하였다.\n",
            "\n",
            "사후학습(post-training)은 생각 모드와 비생각 모드를 통합하고, 소규모 모델의 효율적 개발을 위한 '강-약 지식 증류(Strong-to-Weak Distillation)'를 중심으로 설계되었다. 네 단계로 구성된 훈련 과정은 긴 사슬 추론(Chain-of-Thought) 학습, 강화 학습을 통한 추론 능력 향상, 모드 통합 및 일반 도메인 강화 학습을 포함한다. 이를 통해 Qwen3는 사용자가 모드를 자유롭게 전환하고, 추론 깊이를 설정할 수 있게 하며, 소형 모델도 고성능을 유지하면서 개발 비용과 시간을 크게 절감할 수 있다.\n",
            "\n",
            "평가 결과, Qwen3-235B-A22B는 오픈소스 모델 중 최상위 성능을 기록하며 GPT-4o, Gemini2.5-Pro 등 폐쇄형 모델과 견줄 만한 추론 능력을 보였다. 또한, 32B 크기의 조밀 모델 Qwen3-32B와 경량 모델들 역시 강력한 경쟁력을 갖추었으며, 특히 수학, 코딩, 다국어 처리, 에이전트 작업에서 두각을 나타냈다. 생각 예산을 늘림에 따라 성능이 꾸준히 향상되는 점과, 사후학습 과정에서 지식 증류가 강화 학습 대비 성능과 효율성에서 우수함이 확인되었다. 향후 연구는 데이터 품질 향상, 모델 아키텍처 개선, 초장문 처리 확장, 그리고 환경 피드백을 활용한 에이전트 강화 학습에 집중할 계획이다.\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\" Qwen3의 Technical Report가 아래에 주어집니다.\n",
        "내용을 요약하여 아래에 출력하세요.\n",
        "요약은 4개의 문단과 문단별 5개의 문장으로 작성하세요.\n",
        "---\n",
        "{text}\n",
        "---\n",
        "요약:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template = prompt_template)\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "answer = chain.invoke({'text':corpus.page_content})\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eTmteJPB-Ce"
      },
      "source": [
        "## 2. map_reduce\n",
        "map_reduce는 청크 단위로 요약을 만들고, 이를 합친 결과를 이용해 최종 요약문을 다시 작성합니다.   \n",
        "\n",
        "map 과정과 combine 과정이 존재하며, 각각 다른 프롬프트를 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4_dIaC0B-Ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "421d687e-8ef9-4f29-c39d-19af83ffc442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1/55 [00:08<07:50,  8.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 0\n",
            "Qwen3는 Qwen 모델 시리즈의 최신 버전으로, 성능, 효율성, 다국어 지원을 크게 향상시킨 대형 언어 모델(LLM)들로 구성되어 있다. 이 시리즈는 0.6억에서 2350억 파라미터 규모의 밀집형(dense) 및 전문가 혼합(MoE) 아키텍처 모델을 포함하며, 복잡한 다단계 추론을 위한 '사고 모드'와 빠른 맥락 기반 응답을 위한 '비사고 모드'를 통합하여 하나의 통합된 프레임워크 내에서 동적 전환이 가능하다. 또한, 사용자가 추론 시 계산 자원을 적응적으로 할당할 수 있는 '사고 예산' 메커니즘을 도입해 작업 복잡도에 따라 지연 시간과 성능의 균형을 맞출 수 있다.\n",
            "\n",
            "Qwen3는 주요 모델의 지식을 활용해 소규모 모델 구축에 필요한 계산 자원을 크게 줄이면서도 경쟁력 있는 성능을 유지하고, 코드 생성, 수학적 추론, 에이전트 작업 등 다양한 벤치마크에서 최첨단 결과를 달성한다. 전작 Qwen2.5와 비교해 지원 언어 수를 29개에서 119개로 대폭 확대하여 다국어 이해 및 생성 능력을 강화함으로써 글로벌 접근성을 향상시켰다. 모든 Qwen3 모델은 Apache 2.0 라이선스 하에 공개되어 연구 및 개발 커뮤니티의 재현성과 협업을 적극 지원한다.\n",
            "\n",
            "이 보고서는 AGI 목표를 향한 대형 기초 모델의 발전 맥락에서 Qwen3의 설계 철학과 기술적 진보를 설명하며, 대규모 데이터와 강화학습을 활용한 추론 최적화, 오픈소스 생태계에서의 역할 강화 등을 강조한다. 특히, 2350억 파라미터 MoE 모델인 Qwen3-235B-A22B는 높은 성능과 효율적인 추론을 동시에 달성하도록 설계되어 다양한 다운스트림 응용에 적합하다. Qwen3는 사용자 요구에 따라 사고 모드와 비사고 모드를 자유롭게 전환 가능케 하여 다양한 사용 시나리오에 유연하게 대응한다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▎         | 2/55 [00:17<07:32,  8.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 1\n",
            "Qwen3는 다양한 작업과 분야에서 최첨단 성능을 발휘하는 오픈 웨이트 대형 언어 모델(LLM) 모음으로, 0.6억에서 2350억 매개변수까지 다양한 크기의 밀집형 및 Mixture-of-Experts(MoE) 모델을 제공한다. 대표 모델인 Qwen3-235B-A22B는 2350억 매개변수와 토큰당 220억 활성화 매개변수를 갖춘 MoE 구조로, 높은 성능과 효율적인 추론을 동시에 달성한다. 이 모델은 사고 모드와 비사고 모드라는 두 가지 작동 모드를 하나의 모델에 통합하여, 사용자가 작업에 맞게 유연하게 모드 전환을 할 수 있도록 지원한다.\n",
            "\n",
            "Qwen3는 약 36조 개 토큰, 119개 언어 및 방언을 포함하는 대규모 다국어 데이터로 사전학습되었으며, 멀티모달 접근 방식을 통해 PDF 문서에서 텍스트를 추출하고 수학 및 코딩 관련 합성 데이터를 생성하는 등 훈련 데이터를 확장했다. 사전학습은 일반 지식을 쌓는 1단계, STEM 및 코딩 관련 추론 능력을 강화하는 2단계, 그리고 최대 컨텍스트 길이를 4,096에서 32,768 토큰으로 확장하는 3단계로 이루어진다. 이러한 단계적 학습 전략은 모델의 전반적 성능과 다중 언어 처리 능력을 크게 향상시킨다.\n",
            "\n",
            "사후학습에서는 장기 체인오브생각(CoT) 및 강화학습을 활용해 추론 능력을 극대화하고, 사고 및 비사고 모드를 모두 다룰 수 있도록 데이터 통합 및 일반 도메인 강화학습을 수행한다. 소형 모델들은 대형 모델로부터 지식을 전이하는 강한-약한(distillation) 방식을 통해 효율적으로 성능을 높였다. 평가 결과, Qwen3의 사전학습 및 사후학습 모델 모두 다양한 벤치마크에서 최첨단 성능을 기록했으며, 특히 코딩, 수학, 에이전트 관련 작업에서 뛰어난 성과를 보였다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 3/55 [00:24<06:47,  7.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 2\n",
            "Qwen3 시리즈는 강화학습 대비 성능과 학습 효율에서 크게 앞서며, 다양한 작업과 도메인에 걸친 벤치마크에서 뛰어난 성능을 입증했다. 기본 사전학습 모델은 최첨단 성능을 달성했고, 사후학습된 모델은 사고 모드 여부와 관계없이 주요 상용 모델 및 대규모 전문가 혼합(MoE) 모델들과 경쟁력 있는 결과를 보였다. 특히 코딩, 수학, 에이전트 관련 작업에서 두드러진 성과를 냈으며, 대표 모델인 Qwen3-235B-A22B는 AIME, LiveCodeBench, CodeForces 등에서 높은 점수를 기록했다. 더불어 사고 토큰에 할당되는 예산을 늘리면 다양한 작업에서 일관된 성능 향상이 관찰되었다. 보고서에서는 모델 구조, 학습 절차, 실험 결과 등을 상세히 다루고 있으며 향후 연구 방향도 제시한다.\n",
            "\n",
            "Qwen3 모델군은 6개의 밀집(dense) 모델과 2개의 MoE 모델로 구성되며, 대표 모델 Qwen3-235B-A22B는 총 2350억 개 파라미터 중 220억 개가 활성화된다. 밀집 모델은 Qwen2.5의 구조를 계승하며, Grouped Query Attention, SwiGLU, Rotary Positional Embeddings, RMSNorm 등을 사용하고 QK-Norm 도입으로 안정적인 학습이 가능하다. MoE 모델은 128명의 전문가를 보유하고 있으며, 토큰당 8명의 전문가가 활성화된다. 이전 버전과 달리 공유 전문가를 제외하고, 글로벌 배치 부하 균형 손실을 적용해 전문가 특화도를 높였다. 이러한 설계와 학습 혁신 덕분에 하위 작업 성능이 크게 향상되었다.\n",
            "\n",
            "Qwen3 모델은 Qwen 토크나이저를 사용하며, 이는 바이트 수준의 byte-pair encoding을 기반으로 약 15만 개의 어휘를 포함한다. 밀집 모델의 상세 구조는 층 수, 헤드 수, 임베딩 연결 여부, 컨텍스트 길이 등으로 차별화되며, 모델 크기에 따라 0.6억에서 32억 파라미터까지 다양하다. 이처럼 Qwen3는 효율적이고 확장 가능한 아키텍처를 통해 다방면에서 최적화된 성능을 제공하며, 차별화된 설계 요소들이 종합적으로 모델의 강력한 성능을 뒷받침한다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 4/55 [00:29<05:58,  7.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 3\n",
            "Qwen3 모델은 Qwen의 토크나이저를 사용하며, 바이트 레벨의 바이트 페어 인코딩 방식을 적용해 151,669개의 어휘 크기를 갖는다. 모델 아키텍처는 0.6B부터 32B까지 다양한 크기로 구성되며, MoE(전문가 혼합) 모델도 30B와 235B 규모로 제공된다. 또한, 문맥 길이는 최대 128K 토큰까지 지원해 대규모 문서 처리에 적합하다.\n",
            "\n",
            "사전학습 데이터는 Qwen2.5 대비 두 배 이상의 토큰 수와 세 배 이상의 언어 다양성을 확보해 119개 언어와 방언, 총 36조 토큰으로 구성된다. 데이터는 코딩, STEM, 추론, 서적, 다국어, 합성 데이터 등 다양한 고품질 콘텐츠를 포함하고 있으며, Qwen2.5-VL과 Qwen2.5 모델을 활용해 PDF 문서 텍스트 인식 및 정제 과정을 거쳐 데이터 품질을 높였다. 또한, 다국어 데이터 주석 시스템을 도입해 30조 개 이상의 토큰에 교육적 가치, 분야, 안전성 등을 세밀하게 라벨링해 인스턴스 단위로 데이터 혼합을 최적화한다.\n",
            "\n",
            "사전학습은 세 단계로 진행되며, 첫 번째 일반 단계에서는 119개 언어의 30조 토큰 이상을 4,096 토큰 시퀀스 길이로 학습한다. 이 단계에서 모델은 언어 능력과 일반 지식을 충분히 학습하며, 이후 단계에서는 더 특화된 학습이 진행될 예정이다. Qwen3는 대규모 다국어 및 다도메인 학습을 통해 언어적 범용성과 성능을 크게 향상시킨 최신 언어 모델임을 보여준다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 5/55 [00:37<06:02,  7.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 4\n",
            "Qwen3 모델은 세 단계의 사전학습 과정을 거쳐 언어 능력, 추론력, 그리고 긴 문맥 이해 능력을 체계적으로 향상시킨다. 첫 번째 일반 단계에서는 119개 언어와 방언을 포함한 30조 개 이상의 토큰을 사용해 기본적인 언어 및 세계 지식을 학습한다. 두 번째 추론 단계에서는 STEM, 코딩, 추론 관련 데이터 비중을 늘려 약 5조 개의 고품질 토큰을 추가 학습하며 학습률 감쇠 속도를 조절한다. 마지막으로 긴 문맥 단계에서는 최대 32,768 토큰 길이의 고품질 긴 문맥 데이터를 활용해 모델의 문맥 처리 능력을 크게 확대한다.\n",
            "\n",
            "사전학습 과정에서는 RoPE 주파수 기반을 10,000에서 1,000,000으로 늘리고, YARN과 Dual Chunk Attention 기법을 도입해 추론 시 시퀀스 길이 처리 용량을 4배 향상시켰다. 또한, 세 단계 사전학습 데이터를 바탕으로 최적 하이퍼파라미터(학습률 스케줄러, 배치 크기 등)에 관한 스케일링 법칙을 개발하여 각 모델 유형에 맞는 학습 전략을 체계적으로 설정하였다. 이러한 세밀한 데이터 라벨링과 인스턴스 수준의 데이터 혼합 최적화는 효과적인 데이터 필터링과 조합을 가능하게 했다.\n",
            "\n",
            "Qwen3 시리즈의 기본 언어 모델들은 일반 지식, 추론, 수학, 과학 지식, 코딩, 다국어 능력 등 다양한 영역에서 15개 이상의 벤치마크를 통해 종합적으로 평가되었다. 평가에는 MMLU, BBH, GSM8K, HumanEval, MultiPL-E 등 다양한 태스크가 포함되어 있으며, 0~8샷과 체인 오브 씽킹(Chain of Thought) 기법이 적용된 환경에서 성능을 검증하였다. 이를 통해 Qwen3 모델이 다방면에서 균형 잡힌 능력을 갖추었음을 확인할 수 있다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 6/55 [00:45<06:11,  7.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 5\n",
            "Qwen3 시리즈의 베이스 모델은 Qwen2.5 및 기타 주요 오픈소스 베이스 모델과 비교하여 동일한 평가 환경에서 우수한 성능을 보였다. 특히 Qwen3-235B-A22B-Base 모델은 파라미터 수가 적음에도 불구하고 DeepSeek-V3, Llama-4-Maverick, Qwen2.5-72B-Base 등과 같은 경쟁 모델들을 대부분의 벤치마크에서 능가했다. Qwen3 MoE(전문가 혼합) 베이스 모델은 1/5 수준의 활성화 파라미터로 Qwen3의 일반 밀집(dense) 모델과 동등한 성능을 달성했고, Qwen2.5 MoE 모델과 비교해도 절반 이하의 활성화 파라미터로 더 나은 성능을 보이며 학습 및 추론 비용 면에서 유리하다.\n",
            "\n",
            "Qwen3 일반 밀집 베이스 모델은 고파라미터 스케일에서 Qwen2.5 모델과 비슷하거나 더 뛰어난 성능을 내며, 특히 STEM, 코딩, 추론 분야 벤치마크에서 우위를 점했다. 이는 Qwen3 아키텍처의 개선, 학습 토큰의 규모 확대, 고도화된 학습 전략 덕분이다. 1.7B부터 235B까지 다양한 규모의 Qwen3 모델들이 대응하는 Qwen2.5 모델과 유사하거나 뛰어난 성능을 보여, 모델 크기 대비 효율성 측면에서 큰 진전을 이뤘다.\n",
            "\n",
            "종합적으로 Qwen3 시리즈는 파라미터 수 대비 성능 최적화에 성공했으며, 특히 MoE 구조를 활용한 모델은 적은 활성화 파라미터로 비용 효율적이면서도 높은 성능을 발휘한다. 이로 인해 Qwen3는 추론 및 학습 비용을 줄이고도 최첨단 성능을 유지할 수 있어 다양한 멀티언어 및 코딩 작업에서 강력한 경쟁력을 확보했다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 7/55 [00:54<06:16,  7.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 6\n",
            "Qwen3-235B-A22B-Base 모델은 다양한 평가 벤치마크에서 최상위 성능을 기록하며, 동종 최고 성능 모델들과 비교해 뛰어난 우수성을 입증했다. Llama-4-Maverick-Base와 같은 파라미터 수가 두 배인 모델보다 대부분의 벤치마크에서 우수한 성능을 보였으며, DeepSeek-V3-Base 대비 1/3 수준의 파라미터만으로 15개 중 14개 벤치마크에서 더 좋은 결과를 냈다. 또한, Qwen2.5-Plus와 비교 시 파라미터 수와 활성화 파라미터가 더 적으면서도 더 높은 성능을 보여, 향상된 사전 학습 데이터, 훈련 전략 및 모델 구조의 강점을 확인할 수 있다. Qwen2.5-72B-Base 대비 모든 벤치마크에서 우위를 보였으며, 활성화 파라미터는 1/3 미만으로 유지해 효율성과 경제성을 동시에 달성했다. 아울러, 모델 아키텍처 덕분에 추론 및 학습 비용도 크게 절감되었다.\n",
            "\n",
            "Qwen3-32B-Base는 Qwen3 시리즈에서 가장 큰 데스트 모델로, 비슷한 크기의 Gemma-3-27B 및 Qwen2.5-32B와 비교되었다. 또한, 파라미터 수는 Qwen3-32B-Base의 세 배에 달하지만 활성화 파라미터는 절반 수준인 Llama-4-Scout와도 경쟁하였다. Qwen3-235B-A22B-Base는 총 2350억 파라미터 중 220억 활성화 파라미터를 사용하는 MoE 아키텍처로, 일반 과제, 수학·STEM, 코딩 및 다국어 과제 등 다양한 분야에서 탁월한 성능을 입증했다. 표 3에 따르면 MMLU, GSM8K, EvalPlus 등 주요 벤치마크에서 다른 강력한 오픈소스 모델들을 능가하는 점수를 기록하였다. 전반적으로 Qwen3 시리즈는 규모 대비 효율성과 성능 면에서 현존하는 공개 모델 중 최고 수준임을 보여준다.\n",
            "\n",
            "요약하면, Qwen3 시리즈는 최신 MoE 모델 중에서도 뛰어난 성능과 비용 효율성을 동시에 달성한 혁신적인 모델이다. 특히 Qwen3-235B-A22B-Base는 적은 활성 파라미터로도 방대한 작업을 뛰어나게 처리하며, 기존 대형 모델들이 가진 한계를 극복하였다. 이는 향상된 데이터, 학습 기법, 그리고 최적화된 아키텍처 설계 덕분에 가능했으며, 앞으로 인공지능 연구 및 실용적 적용에서 중요한 이정표로 평가된다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 8/55 [01:00<05:50,  7.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 7\n",
            "Qwen3 시리즈는 여러 오픈 소스 대형 언어 모델과 다양한 작업군에서 성능을 비교한 결과, 전반적으로 우수한 성적을 보였다. 32B 파라미터 모델 기준으로 BBH, GSM8K, EvalPlus, MBPP 등 다양한 벤치마크에서 최고 또는 두 번째로 높은 점수를 기록하며 특히 수학 및 코딩 과제에서 뛰어난 성능을 나타냈다. 또한, Qwen3-32B는 Dense 아키텍처를 기반으로 하면서도 72B 파라미터의 Qwen2.5-72B 모델과 경쟁할 만한 성능을 보여, 파라미터 효율성과 모델 설계의 우수성을 입증했다.\n",
            "\n",
            "14B 및 30B 파라미터 급 모델에서도 Qwen3는 Gemma, Qwen2.5 등과 비교해 높은 점수를 획득했으며, MoE 아키텍처를 채택한 모델들과도 견줄 만한 성능을 보였다. 특히 Qwen3-30B-A3B 모델은 MoE 구조임에도 불구하고 활성화된 파라미터 수가 적음에도 불구하고 좋은 결과를 냈다. 다국어 작업 및 일반 지식 테스트에서도 안정적인 결과를 내며 다양한 분야에 걸쳐 균형 잡힌 성능을 갖추었다.\n",
            "\n",
            "종합적으로 Qwen3 시리즈는 파라미터 수 대비 효율적인 학습과 강력한 성능을 기반으로, 수학, 코딩, 다국어 등 다양한 과제에서 경쟁력을 확보한 최신 대형 언어 모델임을 보여준다. Dense와 MoE 아키텍처를 넘나드는 설계 다양성을 통해 다양한 규모와 목적에 적합한 모델을 제공하며, 지속적으로 공개되는 오픈 소스 모델 중에서도 상위권에 위치하고 있다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▋        | 9/55 [01:06<05:19,  6.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 8\n",
            "Qwen3 시리즈는 다양한 크기와 구조(밀집형(Dense) 및 전문가 혼합형(MoE))의 모델을 포함하며, 여러 벤치마크에서 강력한 성능을 보인다. 특히 Qwen3-30B-A3B 모델은 MMLU, BBH, SuperGPQA와 같은 일반 과제에서 높은 점수를 기록하며 우수한 성능을 입증했다. MoE 아키텍처를 활용한 모델들은 활성화 파라미터 수가 상대적으로 적음에도 불구하고, 효율적인 성능을 보여준다.\n",
            "\n",
            "수학 및 STEM 과제에서는 Qwen3-14B와 30B 모델이 GSM8K, MATH, GPQA 등에서 경쟁력 있는 결과를 냈으며, 코딩 작업 평가(EvalPlus, MultiPL-E, MBPP 등)에서도 상위권 성능을 나타냈다. 이는 Qwen3 모델이 다방면의 문제 해결 능력을 고루 갖추고 있음을 시사한다. 다국어 과제 평가(MGSM, MMMLU 등)에서도 Qwen3 시리즈는 기존 Qwen2.5 및 Llama-3 모델 대비 개선된 점수를 기록하였다.\n",
            "\n",
            "또한, Qwen3-8B 모델은 같은 크기의 Qwen2.5-14B와 비교해도 유사하거나 더 나은 성능을 보이며, 8B급 모델 중에서도 뛰어난 경쟁력을 가진 것으로 나타났다. 전반적으로 Qwen3 시리즈는 다양한 크기와 아키텍처 조합을 통해 높은 효율성과 성능을 동시에 달성하며, 최신 오픈소스 대형 언어 모델 중 상위권에 위치하고 있음을 보여준다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 10/55 [01:14<05:24,  7.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 9\n",
            "Qwen3 시리즈는 다양한 크기와 파라미터 수를 가진 대형 언어 모델로, 여러 벤치마크에서 뛰어난 성능을 보였다. 4B 파라미터 모델(Qwen3-4B)은 GPQA, GSM8K, MATH, EvalPlus, MultiPL-E, MBPP, CRUX-O, MGSM, MMMLU, INCLUDE 등 다양한 일반, 수학, 코딩, 다국어 과제에서 기존 오픈소스 모델 대비 상위권의 점수를 기록하였다. 특히 GSM8K와 MATH 같은 수학 문제 해결 능력에서 80% 이상의 고성능을 달성하며, 7B 모델에 근접하는 결과를 보여주었다.\n",
            "\n",
            "한편, 1.7B 및 0.6B 크기의 소형 모델들도 일반 과제와 수학 및 STEM 문제에서 경쟁력 있는 성능을 나타냈다. 예를 들어 Qwen3-1.7B는 62.63점의 MMLU 점수와 75.44점의 GSM8K 점수를 기록, 동일 규모의 기존 모델들보다 높은 점수를 나타냈다. 이러한 결과는 모델이 파라미터 수가 작아도 효율적으로 학습되고, 다양한 작업에서 균형 잡힌 성능을 유지함을 시사한다.\n",
            "\n",
            "종합적으로 Qwen3는 다양한 크기의 모델에서 균형 잡힌 성능 향상을 보이며, 특히 수학 및 코딩 과제에서 강세를 보인다. 이는 대규모 언어 모델의 응용 범위를 넓히고, 다국어 능력까지 갖춘 점에서 실용성과 경쟁력이 높다는 것을 나타낸다. 따라서 Qwen3 모델은 연구 및 산업 현장에서 폭넓게 활용 가능한 최첨단 오픈소스 언어 모델임을 확인할 수 있다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 11/55 [01:20<05:08,  7.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 10\n",
            "Qwen3 시리즈는 이전 세대 모델들과 비교해 파라미터 수 대비 뛰어난 성능을 보여준다. Qwen3-32B-Base는 32B 파라미터 모델 중에서 Qwen2.5-32B-Base 및 Gemma-3-27B Base를 대부분의 벤치마크에서 능가하며, 특히 MMLU-Pro와 SuperGPQA에서 큰 성능 향상을 기록했다. 더 나아가, Qwen3-32B-Base는 72B 파라미터의 Qwen2.5-72B-Base와 비교했을 때도 절반 이하의 파라미터 수임에도 불구하고 15개 평가 지표 중 10개에서 우수한 결과를 내, 수학, 코딩, 추론 분야에서 두드러진 강점을 나타냈다.\n",
            "\n",
            "또한 Qwen3-32B-Base는 Llama-4-Scout-Base 모델 대비 3분의 1 수준의 파라미터 수임에도 불구하고 모든 벤치마크에서 월등한 성능을 보였으며, 활성화 파라미터 수는 오히려 두 배에 달해 효율성 측면에서도 우수함을 입증했다. Qwen3-14B-Base와 Qwen3-30B-A3B-Base도 유사 크기의 경쟁 모델인 Gemma-3-12B Base, Qwen2.5-14B Base, Qwen2.5-Turbo, Qwen2.5-32B-Base와 비교해 현저하게 더 나은 성능을 보여, Qwen3 시리즈가 다양한 규모에서 강력한 성능 경쟁력을 갖췄음을 보여준다.\n",
            "\n",
            "종합적으로 Qwen3 모델은 파라미터 효율성과 성능 모두에서 크게 향상된 결과를 내며, 특히 코딩, 수학, 다국어 및 추론 과제에서 탁월한 능력을 발휘한다. 이는 대형 언어 모델의 최적화 및 설계 기술이 진일보했음을 시사하며, Qwen3는 현존하는 주요 오픈소스 모델과 비교해 경쟁력 있는 성능을 갖춘 차세대 대표 모델임을 확인시켜준다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 12/55 [01:27<05:00,  6.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 11\n",
            "Qwen3 시리즈 모델은 다양한 크기에서 경쟁력 있는 성능을 보이며, 특히 Qwen3-14B-Base는 유사 크기의 Qwen2.5-14B-Base 및 Gemma-3-12B-Base 모델보다 모든 벤치마크에서 우수한 결과를 나타낸다. 또한 Qwen3-30B-A3B는 활성화된 파라미터 수가 적음에도 불구하고 Qwen2.5-14B-Base를 모든 작업에서 크게 능가하며, Qwen3-14B-Base 및 Qwen2.5-32B-Base와도 비슷한 성능을 보여 추론 및 학습 비용 측면에서 큰 이점을 제공한다. 엣지 디바이스용 소형 모델들도 Qwen2.5, Llama-3, Gemma-3 기반 모델과 비교해 우수한 성능을 유지하며, 특히 STEM 및 코딩 분야에서 더 큰 모델을 능가하는 경우도 많아 Qwen3 모델의 향상된 성능을 입증한다.\n",
            "\n",
            "Qwen3의 사후 학습 파이프라인은 두 가지 핵심 목표를 중심으로 설계되었다. 첫째, ‘사고 제어(Thinking Control)’로서 비사고 모드와 사고 모드를 통합하여 사용자가 모델의 추론 여부와 사고 깊이를 조절할 수 있게 한다. 둘째, ‘강한 모델에서 약한 모델로의 증류(Strong-to-Weak Distillation)’를 통해 대규모 모델의 지식을 활용하여 경량 모델의 개발 비용과 계산 자원을 크게 절감한다. 이 과정은 네 단계로 구성되며, 초기 두 단계는 사고 능력 개발에, 후속 두 단계는 비사고 기능 통합에 초점을 맞춘다.\n",
            "\n",
            "예비 실험 결과, 교사 모델의 출력 로짓을 학생 모델에 직접 증류하는 방식이 경량 모델의 성능을 효과적으로 향상시키며, 사고 과정의 세밀한 제어도 가능하게 한다. 이 방법은 모든 소형 모델에 대해 개별적으로 4단계 학습을 수행하는 것보다 훨씬 효율적이며, GPU 사용 시간을 1/10로 줄이면서도 Pass@1 및 Pass@64 점수에서 향상된 결과를 보인다. 따라서 Qwen3 시리즈는 고성능과 효율성을 동시에 추구하는 혁신적인 학습 전략을 제시한다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▎       | 13/55 [01:34<04:52,  6.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 12\n",
            "Qwen3의 훈련 과정은 총 네 단계로 구성되며, 처음 두 단계는 모델의 '사고' 능력을 개발하는 데 중점을 두고, 이후 두 단계는 강력한 '비사고' 기능을 통합하는 것을 목표로 한다. 특히, 교사 모델의 출력 로짓을 경량 학생 모델에 직접 증류하는 방식이 효과적임을 보였으며, 이는 모든 소규모 모델에 대해 4단계 훈련을 개별적으로 수행할 필요를 없애고, Pass@1 및 Pass@64 점수 향상과 함께 훈련 효율성도 크게 개선한다. 이 방법은 기존 4단계 훈련 대비 GPU 시간의 10분의 1만 소요되어 효율적이다.\n",
            "\n",
            "첫 번째 단계인 Long-CoT Cold Start는 수학, 코드, 논리적 추론, 일반 STEM 문제 등 다양한 분야를 포괄하는 엄격하게 필터링된 데이터셋을 구축하여, 깊은 추론이 요구되는 문제들로 구성된 훈련을 시작한다. 이 과정에서 Qwen2.5-72B-Instruct를 활용해 쉽지 않은 문제만을 선별하고, QwQ-32B 모델과 인간 평가자가 답변의 정확성과 질을 검증한다. 이 초기 단계에서는 모델이 즉각적인 추론 성능에 치중하지 않고 기초적 추론 패턴을 학습하도록 하여, 이후 강화 학습 단계에서 개선 가능성을 극대화한다.\n",
            "\n",
            "두 번째 단계인 Reasoning RL에서는 Cold Start에서 사용되지 않은, 학습 가능하고 난이도 높은 3,995개의 쿼리-검증 쌍을 활용해 GRPO 알고리즘으로 모델을 업데이트한다. 대형 배치 크기와 다수의 롤아웃, 그리고 오프 정책 훈련을 통해 샘플 효율성을 높이고, 엔트로피 조절로 탐색과 착취의 균형을 맞추는 전략을 적용한다. 이러한 체계적인 접근으로 모델의 추론 능력과 탐색 역량을 동시에 강화하며, 이후 단계에서 사고 및 비사고 모드를 융합하는 훈련으로 확장된다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 14/55 [01:40<04:31,  6.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 13\n",
            "Qwen3 모델의 학습 과정에서는 대규모 배치 크기와 높은 쿼리 롤아웃 수를 활용하고, 샘플 효율성을 높이기 위해 오프폴리시 학습을 적용하는 것이 효과적임을 확인하였다. 또한, 모델 엔트로피를 조절하여 탐험과 활용의 균형을 맞추고 안정적인 학습을 유지하는 것이 중요하며, 이를 통해 수동적인 하이퍼파라미터 조정 없이도 학습 보상과 검증 성능에서 일관된 향상을 달성했다. 예를 들어, Qwen3-235B-A22B 모델은 170회의 강화학습(RL) 스텝 동안 AIME’24 점수가 70.1에서 85.1로 상승하였다.\n",
            "\n",
            "Thinking Mode Fusion 단계에서는 기존의 “생각하는” 모델과 “생각하지 않는” 기능을 통합하여 두 가지 사고 모드를 하나의 모델 내에서 관리하고, 별도의 모델 배포 비용과 복잡성을 줄이고자 하였다. 이를 위해 추론 RL 모델에 대해 지속적인 감독 학습(SFT)을 수행하고, /think 및 /no think 플래그를 포함한 새로운 채팅 템플릿을 설계하여 사용자가 동적으로 사고 모드를 전환할 수 있도록 하였다. SFT 데이터셋은 생각하는 데이터와 생각하지 않는 데이터를 결합하였으며, 후자는 코딩, 수학, 다국어, 창의적 글쓰기 등 다양한 영역을 포괄하고 특히 저자원 언어 번역 작업의 비중을 높였다.\n",
            "\n",
            "이 통합된 사고 모드 모델은 중간 생각 단계에서의 응답을 생성할 수 있는 능력을 갖추어, 사용자가 설정한 사고 길이 제한에 도달하면 자동으로 사고를 중단하고 누적된 추론 결과를 바탕으로 최종 답변을 생성하는 사고 예산 제어 기능을 지원한다. 이러한 사고 모드의 융합과 제어는 Qwen3 모델이 다양한 복잡도의 작업에서 일관되고 효율적으로 대응할 수 있게 하는 기반이 된다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 15/55 [01:47<04:25,  6.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 14\n",
            "Qwen3 모델은 복잡한 다중 턴 대화에서 사용자 쿼리에 무작위로 /think와 /no think 플래그를 삽입하여 모델이 마지막 플래그에 맞춰 응답하도록 하는 Thinking Mode Fusion 기법을 도입했다. 이 방식은 모델이 비사고 모드와 사고 모드를 모두 학습함으로써 불완전한 사고 상태에서도 응답을 생성할 수 있는 능력을 자연스럽게 획득하게 하며, 사용자가 설정한 사고 시간 예산에 도달하면 사고를 중단하고 즉시 최종 답변을 생성하도록 한다. 이러한 사고 예산 제어 기능은 별도의 학습 없이도 사고 모드 융합을 통해 자연스럽게 나타난다.\n",
            "\n",
            "일반 RL 단계에서는 20개 이상의 다양한 작업별 맞춤 보상 체계를 구축하여 모델의 능력과 안정성을 전반적으로 향상시켰다. 주요 목표는 사용자 지시사항 준수, 응답 형식 일치, 선호도 조율, 도구 호출 능력, 그리고 특수 상황에서의 정확성 강화 등이며, 이를 평가하기 위해 규칙 기반 보상, 참조 답변이 있는 모델 기반 보상, 참조 없는 모델 기반 보상 등 세 가지 보상 방식을 활용한다. 이로써 모델은 다양한 시나리오에서 사용자 기대에 부합하는 자연스럽고 정확한 응답을 제공할 수 있다.\n",
            "\n",
            "Strong-to-Weak Distillation 파이프라인은 경량 모델군(Qwen3-0.6B부터 14B 및 MoE 모델 포함)의 성능과 모드 전환 능력을 향상시키기 위해 고안되었다. 이 과정은 오프폴리시 증류 단계에서 교사 모델의 사고 및 비사고 모드 출력을 결합해 기본 추론 및 모드 전환 능력을 학습시키고, 이후 온폴리시 증류 단계에서 학생 모델이 직접 정책을 수행하며 학습을 심화하는 두 단계로 이루어진다. 이를 통해 경량 모델도 고성능 및 사고 모드 스위칭 역량을 갖추게 된다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 16/55 [01:51<03:50,  5.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 15\n",
            "The Strong-to-Weak Distillation pipeline is designed to optimize lightweight models, including five dense models and one MoE model, by enhancing performance and enabling robust mode-switching capabilities. The distillation process comprises two main phases: off-policy distillation, where teacher model outputs from both /think and /no think modes are combined to build foundational reasoning and mode-switching skills in student models, and on-policy distillation, where the student model generates responses under specific modes and is fine-tuned by minimizing KL divergence with large teacher models.\n",
            "\n",
            "For post-training evaluation, the instruction-tuned models undergo comprehensive testing using automatic benchmarks across multiple languages and task categories. Benchmarks span general tasks like MMLU-Redux and GPQA-Diamond, alignment tasks assessing instruction-following and human preference alignment, and math and text reasoning tasks using advanced math and logical reasoning datasets. This thorough evaluation framework ensures the models' capabilities are measured in diverse dimensions, including reasoning, creativity, and alignment with human intent.\n",
            "\n",
            "Overall, Qwen3’s distillation and evaluation approach systematically strengthens lightweight models by transferring capabilities from larger teacher models and rigorously validating performance across multilingual and multifaceted benchmarks. This enables the development of efficient yet powerful models capable of adaptive thinking and high-quality output generation.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 17/55 [01:57<03:49,  6.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 16\n",
            "Qwen3 모델 평가를 위해 다양한 특화 벤치마크가 활용되었다. 지시 수행 능력은 IFEval, 인간 선호도 정렬은 Arena-Hard와 AlignBench v1.1, 글쓰기 능력은 Creative Writing V3와 WritingBench로 측정되었다. 수학 및 논리 추론 능력은 MATH-500, AIME’24/’25, ZebraLogic, AutoLogi 등 고난도 문제로 평가되었으며, AIME 문제는 64회 샘플링 후 평균 정확도를 최종 점수로 삼았다. 코딩 및 에이전트 과제는 BFCL v3, LiveCodeBench, Codeforces Ratings를 통해 시험되었고, 모델 성능과 전문 프로그래머 간 격차도 코드포스 Elo 등급으로 분석했다.\n",
            "\n",
            "다국어 능력 평가는 지시 수행, 지식, 수학, 논리 추론의 네 가지 영역으로 나누어 진행되었다. 지시 수행은 Multi-IF(8개 언어), 지역 지식은 INCLUDE(44개 언어), 일반 지식은 MMMLU(14개 언어)로 평가했으며, 수학 문제는 MT-AIME2024(55개 언어)와 PolyMath(18개 언어)를 사용했다. 논리 추론은 10개 언어를 포함한 MlogiQA로 측정했다. 평가 효율을 위해 일부 벤치마크 데이터는 10% 샘플링하였으며, 생성 시 샘플링 온도, top-p, top-k, presence penalty 등 하이퍼파라미터도 세밀하게 조정하였다.\n",
            "\n",
            "Qwen3-235B-A22B 모델은 생각 모드와 비생각 모드에서 각각 최적화된 파라미터를 적용하며, 최대 출력 길이를 32,768~38,912 토큰으로 설정해 충분한 추론 공간을 제공한다. 비교 대상 모델로는 OpenAI-o1, DeepSeek-R1, Grok-3-Beta (Think), Gemini2.5-Pro 등이 있으며, Qwen3-235B-A22B는 모듈러 MoE 구조를 기반으로 22B 활성화 파라미터를 갖추고 있어 경쟁력 있는 성능을 보였다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 18/55 [02:04<03:54,  6.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 17\n",
            "Qwen3-235B-A22B 모델은 사고(Thinking) 모드와 비사고(Non-thinking) 모드에서 각각 최대 출력 길이를 32,768 토큰으로 설정했으며, AIME’24 및 AIME’25 문제에서는 충분한 사고 공간을 제공하기 위해 이를 38,912 토큰으로 확장했다. 표 11의 사고 모드 성능 비교에서 Qwen3-235B-A22B는 AIME’24(92.0)와 AIME’25(86.7) 등 수학 및 텍스트 추론 분야에서 높은 점수를 기록하며, 여러 일반, 정렬, 에이전트 및 코딩, 다중언어 과제에서도 경쟁력 있는 성능을 보여주었다. 특히 MATH-500, AutoLogi, Arena-Hard 등 여러 벤치마크에서 최상위권 성적을 보이며, CodeForces 점수도 2056점(상위 98.2%)으로 우수함을 입증했다.\n",
            "\n",
            "비사고 모드(표 12)에서는 GPT-4o, DeepSeek-V3 등 여러 모델과 비교했을 때 Qwen3-235B-A22B가 MMLU-Redux(89.2)와 GPQA-Diamond(62.9) 등에서 안정적인 성능을 보였다. 이 모드에서 모델은 22B 활성화 파라미터를 사용하며, 총 235B 파라미터 규모를 유지하는 MoE 아키텍처 기반이다. Dense와 MoE 구조를 포함한 다양한 모델과 비교하였을 때, Qwen3는 균형 잡힌 성능과 효율성을 갖춘 점이 돋보인다.\n",
            "\n",
            "종합적으로 Qwen3-235B-A22B는 사고 및 비사고 모드 모두에서 광범위한 벤치마크를 통해 뛰어난 추론 능력과 다중 영역 적용 가능성을 입증했다. 특히 사고 모드에서 수학 문제 해결 능력이 두드러지며, 비사고 모드에서도 전반적인 일반 및 언어 이해 능력을 고루 갖추고 있다. 이는 대규모 MoE 아키텍처와 충분한 토큰 처리 능력을 바탕으로 한 모델 설계의 강점을 보여주는 결과이다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▍      | 19/55 [02:10<03:37,  6.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 18\n",
            "Qwen3 모델 평가 결과에 따르면, 플래그십 모델인 Qwen3-235B-A22B은 공개 소스 모델 중 최고 수준의 성능을 보이며, DeepSeek-R1과 DeepSeek-V3 등 강력한 경쟁 모델을 능가한다. 이 모델은 사고 능력(Reasoning)과 비사고 능력 모두에서 뛰어난 성과를 나타내며, OpenAI-o1, Gemini2.5-Pro, GPT-4o 같은 폐쇄형 최상위 모델들과도 대등한 경쟁력을 갖추고 있다. 전반적으로 Qwen3-235B-A22B는 깊이 있는 추론 능력과 포괄적인 일반 능력을 동시에 갖춘 점이 돋보인다.\n",
            "\n",
            "또 다른 플래그십인 Qwen3-32B는 이전 모델인 QwQ-32B보다 대부분의 벤치마크에서 우수한 성능을 보이며, OpenAI-o3-mini와 비슷한 수준의 추론 능력을 선보인다. 특히, 비사고 모드에서도 뛰어난 성능을 나타내며, 이전의 비사고 플래그십 모델인 Qwen2.5-72B-Instruct를 능가하는 결과를 기록했다. 이는 Qwen3-32B가 추론과 비추론 작업 모두에서 균형 잡힌 강력한 성능을 제공함을 의미한다.\n",
            "\n",
            "요약하면, Qwen3 시리즈는 공개 소스 AI 모델 중 최상위 성능을 기록하며, 사고력과 일반화 능력에서 탁월한 경쟁력을 갖추고 있다. Dense 및 MoE 아키텍처를 기반으로 한 다양한 모델들이 각각 추론 및 비추론 작업에서 뛰어난 결과를 내고 있어, 연구 및 실무 적용에서 유용한 선택지가 될 것으로 평가된다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▋      | 20/55 [02:17<03:44,  6.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 19\n",
            "Qwen3 시리즈는 오픈소스 및 폐쇄소스 모델들과의 비교에서 뛰어난 추론 능력과 전반적인 성능을 입증했다. 특히 플래그십 모델 Qwen3-235B-A22B는 오픈소스 모델 중 DeepSeek-R1, DeepSeek-V3를 포함한 강력한 베이스라인을 능가하며, 폐쇄소스 모델인 OpenAI-o1, Gemini2.5-Pro, GPT-4o와도 경쟁력 있는 성능을 보였다. 이는 60% 활성화와 35% 파라미터만 사용하면서도 수학, 에이전트, 코딩 등 추론이 요구되는 작업에서 두드러진 성과를 나타내어 오픈소스 모델의 추론 역량이 폐쇄소스 모델과 거의 대등해졌음을 의미한다.\n",
            "\n",
            "Qwen3-32B는 이전 최강 추론 모델 QwQ-32B를 대부분 벤치마크에서 앞서며, 폐쇄소스 OpenAI-o3-mini와도 대등한 성능을 보였다. 또한 비추론 모드에서도 전작 Qwen2.5-72B-Instruct를 능가하여 추론 및 비추론 모드 모두에서 강력한 모델임을 입증했다. 이는 Qwen3-32B가 32B 규모에서 최첨단 추론 모델로 자리매김했음을 시사하며, 다국어 처리와 정렬(alignment) 성능에서도 우수하다.\n",
            "\n",
            "경량 모델군인 Qwen3-30B-A3B, Qwen3-14B 등은 Strong-to-Weak Distillation 기법을 통해 동등하거나 더 큰 파라미터를 가진 오픈소스 모델들보다 일관되게 우수한 성능을 제공한다. 이를 통해 Qwen3 시리즈가 다양한 규모와 모드에서 고성능을 유지하며, 특히 추론 능력과 일반화 능력에서 업계 최상위권에 위치함을 확인할 수 있다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 21/55 [02:25<03:52,  6.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 20\n",
            "Qwen3-32B 모델은 사고(Thinking) 모드에서 기존 최고 성능 모델인 QwQ-32B를 17개 중 23개 벤치마크에서 능가하며, 32B 크기에서 최첨단 추론 모델로 자리매김했다. 또한, OpenAI의 폐쇄형 모델인 OpenAI-o3-mini(중간)와 비교해도 정렬(alignment)과 다국어 성능 면에서 우수한 경쟁력을 보였다. 사고 모드가 아닌(non-thinking) 모드에서는 Qwen3-32B가 거의 모든 벤치마크에서 모든 비교 대상 모델보다 뛰어난 성능을 보였으며, 특히 Qwen2.5-72B-Instruct와 비슷한 일반 작업 수행 능력을 가지면서도 정렬, 다국어 및 추론 관련 작업에서 현저한 강점을 나타내었다. 이는 Qwen3 시리즈가 이전 Qwen2.5 시리즈 대비 근본적인 성능 향상을 이뤘음을 입증한다.\n",
            "\n",
            "Qwen3-30B-A3B와 Qwen3-14B 모델 역시 사고 모드에서 QwQ-32B와 비교해 매우 경쟁력 있는 성능을 보여주었다. 특히 Qwen3-30B-A3B는 모델 크기가 더 작고 효율성이 높은 상황에서도 QwQ-32B와 거의 동등한 추론 관련 성능을 달성했다. 비사고 모드에서는 Phi-4, Gemma-3-27B-IT, Qwen2.5-32B-Instruct 등과 비교해 우수한 결과를 보이며, 경량화된 모델임에도 불구하고 높은 품질의 결과물을 생성함을 확인했다. 전반적으로 Qwen3 시리즈는 효율성과 성능 두 측면에서 모두 크게 향상된 최신 LLM임을 증명하였다.\n",
            "\n",
            "평가 결과는 표 13, 14, 15, 16에 상세히 제시되어 있으며, Qwen3 시리즈가 다양한 일반, 정렬, 추론, 코딩 및 다국어 벤치마크에서 기존 최상위 모델들을 능가하거나 동등한 성능을 달성함을 보여준다. 특히, 32B 크기 모델이 70B 크기 모델과 견줄 만한 성능을 보임으로써, 모델 크기 대비 효율성 측면에서 중요한 진전을 이뤘다. 이러한 결과는 Qwen3가 현재 공개된 대규모 언어 모델 중 최첨단 기술을 구현한 사례임을 시사한다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 22/55 [02:30<03:29,  6.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 21\n",
            "Qwen3-32B 모델은 다양한 벤치마크 테스트에서 우수한 성능을 보이며, 특히 수학 및 추론, 코드 작성, 다중언어 과제에서 강점을 나타낸다. MATH-500, AIME, ZebraLogic 등 수학 관련 테스트에서 높은 점수를 기록했고, 코드 작성 능력을 평가하는 BFCL v3, LiveCodeBench v5에서도 경쟁력 있는 결과를 보여주었다. 다중언어 평가인 MMMLU 14개 언어와 INCLUDE, Multi-IF 등 테스트에서도 꾸준히 상위권에 위치하며 다국어 처리 능력을 입증했다. GPT-4o-mini, LLaMA-4 Scout, Qwen2.5-72B와 비교할 때, Qwen3-32B는 파라미터 수는 적지만 전반적으로 균형 잡힌 성능을 나타낸다. 특히 Alignment 및 Creative Writing 분야에서 뛰어난 결과를 내면서 실용성과 창의성 측면에서 강점을 보였다.\n",
            "\n",
            "비교 지표에서 Qwen3-32B는 GPT-4o-mini와 LLaMA-4 Scout보다 적은 파라미터 수(32B)를 가지고도 유사하거나 더 나은 성능을 기록했다. 예를 들어, Arena-Hard에서는 92.8점으로 가장 높은 점수를 기록했고, WritingBench에서도 7.54점으로 상위권에 위치했다. 반면, 코드 작성 관련 CodeForces 점수는 1353점으로 GPT-4o-mini보다 낮지만, 전체적인 평가에서는 고른 성능 분포를 보여주었다. 이러한 결과는 Qwen3-32B의 효율적인 아키텍처와 최적화가 효과적으로 작용했음을 시사한다.\n",
            "\n",
            "요약하면, Qwen3-32B는 다양한 AI 평가 지표에서 뛰어난 균형성과 효율성을 가진 모델로 자리매김했다. 수학, 추론, 언어 및 코드 작성 등 여러 영역에서 강력한 성능을 발휘하며, 특히 적은 파라미터 수에도 불구하고 GPT-4o-mini 및 LLaMA-4 Scout와 견줄 만한 결과를 보여준다. 이러한 특성은 Qwen3-32B가 실용적이고 다재다능한 인공지능 솔루션으로 활용될 가능성을 높인다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 23/55 [02:36<03:23,  6.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 22\n",
            "Qwen3 시리즈의 최신 모델인 Qwen3-30B-A3B와 Qwen3-14B는 다양한 벤치마크에서 우수한 성능을 보여주고 있다. 특히, 수학 및 텍스트 추론(MATH-500, AIME 등), 일반 지식(MMLU-Redux, C-Eval), 그리고 코딩 및 에이전트 관련 과제에서도 높은 점수를 기록했다. MoE(Mixture of Experts) 아키텍처를 적용한 Qwen3-30B-A3B 모델은 활성화된 파라미터 수가 적음에도 불구하고, Dense 아키텍처의 대형 모델들과 견줄 만한 경쟁력을 지녔다. 또한, 다국어 및 다양한 추론 작업에서 일관되게 좋은 결과를 내며, CodeForces 같은 코딩 테스트에서도 상위 97% 이상의 등급을 획득했다.\n",
            "\n",
            "본 보고서에서 제시된 비교 표는 Qwen3-30B-A3B와 Qwen3-14B가 기존의 Qwen2.5-32B, Phi-4, Gemma 등 타 모델과 비교해 다방면에서 우수한 성능을 보임을 시사한다. 특히 Alignment, Creative Writing, 그리고 다양한 추론 벤치마크에서 Qwen3 시리즈가 최고 또는 차상위 점수를 기록하며, 인공지능의 전반적 이해력과 문제 해결 능력이 크게 향상되었음을 알 수 있다. 이러한 결과는 Qwen3 모델이 적은 활성화 파라미터로도 뛰어난 효율성과 성능을 발휘할 수 있음을 보여준다.\n",
            "\n",
            "종합하면, Qwen3 시리즈는 MoE 아키텍처 도입과 최적화된 파라미터 구성을 통해 대규모 언어 모델 분야에서 경쟁력을 강화하였다. 다양한 평가 항목에서 강력한 성능을 입증하며, 특히 복잡한 수학 문제, 코딩, 다국어 이해 등 실용적 응용 분야에서 뛰어난 활용 가능성을 보이고 있다. 이는 앞으로 인공지능 연구와 산업 적용에 있어 Qwen3 모델이 중요한 역할을 할 것임을 예고한다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▎     | 24/55 [02:47<03:53,  7.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 23\n",
            "Qwen3 시리즈는 다양한 크기와 구조의 모델로, 일반, 정렬, 수학 및 추론, 에이전트 및 코딩, 다국어 등 여러 과제에서 다른 최신 모델들과 성능을 비교한 결과를 제시하고 있다. 특히 Qwen3-30B-A3B 모델은 MoE(Mixture of Experts) 아키텍처를 활용하여 30B 총 파라미터 중 3B만 활성화하며, MMLU-Redux, C-Eval, AutoLogi 등 다양한 벤치마크에서 상위권 성능을 기록하였다. 14B, 27B, 32B 등 기존 Dense 모델들과 비교 시, Qwen3-30B-A3B는 수학 및 논리 문제 해결 능력에서 경쟁력 있는 점수를 보여주며, 특히 AutoLogi와 Arena-Hard 등에서 우수한 성능을 나타냈다.\n",
            "\n",
            "또한, Qwen3-4B 및 Qwen3-8B 모델은 상대적으로 작은 규모임에도 불구하고 추론(Thinking) 능력을 갖춘 모델로 평가되어, DeepSeek-R1 및 Distill-Qwen 시리즈와 비교해도 유의미한 성능을 보였다. 이들 모델은 수학 문제(MATH-500, AIME)와 코딩 벤치마크 등에서 안정적인 결과를 내며, 특히 Qwen3-8B가 Qwen3-4B보다 전반적으로 더 높은 점수를 획득했다. 전체적으로 Qwen3 시리즈는 다양한 파라미터 규모와 아키텍처를 통해 다방면에서 강력한 성능과 효율성을 동시에 추구하고 있음을 확인할 수 있다.\n",
            "\n",
            "마지막으로, 다국어 및 창의적 글쓰기, 정렬 과제에서도 Qwen3 모델은 우수한 결과를 보여, 다국어 지원과 인간 친화적 인터랙션 측면에서도 경쟁력을 갖추었다. 특히 MLogiQA, PolyMath, MT-AIME2024 등 여러 평가에서 Qwen3-30B-A3B가 상위권을 차지하며, 실세계 문제 해결능력도 강화되었음을 시사한다. 이 보고서는 Qwen3 모델들이 다양한 자연어 처리 및 추론 과제에서 균형 잡힌 성능을 달성하면서, 최신 AI 연구의 발전 방향을 반영하고 있음을 보여준다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 25/55 [02:52<03:29,  6.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 24\n",
            "Qwen3 시리즈 모델들은 다양한 벤치마크에서 우수한 성능을 보여주며, 특히 추론 및 정렬(alignment) 작업에서 두드러진 결과를 나타냈다. Qwen3-8B 모델은 8B 파라미터를 가진 Dense 아키텍처로서 MATH-500, AIME, ZebraLogic, AutoLogi 등 수학 및 텍스트 추론 과제에서 최고 점수를 기록했고, Creative Writing과 WritingBench 같은 창의적 글쓰기 작업에서도 다른 모델 대비 높은 성능을 보였다. 또한, 멀티링구얼 작업에서도 Multi-IF, INCLUDE, MMMLU 14개 언어 등에서 강점을 보이며 다국어 처리 능력이 뛰어남을 입증했다.\n",
            "\n",
            "비교 대상인 DeepSeek-R1-Distill-Qwen-14B, 32B 모델과 Qwen3-4B 모델과의 비교에서 Qwen3-8B는 전반적으로 높은 성능을 유지하며, 특히 Alignment 벤치마크인 IFEval strict prompt, Arena-Hard, AlignBench 등에서 최고 혹은 두 번째로 우수한 결과를 냈다. 코딩 및 에이전트 관련 작업에서도 BFCL v3, LiveCodeBench v5, CodeForces 문제 해결에서 좋은 성과를 보여, 다방면에서 균형 잡힌 성능을 갖춘 모델임을 확인할 수 있다.\n",
            "\n",
            "한편, Qwen3-4B 모델도 파라미터 수가 적음에도 불구하고 상당히 경쟁력 있는 성능을 내며, MLogiQA, PolyMath 등의 복잡한 추론 과제에서 의미 있는 결과를 보였다. 전반적으로 Qwen3 시리즈는 기존 Qwen2.5 및 LLaMA-3.1, Gemma-3 같은 동급 모델 대비 한층 개선된 성능을 제공하며, 특히 추론, 정렬, 멀티링구얼, 창의적 작업 등 다차원적인 언어 이해 및 생성 능력에서 강점을 가진 최신 거대 언어 모델임을 보여준다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 26/55 [03:01<03:40,  7.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 25\n",
            "Qwen3 시리즈와 이전 버전인 Qwen2.5 및 기타 모델들의 성능 비교를 통해 Qwen3의 우수성을 확인할 수 있다. Qwen3-14B 모델은 14억 개의 파라미터를 가진 Dense 아키텍처로, 다양한 벤치마크에서 높은 점수를 기록했다. 예를 들어, MMLU-Redux에서 80.0점, GPQA-Diamond에서 45.5점, C-Eval에서 78.0점으로 전반적인 자연어 이해 및 추론 능력이 크게 향상되었다. 또한, Alignment 테스트에서도 IFEval strict prompt 81.0점, Arena-Hard 68.3점으로 인간과의 상호작용 품질 역시 개선되었다. 수학 및 논리 추론, 코딩 능력 평가에서도 Qwen3-14B가 우수한 성능을 나타내면서 종합적인 모델 성능 향상을 입증했다.\n",
            "\n",
            "Qwen3-4B와 8B 모델도 비슷한 아키텍처를 기반으로 하면서 각각 4억 및 8억 개의 파라미터를 가졌고, 다양한 벤치마크에서 경쟁력 있는 결과를 보였다. 특히 MATH-500, AIME, AutoLogi 등 수학 및 논리 문제에서 높은 점수를 기록하여 복잡한 문제 해결 능력이 뛰어남을 보여주었다. 다국어 평가에서도 Qwen3 모델군은 다방면에서 우수한 성능을 나타내어, 글로벌 환경에서 활용 가능성을 높였다. 코딩 및 에이전트 관련 벤치마크에서는 LiveCodeBench, BFCL v3 등의 평가에서 고른 점수를 획득하여 실용적인 응용에 적합함을 확인할 수 있다.\n",
            "\n",
            "더 작은 규모의 Qwen3-1.7B와 0.6B 모델은 각각 1.7억과 0.6억 파라미터를 가진 모델로, 다른 소형 모델들과 비교했을 때도 월등한 성능을 보였다. 예를 들어, MMLU-Redux에서 73.9점과 55.6점을 기록하며 동일 규모 대비 우수함을 입증했다. 이처럼 Qwen3 시리즈는 다양한 규모에서 균형 잡힌 성능과 효율성을 제공하며, 대형 모델부터 경량화 모델까지 폭넓게 활용할 수 있는 가능성을 보여준다. 전반적으로 Qwen3는 자연어 처리, 추론, 코딩, 다국어 등 다방면에서 전작 대비 현격한 성능 향상을 이루어낸 차세대 언어 모델임을 알 수 있다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▉     | 27/55 [03:08<03:24,  7.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 26\n",
            "Qwen3 시리즈 모델은 다양한 크기와 아키텍처를 바탕으로 여러 벤치마크 테스트에서 우수한 성능을 보였다. Qwen3-1.7B 모델은 1.7억 개의 활성화 파라미터를 가지고 있으며, MMLU-Redux, C-Eval, LiveBench 등 일반 및 정렬(Alignment) 작업에서 다른 최신 모델들을 능가하는 점수를 기록했다. 특히, 수학 및 텍스트 추론(Math & Text Reasoning) 분야에서 MATH-500, AutoLogi, ZebraLogic 테스트에서 90% 이상의 높은 정확도를 나타내며 뛰어난 추론 능력을 증명했다.\n",
            "\n",
            "비교 대상인 DeepSeek-R1-Distill-Llama-8B와 같은 대형 모델과 비교했을 때도 Qwen3-1.7B는 적은 파라미터 수에도 불구하고 상당히 경쟁력 있는 성능을 보여, 효율성과 성능 간의 균형이 잘 맞춰져 있음을 알 수 있다. 또한, Qwen3-0.6B 모델도 소형 모델임에도 불구하고 여러 작업에서 중간 이상의 점수를 획득하여 경량화 모델로서의 가능성을 입증했다. 다국어 작업 및 코딩, 에이전트 관련 벤치마크에서도 안정적인 성능을 보여 전반적인 활용도가 높다.\n",
            "\n",
            "한편, 비추론(Non-thinking) 작업에 대해 비교한 결과 Qwen3-1.7B는 Phi-4-mini, Qwen2.5 시리즈 등과 견줘도 높은 점수를 기록하여 다재다능함을 강조했다. 특히 일반 및 정렬 작업에서 우수한 성능을 나타내며, 이는 Qwen3 모델이 다양한 분야에서 범용적으로 사용할 수 있는 강력한 언어 모델임을 시사한다. 이러한 결과는 Qwen3가 최첨단 기술을 적용해 설계된 점과, 다양한 자연어 이해 및 생성 작업에서 탁월한 성능을 제공함을 보여준다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████     | 28/55 [03:16<03:23,  7.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 27\n",
            "Qwen3 시리즈는 다양한 크기의 언어 모델(0.6B에서 30B 파라미터까지)을 포함하며, Strong-to-Weak Distillation 기법을 통해 적은 활성 파라미터만으로도 뛰어난 추론 능력을 보유하도록 설계되었다. 특히 Qwen3-30B-A3B와 Qwen3-14B 모델은 기존 Qwen2.5-32B-Instruct 모델보다 적은 파라미터와 활성화된 파라미터로도 대부분의 벤치마크에서 우수한 성능을 기록해 효율성과 비용 측면에서 개선을 이루었다. 다양한 벤치마크(일반, 정렬, 수학 및 추론, 에이전트 및 코딩, 다국어 과제)에서 Qwen3 모델은 경쟁 모델들과 비교해 높은 점수를 획득하며 우수성을 입증했다.\n",
            "\n",
            "특히 Qwen3-8B 및 4B 모델은 thinking 모드에서 DeepSeek-R1-Distill-Qwen-14B/32B와 비교되었고, non-thinking 모드에서는 LLaMA-3.1-8B-Instruct, Gemma-3-12B-IT, Qwen2.5-7B/14B-Instruct 등과 경쟁하여 좋은 성과를 보였다. 저용량 모델인 Qwen3-1.7B와 0.6B 역시 DeepSeek 모델과 비교되며 경량화된 모델임에도 불구하고 상당한 성능을 유지했다. 이러한 결과는 Qwen3 시리즈가 모델 크기와 계산 자원에 따른 유연성을 갖추면서도 실용적인 수준의 성능을 제공함을 시사한다.\n",
            "\n",
            "종합적으로, Qwen3 모델군은 강력한 증류 기법과 효율적인 파라미터 활용을 통해 다양한 과제에서 탁월한 성능을 나타내며, 대규모 모델 대비 비용 효율적인 대안으로 자리매김하고 있다. 이로써 실무 및 연구 환경에서 다양한 규모와 용도에 맞는 최적의 선택지를 제공하고, 향후 인공지능 언어 모델의 확장성과 적용 가능성을 제고하는 데 기여할 것으로 기대된다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 29/55 [03:21<02:57,  6.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 28\n",
            "Qwen3 시리즈 모델들은 훨씬 적은 활성화 및 총 파라미터 수로도 우수한 성능을 보이며, 비용 효율적인 운영이 가능하다. Qwen3-8B 및 4B 모델들은 기존의 DeepSeek 및 LLaMA, Gemma, Qwen2.5 시리즈 모델들과 다양한 모드(생각 모드와 비생각 모드)에서 비교 평가되었으며, 전반적으로 더 많은 파라미터를 가진 기존 모델들보다 뛰어난 성능을 나타냈다. 이러한 결과는 Strong-to-Weak Distillation 기법의 효과를 입증하며, 경량화와 비용 절감이 동시에 가능함을 보여준다.\n",
            "\n",
            "Qwen3 모델은 ‘Thinking Budget’ 조절을 통해 수학, 코딩, STEM 분야 벤치마크에서 지능 수준을 점진적으로 향상시킬 수 있음을 확인했다. 특히, 출력 길이를 32K 이상으로 확장할 경우 향후 성능 개선 가능성도 제시되어 추가 연구가 기대된다. 또한, on-policy distillation 기법은 강화학습 대비 GPU 사용 시간을 약 1/10 수준으로 줄이면서도 더 우수한 성능을 달성했다. 이는 교사 모델의 로짓을 활용해 학생 모델의 탐색 공간과 추론 능력을 효과적으로 확장시킨 결과로, AIME 벤치마크에서의 pass@64 점수 향상으로도 확인된다.\n",
            "\n",
            "마지막으로, Thinking Mode Fusion 및 일반 강화학습의 효과에 대한 평가도 진행되었으며, Qwen3의 다양한 학습 및 추론 전략이 모델의 전반적 성능 향상에 기여하고 있음을 시사한다. 이 연구는 대규모 언어 모델의 경량화와 효율적 학습 방법에 대한 중요한 실험적 근거를 제공하며, 앞으로의 확장성과 비용 절감 측면에서 큰 의미를 가진다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▍    | 30/55 [03:29<02:54,  6.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 29\n",
            "Qwen3-32B 모델의 성능 향상을 위해 세 가지 학습 단계인 Reasoning RL, Thinking Mode Fusion, General RL이 적용되었다. 각 단계는 다양한 벤치마크 데이터셋에서 평가되었으며, 특히 Thinking Mode Fusion 단계에서 대부분의 태스크에서 유의미한 성능 향상이 관찰되었다. 예를 들어, CounterFactQA, LengthCtrl, ThinkFollow, ToolUse 같은 내부 벤치마크에서는 최대 10% 이상의 점수 상승이 나타났다. 이는 모델이 상황에 맞는 사고 모드를 전환하고 복잡한 도구 사용 및 길이 조절 같은 세부적인 능력을 강화했음을 보여준다. 반면, 일부 지식 및 STEM 관련 벤치마크에서는 소폭 성능 저하가 있었으나, 전반적인 효율성 향상과 균형 잡힌 성능 개선에 성공했다.\n",
            "\n",
            "또한, Off-policy Distillation, Reinforcement Learning, On-policy Distillation 세 가지 방법을 비교한 결과, On-policy Distillation이 가장 적은 GPU 시간(1,800시간)으로 대규모 성능 향상을 이뤄냈다. AIME, MMLU, GPQA 등 주요 수학 및 언어 이해 벤치마크에서 On-policy Distillation 단계가 최고 성능을 기록하며 효율성과 효과성을 입증했다. 이러한 과정은 Qwen3-32B가 복잡한 추론, 대화, 도구 활용 능력을 동시에 향상시키도록 설계된 점을 시사한다. 특히, Thinking Mode Fusion과 General RL의 결합이 모델의 다중 사고 모드 전환 및 일반화 능력 강화에 중요한 역할을 한 것으로 평가된다.\n",
            "\n",
            "결론적으로, Qwen3-32B의 학습 전략은 강화 학습과 사고 모드 융합을 통해 다양한 유형의 문제 해결 능력을 크게 개선하였다. 내부 및 외부 벤치마크 모두에서 균형 잡힌 성능 향상이 관찰되며, 특히 창의적 글쓰기, 다중 턴 대화, 도구 호출 안정성 등 실제 응용에 중요한 영역에서 뛰어난 결과를 보였다. 향후 연구에서는 이러한 멀티 모드 학습 전략을 더욱 정교화하여 모델의 추론 능력과 사용자 맞춤형 반응 생성 능력을 강화할 가능성이 크다. Qwen3-32B는 현재의 AI 모델 개발에 있어 사고 모드 제어와 강화 학습의 융합이 효과적인 접근임을 입증하는 중요한 사례라 할 수 있다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▋    | 31/55 [03:35<02:41,  6.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 30\n",
            "Qwen3는 사고 모드(thinking mode)와 비사고 모드(non-thinking mode)를 통합한 최신 대형 언어 모델로, 36조 토큰의 방대한 데이터로 사전학습되어 119개 언어 및 방언을 이해하고 생성할 수 있다. 3단계 훈련에서 모델은 두 가지 모드를 전환하는 능력을 갖추었으며, 일반적인 지식 및 명령 수행 능력이 향상되었다. 4단계에서는 두 모드 모두에서 정확한 모드 전환이 가능해져, ThinkFollow 점수가 98.9로 크게 개선되었다.\n",
            "\n",
            "그러나 지식, STEM, 수학, 코딩 분야에서는 사고 모드 융합과 일반 강화학습이 큰 성과를 내지 못했으며, 오히려 AIME’24와 LiveCodeBench 같은 고난도 과제에서는 사고 모드 성능이 다소 떨어졌다. 이는 모델이 더 광범위한 일반 임무에 맞춰 훈련되면서 전문적인 복잡 문제 해결 능력이 일부 희생된 결과로 분석된다. 연구진은 이러한 성능 저하를 감수하고 모델의 전반적인 다재다능성을 높이는 방향을 선택하였다.\n",
            "\n",
            "향후 연구는 데이터 품질과 다양성을 높여 사전학습 규모를 확장하고, 모델 아키텍처와 훈련법 개선을 통해 효과적인 압축과 극장 맥락 확장 등을 추구할 계획이다. 또한 환경 피드백을 활용하는 에이전트 기반 강화학습에 컴퓨팅 자원을 집중하여, 추론 시간 확장이 필요한 복잡한 과제를 수행할 수 있는 에이전트를 개발하는 데 주력할 것이다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 32/55 [03:40<02:24,  6.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 31\n",
            "Qwen3 기술 보고서는 Qwen 시리즈 언어 모델의 장문 처리 능력과 성능 평가 결과를 상세히 다루고 있다. 특히 RULER 벤치마크를 통해 4K부터 128K까지 다양한 길이의 문맥에서 모델들이 어떻게 작동하는지를 실험하였다. Qwen2.5 시리즈의 72B 모델이 가장 높은 평균 점수(95.1)를 기록하며 장문 이해에 강점을 보였고, Qwen3 시리즈는 상대적으로 작은 파라미터 수임에도 불구하고 우수한 성능을 나타냈다. Qwen3-14B 모델은 평가 데이터가 공개되지 않았으나, 4B 및 8B 모델은 각각 85.2와 89.1의 평균 점수로 실용적인 성능을 증명하였다. 이 결과는 Qwen 모델들이 대규모 문맥에서도 안정적으로 정보를 처리할 수 있음을 시사한다.\n",
            "\n",
            "또한, 보고서는 비사고(non-thinking) 모드에서의 성능 차이도 분석하여, 다양한 운영 모드에서의 모델 효율성을 비교하였다. Qwen 시리즈의 모델들은 긴 문맥을 다루는 능력에서 일관된 성능 향상을 보여, 실제 응용에서 긴 텍스트 처리에 적합함을 보여준다. 전반적으로 Qwen3는 이전 버전 대비 모델 크기와 성능의 균형을 잘 맞추었으며, 초대규모 문맥 이해 능력 향상에 초점을 맞추었다. 이러한 결과는 자연어 처리 분야에서 대형 언어 모델의 활용 가능성을 넓히는 중요한 진전으로 평가된다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 33/55 [03:52<02:56,  8.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 32\n",
            "Qwen3 기술 보고서에서는 긴 문맥 처리와 다국어 능력 평가 결과를 중심으로 Qwen3 시리즈 모델들의 성능을 상세히 분석하였다. 긴 문맥 처리 능력을 측정하기 위해 RULER 벤치마크를 사용하였으며, 비사고 모드에서 Qwen3 모델들이 동일 크기의 Qwen2.5 모델들보다 우수한 성능을 보였다. 사고 모드에서는 성능이 다소 저하되었는데, 이는 해당 작업들이 추론보다는 정보 검색에 초점이 맞춰져 있어 사고 모드의 추가적인 추론 과정이 오히려 방해가 될 수 있음을 시사한다. 연구팀은 향후 사고 모드에서도 긴 문맥 처리 능력을 향상시키기 위한 노력을 지속할 계획임을 밝혔다.\n",
            "\n",
            "다국어 능력 평가에서는 스페인어, 프랑스어, 포르투갈어, 이탈리아어, 아랍어, 일본어, 한국어, 인도네시아어, 러시아어, 베트남어, 독일어, 태국어 등 다양한 언어에서 Qwen3 모델이 경쟁력 있는 성능을 나타냈다. 특히 Belebele 벤치마크를 활용해 80개 지원 언어에 대해 평가한 결과, Qwen3는 유사 크기의 Gemma 모델과 비교해 대등한 성능을 보였으며 Qwen2.5 모델 대비 월등히 뛰어났다. 이를 통해 Qwen3 시리즈가 광범위한 언어 환경에서도 안정적이고 우수한 자연어 이해 능력을 갖추었음을 확인할 수 있다.\n",
            "\n",
            "종합적으로, Qwen3 모델은 긴 문맥 처리와 다국어 이해 능력에서 전 세대 모델 대비 진일보한 성능을 기록하였다. 비사고 모드에서의 긴 문맥 처리 우수성과 다국어 벤치마크에서의 강력한 경쟁력은 Qwen3의 핵심 강점으로 평가된다. 향후 연구에서는 사고 모드의 긴 문맥 처리 성능 최적화와 더불어 다양한 언어와 작업에서 모델의 활용도를 더욱 높이는 방향으로 발전할 전망이다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 34/55 [03:58<02:34,  7.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 33\n",
            "Qwen3 시리즈 모델들은 다양한 벤치마크 평가에서 경쟁력 있는 성능을 보여주며, 특히 강력한 다국어 능력을 입증하였다. Belebele 벤치마크를 활용해 80개 지원 언어(최적화되지 않은 42개 언어 제외)에 대해 평가한 결과, Qwen3는 유사 크기의 Gemma 모델과 대등한 성능을 보였고, 이전 버전인 Qwen2.5 대비 현저히 우수한 성능을 나타냈다. 이는 Qwen3가 다국어 자연어 이해 분야에서 뛰어난 범용성과 확장성을 갖추었음을 시사한다.\n",
            "\n",
            "스페인어를 포함한 여러 언어별 세부 성능 지표를 보면, Qwen3-235B-A22B 모델은 다양한 평가 항목에서 상위권 성적을 기록했으며, GPT-4o-2024-1120 및 Gemma-3-27b-IT 등의 다른 최신 모델과 비교해도 경쟁력 있는 결과를 보였다. 특히 사고 모드(Thinking Mode)에서 더욱 강력한 성능을 발휘해 복합적인 언어 이해 및 추론 작업에 적합함을 확인할 수 있다. 이러한 성과는 Qwen3 시리즈가 다양한 크기와 구성으로 다양한 응용 환경에 유연하게 대응할 수 있음을 의미한다.\n",
            "\n",
            "종합적으로, Qwen3는 광범위한 언어와 작업에 대해 높은 정확도와 일관된 성능을 제공하며, 다국어 자연어 처리 분야에서 중요한 발전을 이뤘다. 이 모델들은 다국어 벤치마크에서 기존 모델들을 능가하거나 대등한 성능을 유지하면서, 특히 중대형 모델에서 뛰어난 효율성과 확장성을 보여줌으로써 향후 인공지능 언어 모델 연구 및 실무 적용에 큰 기여를 할 것으로 기대된다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▎   | 35/55 [04:05<02:26,  7.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 34\n",
            "Qwen3 시리즈는 다양한 크기와 설정으로 여러 벤치마크에서 우수한 성능을 보였다. 특히, 235B, 32B, 30B 모델은 프랑스어 및 다중 언어 벤치마크에서 상위권 점수를 기록하며 Gemini2.5-Pro 및 GPT-4o-2024-1120과 견줄 만한 성과를 냈다. 크기가 클수록 전반적으로 높은 정확도와 평균 점수를 나타냈으며, Thinking Mode에서 더 뛰어난 성능을 보이는 경향이 있다.\n",
            "\n",
            "Thinking Mode와 Non-thinking Mode 간에도 차이가 확인되었는데, Thinking Mode에서 Qwen3-235B-A22B와 Qwen3-32B가 가장 높은 평균 점수를 기록했다. 반면, Non-thinking Mode에서는 Qwen3-14B가 상대적으로 높은 점수를 보이며 다양한 벤치마크에서 안정적인 성능을 유지하였다. GPT-4o-2024-1120과 Gemma-3-27b-IT 등 경쟁 모델과 비교할 때도 Qwen3 모델군은 강력한 경쟁력을 갖추고 있음을 보여준다.\n",
            "\n",
            "이 보고서는 Qwen3 모델들이 다국어 및 복합적인 평가 기준에서 뛰어난 성과를 내며, 특히 대형 모델일수록 성능이 향상됨을 강조한다. 또한, 다양한 활용 시나리오에 맞춘 Thinking Mode의 중요성을 시사하며, 앞으로의 연구 및 개발 방향에 있어 모델 크기와 모드 선택이 핵심 요소임을 시사한다. 전반적으로 Qwen3는 최신 AI 언어 모델 분야에서 강력한 입지를 다지고 있음을 확인할 수 있다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 36/55 [04:11<02:10,  6.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 35\n",
            "Qwen3 시리즈 모델들은 다양한 크기(0.6B부터 235B까지)와 모드(Thinking Mode, Non-thinking Mode)에서 포르투갈어 벤치마크 테스트를 수행하여 성능을 평가받았다. Thinking Mode에서는 Qwen3-235B-A22B가 대부분의 벤치마크 항목에서 최고 혹은 두 번째로 높은 점수를 기록하며 평균 77.0점으로 가장 우수한 성능을 보였다. Qwen3-32B와 Qwen3-14B도 70점대 중반의 안정적인 성능을 나타내며 상위권에 위치했다.\n",
            "\n",
            "Non-thinking Mode에서는 전체적으로 점수가 다소 낮아지는 경향이 있었으며, Qwen3-235B-A22B가 평균 61.2점으로 가장 높은 평가를 받았다. GPT-4o-2024-1120과 Gemma-3-27b-IT 같은 경쟁 모델들도 다수 포함되었는데, 이들 모델은 Qwen3의 대형 모델에 비해 다소 낮은 점수를 받았다. 모델 크기가 커질수록 점수가 상승하는 경향이 뚜렷하게 나타났으며, 특히 30B 이상 모델들이 뛰어난 성능을 보였다.\n",
            "\n",
            "이 결과는 Qwen3 시리즈가 대형 언어모델 시장에서 경쟁력 있는 성능을 발휘함을 입증하며, Thinking Mode가 활성화될 때 모델의 문제 해결 능력과 이해력이 크게 향상됨을 보여준다. 또한, 다양한 벤치마크 항목에서 고르게 높은 성능을 달성한 점은 Qwen3 모델의 다재다능함을 시사한다. 전반적으로 Qwen3는 포르투갈어 처리 분야에서 강력한 성능을 가진 모델임을 확인할 수 있다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 37/55 [04:18<02:03,  6.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 36\n",
            "Qwen3 시리즈는 다양한 크기의 모델로 구성되어 있으며, 이들의 성능은 벤치마크 점수를 통해 평가된다. 특히 이탈리아어 벤치마크에서 Qwen3-32B, Qwen3-30B-A3B, Qwen3-14B 모델이 높은 점수를 기록하며 우수한 성능을 보였다. Gemini2.5-Pro 모델이 다중 지능형 문제 해결 능력에서 가장 뛰어난 성능을 나타냈고, Qwen3-235B-A22B와 Qwen3-32B도 상위권에 위치했다. 사고 모드(Thinking Mode)에서의 성능은 비사고 모드(Non-thinking Mode)보다 전반적으로 우수했으며, 이는 모델의 문제 해결 능력과 이해력이 향상되었음을 시사한다. 작은 모델일수록 점수가 낮아지는 경향을 보였지만, Qwen3 시리즈 전체적으로 균형 잡힌 성능을 나타냈다.\n",
            "\n",
            "또한, Qwen3 모델들은 GPT-4o-2024-1120과 Gemma-3-27b-IT 같은 최신 경쟁 모델과 비교했을 때도 상당한 경쟁력을 갖추고 있다. 특히 사고 모드에서의 Qwen3-235B-A22B 모델은 중간 크기 모델 대비 높은 점수를 달성하여, 대규모 모델이 갖는 장점을 잘 활용함을 보여주었다. 이와 함께, PolyMath, MMMLU, MT-AIME24와 같은 다양한 평가 지표에서 Qwen3 시리즈가 고르게 좋은 성과를 냈다. 비사고 모드에서는 점수가 다소 낮아지긴 했으나, 여전히 주요 벤치마크에서 경쟁력 있는 성능을 유지했다. 이는 Qwen3 모델들이 다양한 상황에서 유연하게 적용 가능함을 의미한다.\n",
            "\n",
            "종합적으로, Qwen3 기술 보고서는 다국어 및 다중 과제 수행능력에서 Qwen3 시리즈가 뛰어난 성능을 보이며, 특히 사고 모드에서의 향상된 능력이 주목할 만하다고 결론짓는다. 모델 크기에 따른 성능 차이가 명확하나, 14B 이상 모델에서부터는 고성능을 안정적으로 발휘하는 경향을 보인다. 경쟁 모델과 비교해도 우수한 점수를 기록하며, 이는 Qwen3 시리즈가 현 시점에서 매우 경쟁력 있는 대형 언어 모델임을 시사한다. 앞으로도 다양한 언어와 문제 영역에서의 확장 가능성이 기대된다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 38/55 [04:26<02:01,  7.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 37\n",
            "Qwen3 시리즈는 다양한 크기와 구성으로 다국어 벤치마크 평가에서 뛰어난 성능을 보이고 있다. 특히 Qwen3-30B-A3B와 Qwen3-14B 모델은 아랍어와 일본어 벤치마크에서 평균 점수 60점 이상을 기록하며 상위권에 위치한다. 이 모델들은 MLogiQA, INCLUDE, MMMLU, MT-AIME24, PolyMath 등 다양한 태스크에서 높은 점수를 획득해 다재다능한 자연어 이해 능력을 입증했다. 또한, 생각(Thinking) 모드를 활성화한 상태에서의 성능이 비활성화 상태보다 현저히 우수해, 사고 과정의 모방이 모델의 문제 해결 능력을 크게 향상시킨다는 점이 확인되었다. 이로써 Qwen3 모델들은 대규모 언어 모델 분야에서 경쟁력 있는 기술적 진보를 보여준다.\n",
            "\n",
            "특히 아랍어 벤치마크에서 Gemini2.5-Pro 모델이 최고 점수를 기록했으나, Qwen3-235B-A22B와 Qwen3-32B 등도 근접한 성과를 내며 강력한 경쟁력을 나타냈다. 일본어 평가에서도 Gemini2.5-Pro가 우수한 성능을 보였고, Qwen3 시리즈는 다양한 크기별로 점진적인 성능 향상을 보여 모델 크기와 성능 간의 상관관계를 뒷받침한다. 반면, 비슷한 크기임에도 생각 모드 미적용 모델은 낮은 점수를 기록, 사고 과정 시뮬레이션의 중요성이 강조된다. 이 결과들은 다국어 지원과 사고 모드 도입이 대형 언어 모델의 실제 활용도를 높이는 핵심 요소임을 시사한다.\n",
            "\n",
            "종합하면, Qwen3 모델은 다양한 언어와 태스크에서 뛰어난 성능을 보이며, 사고 모드 활성화가 성능 개선에 크게 기여한다. 모델 크기와 구성에 따라 점진적으로 성능이 향상되며, Gemini 시리즈와의 비교에서도 경쟁력 있는 결과를 산출한다. 이러한 특성은 Qwen3가 다국어 자연어 처리와 복잡한 문제 해결에 적합한 차세대 언어 모델임을 입증한다. 앞으로도 사고 모드 기술과 모델 최적화를 통해 더욱 진보된 성능을 기대할 수 있다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 39/55 [04:31<01:44,  6.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 38\n",
            "Qwen3 기술 보고서는 다양한 크기와 구성의 Qwen3 모델들이 일본어와 한국어 벤치마크 테스트에서 우수한 성능을 보였음을 보여준다. 특히, 235B 파라미터를 가진 Qwen3-235B-A22B 모델은 일본어 및 한국어 테스트에서 Thinking Mode를 활용할 때 평균 점수 76.0과 75.1로 가장 높은 성과를 기록하였다. 이 모델은 MLogiQA, INCLUDE, MMMLU, MT-AIME24, PolyMath 등 다양한 평가 항목에서 탁월한 결과를 나타내며, 대형 모델의 강점을 입증하였다.\n",
            "\n",
            "또한, Qwen3-32B 및 Qwen3-30B-A3B 모델들도 전반적으로 높은 점수를 기록하며 대형 모델군 내에서 경쟁력 있는 성능을 보였다. 반면, 소형 모델군인 Qwen3-1.7B 및 Qwen3-0.6B는 상대적으로 낮은 점수를 기록하였는데, 이는 모델 크기에 따른 성능 차이를 명확히 드러낸다. Thinking Mode와 Non-thinking Mode 간에는 성능 차이가 존재하며, Thinking Mode가 전반적으로 더 우수한 결과를 도출하였다.\n",
            "\n",
            "보고서에서는 Gemini2.5-Pro와 GPT-4o-2024-1120 등 타 모델과의 비교도 포함되어, Qwen3 시리즈가 현존하는 여러 대형 언어 모델과 견줄만한 성능을 갖추었음을 강조한다. 이러한 결과는 Qwen3 모델이 다국어 및 다양한 과제에서 효과적인 활용 가능성을 제시하며, 향후 연구 및 응용에 중요한 참고 자료가 될 것으로 기대된다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 40/55 [04:35<01:27,  5.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 39\n",
            "Qwen3 기술 보고서의 일부 벤치마크 결과는 한국어와 인도네시아어에서 다양한 모델들의 성능을 비교 분석한 내용이다. 한국어 평가에서 Gemini2.5-Pro와 Qwen3-235B-A22B 모델이 전반적으로 높은 점수를 기록했으며, 특히 복합적인 사고 모드(Thinking Mode)에서 우수한 성능을 보였다. 반면 비사고 모드(Non-thinking Mode)에서는 GPT-4o와 Gemma-3-27b-IT 등이 상대적으로 좋은 성과를 나타냈다.\n",
            "\n",
            "인도네시아어 벤치마크 결과 역시 Gemini2.5-Pro와 Qwen3-235B-A22B 모델이 가장 높은 평균 점수를 기록하며, 대형 모델들이 다국어 처리 능력에서 강점을 보였다. Qwen3 시리즈는 모델 크기에 따라 성능 차이를 보였는데, 대형 모델일수록 높은 점수를 획득하는 경향을 보였다. 또, 사고 모드에서는 전반적으로 더 나은 성능을 나타냈으며, 이는 복잡한 문제 해결능력과 관련이 있다.\n",
            "\n",
            "전반적으로 Qwen3 계열 대형 언어 모델들은 다국어 벤치마크에서 강력한 성능을 발휘하며, 특히 사고 모드에서의 활용이 효과적임을 확인할 수 있었다. Gemini2.5-Pro 모델과 일부 Qwen3-235B 모델은 다양한 언어 환경에서 균형 잡힌 성능을 보여주어, 다국어 자연어 처리 및 복합 문제 해결에 적합한 모델임을 시사한다. 이러한 결과는 향후 다국어 인공지능 서비스 및 연구 개발에 중요한 참고 자료로 활용될 수 있다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▍  | 41/55 [04:43<01:29,  6.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 40\n",
            "Qwen3 시리즈 모델들은 인도네시아어 및 러시아어 벤치마크에서 다양한 성능을 보였다. 인도네시아어에서는 Gemini2.5-Pro와 Qwen3-235B-A22B 모델이 THINKING Mode에서 평균 점수 75.2로 가장 높은 성능을 기록했고, Qwen3-30B-A3B와 Qwen3-32B 모델도 상위권에 위치했다. NON-THINKING Mode에서는 Qwen3-235B-A22B가 평균 56.1점으로 가장 우수한 성능을 나타냈다. 러시아어 벤치마크에서는 Qwen3-235B-A22B가 THINKING Mode에서 68.9점으로 최고 점수를 기록하며 Gemini2.5-Pro와 근소한 차이를 보였다. 전반적으로 대형 모델일수록 벤치마크 점수가 높고 THINKING Mode가 NON-THINKING Mode보다 더 우수한 결과를 나타내는 경향이 있다.\n",
            "\n",
            "각 언어별로 평가된 여러 하위 벤치마크(INCLUDE, MT-AIME24, PolyMath 등)에서 Qwen3-235B-A22B가 일관되게 상위권을 유지하며 뛰어난 다국어 처리 능력을 입증했다. 특히 인도네시아어에서 MT-AIME24 점수가 87.2점으로 가장 높아 번역 및 언어 이해에서 강점을 보였다. 러시아어에서는 PolyMath와 MT-AIME24 영역에서 Qwen3-235B-A22B의 점수가 각각 53.1점과 80.4점으로 우수한 성과를 냈다. 이 데이터는 Qwen3 시리즈가 대규모 언어 모델로서 다국어 환경에서 균형 잡힌 성능을 갖추고 있음을 보여준다. 반면 저용량 모델(Qwen3-0.6B 등)은 상대적으로 낮은 점수를 기록해 모델 크기와 성능 간 상관관계를 확인할 수 있다.\n",
            "\n",
            "요약하면, Qwen3-235B-A22B 모델은 THINKING Mode에서 다국어 벤치마크에서 최상위권 성능을 발휘하며, Gemini2.5-Pro와 경쟁하는 수준이다. 대형 모델들은 복잡한 언어처리 과제에서 강점을 가지며, 다양한 벤치마크를 통해 그 성능이 검증되었다. NON-THINKING Mode에서는 전반적으로 점수가 낮으나, 여전히 Qwen3 시리즈는 경쟁력 있는 결과를 보여주고 있다. 이러한 결과는 Qwen3 모델군이 다국어 AI 응용에 적합한 고성능 언어 모델임을 시사한다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▋  | 42/55 [04:48<01:18,  6.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 41\n",
            "Qwen3 시리즈는 다양한 크기(0.6B부터 235B까지)의 언어 모델로 구성되어 있으며, 베트남어 벤치마크에서 뛰어난 성능을 보이고 있다. 특히 Qwen3-235B-A22B 모델은 Thinking Mode에서 평균 점수 75.2로 가장 우수하며, MLogiQA, INCLUDE, MT-AIME24, PolyMath 등 다양한 테스트에서 상위권을 차지했다. Gemini2.5-Pro와 Qwen3-32B도 높은 성능을 보여 대형 모델들이 복합적인 언어 이해와 추론 능력에서 강점을 가진다.\n",
            "\n",
            "비교군인 GPT-4o-2024-1120, Gemma-3-27b-IT, Qwen2.5-72B-Instruct 등과 비교 시 Qwen3 시리즈의 최신 대형 모델들은 대부분 Thinking Mode에서 우수한 점수를 기록하였다. 반면 Non-thinking Mode에서는 전반적으로 낮은 성능을 보여, 사전 학습된 추론 및 사고 기능을 활용하는 것이 성능 향상에 크게 기여함을 알 수 있다. 또한, 모델 크기가 클수록 벤치마크 점수가 증가하는 경향이 뚜렷하여, 모델 용량과 성능 간의 상관관계가 명확히 관찰된다.\n",
            "\n",
            "종합하면 Qwen3 시리즈는 다양한 크기별로 균형 잡힌 성능을 제공하며, 특히 대형 모델들이 베트남어를 포함한 다국어 벤치마크에서 뛰어난 추론 능력을 입증하였다. Thinking Mode 활용 시 복잡한 문제 해결 능력이 극대화되며, 이는 차세대 다국어 AI 언어 모델 개발에 중요한 시사점을 제공한다. 향후 연구에서는 더 다양한 언어와 과제에 대한 평가 및 최적화가 기대된다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 43/55 [04:56<01:19,  6.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 42\n",
            "Qwen3 시리즈는 다양한 크기와 구성으로 독일어 벤치마크에서 높은 성능을 보이고 있다. 특히 Qwen3-235B-A22B 모델은 Thinking Mode에서 평균 74.0점으로 가장 우수한 성적을 기록했으며, INCLUDE, MMMLU, MT-AIME24, PolyMath 등 여러 평가 항목에서 두드러진 결과를 나타냈다. 중간 크기 모델들인 Qwen3-32B와 Qwen3-30B-A3B도 각각 70.2점과 68.2점의 평균 점수를 보여 경쟁력 있는 성능을 유지했다. 반면 비사고(Non-thinking) 모드에서는 전반적으로 점수가 낮아, 사고 모드가 성능 향상에 중요한 역할을 하는 것으로 보인다. GPT-4o-2024-1120과 Gemma-3-27b-IT 같은 다른 최신 모델과 비교해도 Qwen3의 대형 모델들이 우위를 점하고 있다.\n",
            "\n",
            "모델 크기별로 살펴보면, 더 큰 파라미터 수를 가진 모델일수록 독일어 처리 능력이 향상되는 경향이 뚜렷하다. 예를 들어, Qwen3-235B-A22B는 71.4점의 INCLUDE 점수와 86.0점의 MMMLU 점수를 기록해 가장 뛰어난 점수를 받았다. 반면 0.6B와 1.7B 크기의 소형 모델들은 평균 점수가 30점대 초반에서 40점대 미만에 머무르며 성능 차이를 보여준다. 이와 함께, Thinking Mode와 Non-thinking Mode 간의 점수 차이가 모델 성능 평가에 있어 중요한 변수임을 확인할 수 있다. 즉, 사고 모드 활성화가 모델의 언어 이해 및 추론 능력을 크게 향상시키는 핵심 요소임을 시사한다.\n",
            "\n",
            "결론적으로 Qwen3 시리즈는 대형 모델 중심으로 독일어 벤치마크에서 탁월한 성과를 보이며, 사고 모드 활성화가 성능에 긍정적인 영향을 미친다. 이는 자연어 처리 분야에서 대규모 언어 모델의 중요성과 함께, 적절한 모드 설정이 모델 활용 극대화에 필수적임을 강조한다. 향후 연구에서는 중형 및 소형 모델의 효율성 개선과 함께 사고 모드 최적화가 중요한 연구 방향이 될 것으로 기대된다. 이와 같은 결과들은 Qwen3가 다국어 및 다양한 언어 벤치마크에서 경쟁력 있는 AI 모델임을 입증한다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 44/55 [05:01<01:08,  6.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 43\n",
            "Qwen3 모델 시리즈는 다양한 크기와 구성으로 여러 언어에 대해 벤치마크 테스트를 수행하여 성능을 평가했다. 특히 태국어(Thai) 벤치마크 결과에서 Qwen3-235B-A22B 모델이 Thinking Mode에서 71.4점으로 우수한 성능을 보였으며, Gemini2.5-Pro와 Qwen3-32B 모델도 높은 점수를 기록했다. Non-thinking Mode에서는 전반적으로 점수가 낮았으나, Qwen3-235B-A22B 모델이 37.4점으로 비교적 좋은 결과를 나타냈다. 이는 대형 모델일수록 복잡한 사고가 필요한 작업에서 뛰어난 성능을 발휘함을 시사한다. \n",
            "\n",
            "또한, Qwen3는 Belebele Benchmark를 통해 인도유럽어족(Indo-European) 등 여러 언어 가족을 지원하며, 총 40개의 언어와 다양한 문자 체계를 포괄한다. 이로써 Qwen3는 다양한 언어적 배경을 아우르는 다국어 지원 능력을 갖추었음을 확인할 수 있다. 벤치마크 결과는 모델 크기와 학습 방법에 따라 성능 차이가 크며, 특히 Thinking Mode에서의 성능 향상이 두드러진다. \n",
            "\n",
            "종합적으로 Qwen3는 대규모 다국어 모델로서, 복잡한 언어 이해와 추론 작업에서 우수한 성능을 보이고 다양한 언어와 문자 체계를 지원한다. 이는 다국어 자연어 처리와 인공지능 응용 분야에 광범위하게 활용될 가능성을 시사한다. 앞으로도 모델 최적화와 다양한 언어 지원 강화를 통해 성능이 더욱 향상될 것으로 기대된다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 45/55 [05:06<00:59,  5.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 44\n",
            "Qwen3 모델은 Belebele 벤치마크에서 다양한 언어 계통과 언어 코드를 지원하며, 인도-유럽어족, 중국-티베트어족, 아프로-아시아어족 등 총 16개 언어 계통에 걸쳐 76개 이상의 언어를 포괄한다. 특히 인도-유럽어족에서 40개의 언어를 지원하며, 그 외에도 오스트로네시아어족, 닥리비언어족, 튀르크어족 등 주요 언어 가족을 포함한다. 이처럼 광범위한 언어 지원을 통해 다국어 처리 능력을 강화하였다.\n",
            "\n",
            "성능 비교 결과, Qwen3-32B 모델은 대부분의 언어 계통에서 최고 점수를 기록하며 기존 Qwen2.5 및 Gemma-3 모델을 능가하는 성능을 보였다. ‘Thinking’ 모드에서 특히 높은 점수를 나타내며, 이는 심층적 추론 능력 향상에 기인한다. 14B와 32B 크기 모델 모두에서 Qwen3가 상위권 성능을 유지하며, 다양한 크기에서도 안정적인 다국어 이해력을 입증하였다.\n",
            "\n",
            "종합적으로, Qwen3는 광범위한 언어 지원과 우수한 벤치마크 성능을 결합하여 다국어 대규모 언어 모델의 새로운 기준을 제시한다. 특히 복잡한 언어 계통을 아우르는 다중 언어 처리에서 뛰어난 효율성과 정확도를 달성하였다. 이로써 글로벌 자연어 처리 응용 분야에서의 활용 가능성이 크게 확대되었다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▎ | 46/55 [05:10<00:47,  5.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 45\n",
            "Qwen3 기술 보고서는 다양한 크기와 설정의 Qwen 시리즈 언어 모델들의 성능을 비교 분석한다. 특히 Qwen3-14B 모델은 사고(Thinking) 모드에서 최고 성능을 기록하며, Qwen2.5-14B-Instruct 및 이전 세대 모델들을 능가하는 결과를 보였다. 모델 크기가 작아질수록 성능은 감소하지만, Qwen3 시리즈는 동일 크기 내에서 경쟁력 있는 결과를 나타내며, 사고 모드가 비사고 모드보다 전반적으로 우수함을 확인하였다.\n",
            "\n",
            "보고서에서는 또한 Gemma-3와 Qwen2.5 모델들과의 비교를 통해 Qwen3의 기술적 진보를 강조하며, 다양한 문제 유형과 평가 지표에 대한 상세한 성능 수치를 제공한다. 참고문헌에는 최신 연구 및 관련 기술들이 포함되어 있어, Qwen3 개발 배경과 최신 동향을 이해하는 데 기여한다. 전반적으로 Qwen3는 대규모 언어 모델 분야에서 혁신적인 접근을 통해 성능과 활용 가능성을 크게 향상시킨 결과로 평가된다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 47/55 [05:18<00:47,  5.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 46\n",
            "Qwen 기술 보고서는 대규모 언어 모델의 설계와 응용에 관한 최신 연구를 종합적으로 다루고 있다. 보고서는 모델 아키텍처, 학습 방법, 그리고 성능 평가에 초점을 맞추며, 특히 장기 문맥 처리와 다중 언어 지원 등 현대 언어 모델이 직면한 주요 과제를 해결하는 방안을 제시한다. 또한, Qwen 모델은 코드 생성, 자연어 이해, 대화 등 다양한 태스크에서 뛰어난 성능을 보여, 실용적 활용 가능성을 확대하고 있다. 보고서에는 Qwen2.5-VL과 같은 멀티모달 확장 모델에 대한 기술적 설명도 포함되어 있어, 텍스트와 비주얼 데이터를 통합하는 새로운 연구 동향을 반영한다. 전반적으로 Qwen 시리즈는 최첨단 AI 연구를 실무에 적용하는 데 중요한 참고 자료로 평가된다.\n",
            "\n",
            "보고서는 다양한 벤치마크와 평가 데이터셋을 활용해 모델의 능력을 체계적으로 검증했다. 예를 들어, 122개 언어 변종을 포함하는 Belebele 벤치마크와 같은 다국어 읽기 이해 테스트를 통해 모델의 범용성을 입증했다. 또한, 코드 생성 분야에서는 MultiPL-E 벤치마크를 통한 평가로 모델의 프로그래밍 능력을 검증하여, 개발자 지원 도구로서의 발전 가능성을 탐색했다. 보고서에 소개된 최신 연구들은 훈련 없이도 긴 문맥을 처리하는 기술과 함께, 소수 샘플 학습(few-shot learning) 능력을 강화하는 데 초점을 맞추고 있다. 이러한 기술적 진보는 대규모 언어 모델의 확장성과 적응성을 크게 향상시킨다.\n",
            "\n",
            "마지막으로, Qwen 보고서는 Anthropic의 Claude 3.7 Sonnet, OpenAI의 GPT 시리즈 등 주요 경쟁 모델과의 비교를 통해 자사의 기술 우수성을 강조한다. 보고서 작성진은 모델의 효율성과 정확도를 높이기 위한 다양한 최적화 기법을 적용했으며, 이는 실제 응용 환경에서의 실행 속도와 자원 활용도 향상으로 이어졌다. 향후 연구 방향으로는 멀티모달 통합, 장기 문맥 관리, 그리고 다국어 처리 능력의 지속적인 개선이 제안되었다. 이를 통해 Qwen 시리즈는 AI 언어 모델 분야에서 혁신을 주도하며, 다양한 산업 분야에 걸쳐 광범위하게 활용될 전망이다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 48/55 [05:25<00:44,  6.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 47\n",
            "Qwen3 기술 보고서는 최신 인공지능 및 거대 언어 모델(LLM) 분야에서의 주요 연구와 발전 동향을 체계적으로 정리하고 있다. 특히, 코드 생성과 수학 문제 해결에 특화된 신경망 모델들의 성능 평가 방법론과 최첨단 모델들의 구조적 혁신에 대해 집중적으로 다룬다. MultiPL-E와 같은 다중 프로그래밍 언어 벤치마크, 그리고 DeepSeekMoE의 전문가 특화 혼합 전문가 모델(MoE) 접근법을 통해 모델 확장성과 전문성 향상에 대한 실질적인 성과를 제시한다. 또한, Gemini 2.5와 같은 대형 비전 트랜스포머 모델의 사례를 통해 모델 규모 확장과 성능 최적화의 최신 트렌드를 소개한다. 이 보고서는 LLM의 평가, 학습, 그리고 적용 분야에서의 기술 발전을 종합적으로 이해하는 데 중요한 자료로 활용된다.\n",
            "\n",
            "보고서는 코드 생성 모델의 평가에 대한 Mark Chen 등의 연구와, 수학 문제 해결을 위한 검증자 훈련 방식(Cobbe et al.)을 통해 LLM의 문제 해결 능력 강화 방식을 분석한다. 또한, DeepSeekMoE(2024) 연구를 통해 전문가 특화 모델 설계가 모델의 효율성과 정확도를 어떻게 극대화하는지를 설명한다. 대규모 모델 학습과 확장성 측면에서는 ICML 2023에서 발표된 220억 파라미터 규모의 비전 트랜스포머(Dehghani et al.) 연구가 중요한 참고사례로 언급된다. 구글 딥마인드의 Gemini 2.5(2025) 모델은 최신 대형 AI 시스템 개발의 사례로서, 모델의 사고 및 업데이트 능력 개선에 중점을 둔다. 전반적으로 이 보고서는 다양한 분야에서 LLM의 다각적 평가와 실용성 증대에 관한 중요한 연구 성과들을 종합하여 제시한다.\n",
            "\n",
            "마지막으로, SuperGPQA와 같은 대규모 LLM 평가 프레임워크는 285개 대학원 학문 분야를 아우르는 광범위한 평가를 가능하게 하여, LLM의 학문적 적용 가능성과 전반적인 성능 향상을 지원한다. 이와 함께, 언어 모델의 구조적 혁신으로 게이트 합성곱 네트워크(Dauphin et al., 2017)와 같은 새로운 아키텍처가 소개된다. 보고서는 인공지능 연구자들이 모델 성능 개선과 확장성 확보를 위한 전략을 수립하는 데 유용한 최신 연구 동향과 기술적 인사이트를 제공한다. Qwen3 기술 보고서는 특히 코드 생성, 문제 해결, 다중 언어 처리 및 대규모 모델 훈련과 평가에 관심 있는 연구자 및 개발자들에게 중요한 참고자료로 자리매김한다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 49/55 [05:33<00:41,  6.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 48\n",
            "Qwen3 기술 보고서는 대규모 비전 트랜스포머 모델과 거대 언어 모델(LLM)의 확장 및 평가에 관한 최신 연구들을 종합적으로 다룬다. 특히, 220억 파라미터 규모의 비전 트랜스포머 모델 확장(ICML 2023)과 285개 대학원 학문 분야에 걸친 LLM 평가(SuperGPQA, 2025) 연구가 핵심적으로 소개된다. 또한 Llama 3 모델 군의 개발과 다양한 도메인에 대한 일반화 추정 방법(DoGE, 2023), 그리고 MMLU 벤치마크의 현황(2024)에 대한 분석도 포함된다. 이와 함께 코드 이해 및 실행 능력을 평가하는 CRUXEval 벤치마크(2024)와 강화학습을 통한 추론 능력 강화 연구(DeepSeek-R1)도 주요 주제로 다뤄진다. 연구들은 대규모 모델의 성능 향상과 평가 체계 발전에 기여하며, 멀티모달 및 다분야 AI 시스템 개발에 중요한 기반을 제공한다.\n",
            "\n",
            "보고서는 대규모 파라미터 기반 트랜스포머 모델이 컴퓨터 비전과 자연어 처리 분야 모두에서 성능을 크게 향상시키는 것을 강조한다. 특히, 비전 트랜스포머의 스케일 업과 LLM의 광범위한 학문 분야 적용은 AI 모델의 범용성과 실용성을 확장한다. 또한, 도메인 가중치 재조정 기법(DoGE)은 모델의 일반화 능력을 높이는 방법론으로 제안되며, 이는 다양한 데이터 환경에서 모델 적응력을 개선하는 데 기여한다. CRUXEval과 같은 특화된 벤치마크는 코드 이해 및 실행과 같은 고차원적 AI 능력 평가를 가능하게 하며, 이는 AI의 실제 응용 분야 확대에 중대한 영향을 미친다. 이처럼, Qwen3 보고서는 첨단 AI 모델 개발과 평가의 최신 동향을 체계적으로 정리하여 연구자와 개발자에게 유용한 참고자료를 제공한다.\n",
            "\n",
            "마지막으로, 보고서는 AI 모델의 지속적인 성능 개선을 위해 대규모 데이터셋과 복잡한 평가 지표의 중요성을 강조한다. LLM 평가의 다학문적 접근은 모델이 다양한 분야에서 균형 잡힌 성능을 발휘하도록 돕는다. 또한, 강화학습 기반 추론 강화 기법은 AI가 보다 효율적이고 정확한 의사결정을 내리도록 지원한다. 이와 함께, 모델 크기 확장과 효율적 학습 전략의 병행은 향후 AI 연구에서 핵심적인 과제로 제시된다. 종합적으로, Qwen3 기술 보고서는 AI 모델의 확장, 평가, 그리고 응용에 관한 최신 연구 성과를 집대성하여 미래 AI 기술 발전의 방향성을 제시한다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 50/55 [05:40<00:34,  6.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 49\n",
            "Qwen3 기술 보고서는 최신 대형 언어 모델(LLM) 평가와 개선 방향에 관한 다양한 연구들을 포괄적으로 검토한다. 보고서는 MMLU, CRUXEval, DeepSeek-R1, Multi-IF 등 다중 과제 및 다국어, 다중 턴 대화 능력 평가를 위한 벤치마크의 중요성을 강조하며, 이들 벤치마크가 LLM의 추론, 코드 이해, 실행 능력을 진단하는 데 핵심적임을 소개한다. 또한, RULER, C-Eval, LiveCodeBench와 같은 세부 평가 도구를 통해 긴 문맥 처리 능력과 중국어 및 코드 관련 성능 측정 방법론도 심층 분석한다.\n",
            "\n",
            "보고서는 Qwen2.5-Coder와 Tülü 3 같은 최신 모델들의 기술적 진보와 평가 결과를 바탕으로, 모델 학습 구조 및 사전 훈련 후 미세 조정 전략이 성능 향상에 미치는 영향을 탐구한다. 특히, Transformer 구조의 효율성을 개선한 Pre-RMSNorm, Pre-CRMSNorm 등의 최신 기술과 강화 학습을 통한 추론 능력 증진 방법론도 상세히 다룬다. 이와 함께, 고품질 벤치마크 구축을 위한 데이터 수집 및 정제 과정의 중요성도 강조하여, 평가 신뢰도와 공정성 확보에 기여하는 연구 동향을 제시한다.\n",
            "\n",
            "종합적으로, Qwen3 보고서는 대형 언어 모델의 평가 및 개선을 위한 최신 연구 동향과 기술적 진보를 체계적으로 정리하여, 연구자 및 개발자들이 모델의 실제 활용 가능성과 한계를 명확히 인식할 수 있도록 돕는다. 이를 통해, 향후 LLM의 다방면 능력 강화와 공정하고 신뢰성 높은 평가 체계 구축에 기여할 방향성을 제시한다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 51/55 [05:49<00:29,  7.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 50\n",
            "Qwen3 기술 보고서는 개방형 대형 언어 모델(LLM)의 사후 학습(post-training) 단계에서 최첨단 성능을 달성하기 위한 다양한 접근법과 연구 동향을 종합적으로 다룬다. 모델의 논리적 추론 능력 한계, 다중 작업 이해, 코드 생성 정확성 평가, 그리고 다국어 및 다중 모달 처리 능력 향상에 관한 최신 연구들을 포함한다. 또한, 데이터 혼합 기법(RegMix)과 효율적인 컨텍스트 창 확장(YaRN) 같은 혁신적인 학습 기법도 소개하며, 모델의 훈련과 평가를 위한 새로운 벤치마크(Arena-Hard, AlignBench) 개발 사례도 설명한다. 이러한 연구들은 LLM의 성능과 적용 범위를 넓히고, 실제 응용에서의 신뢰성과 효율성을 높이는 데 초점을 맞춘다. 전반적으로 Qwen3 보고서는 LLM 기술 발전을 위한 다각도의 최신 연구 결과와 실용적 도전 과제를 체계적으로 정리한 중요한 자료이다.\n",
            "\n",
            "보고서에 인용된 주요 연구들은 LLM의 추론 능력 평가(ZebraLogic), 단계적 검증 기법(Let’s verify step by step), 고품질 벤치마크 구축(Arena-Hard, BenchBuilder), 그리고 코드 생성 정확성 평가 등 다양한 분야를 포괄한다. 또한, Meta-AI의 Llama 4 다중 모달 모델과 OpenAI의 GPT-4o, o3 및 o4-mini 모델 등 최첨단 상용 모델들과의 비교 연구도 포함되어 있다. 이들 연구는 LLM의 다중 언어 처리, 멀티태스킹, 논리적 추론, 창의적 글쓰기 능력 강화에 중점을 둔 최신 발전 동향을 반영한다. 다양한 데이터 소스와 학습 기법을 통합해 모델의 범용성과 신뢰도를 제고하려는 노력이 돋보인다.\n",
            "\n",
            "결론적으로 Qwen3 기술 보고서는 LLM의 학습 방법론, 평가 기준, 그리고 실제 적용 가능성을 한층 진전시키기 위한 연구 커뮤니티의 최신 성과를 집대성한 문서이다. 이는 차세대 AI 시스템 개발에 필요한 방향성과 기술적 토대를 제공하며, 대형 언어 모델의 한계 극복과 성능 향상을 위한 실질적 가이드라인 역할을 한다. 연구자 및 개발자는 이 보고서를 통해 LLM의 미래 지향적 연구 과제와 혁신적 접근법을 폭넓게 이해할 수 있다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▍| 52/55 [05:56<00:21,  7.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 51\n",
            "Qwen 시리즈는 대규모 언어 모델(LLM) 분야에서 최신 기술력을 집약한 모델로, 특히 QwQ-32B 버전에서 강화학습을 적극 도입하여 성능과 응답 품질을 크게 향상시켰다. 해당 모델들은 광범위한 멀티태스크 학습과 효율적인 컨텍스트 윈도우 확장 기술(YaRN) 등을 활용하여 다양한 자연어 이해 및 생성 과제에서 우수한 결과를 보인다. 또한, CodeElo 벤치마크를 통해 인간 수준의 코드 생성 능력을 평가하며, 전문화된 전문가 혼합(MoE) 모델을 위한 부하 분산 손실 함수 연구도 진행 중이다.\n",
            "\n",
            "Qwen 모델 개발팀은 특히 미지의 영역에 대한 깊은 성찰과 혁신적 접근을 강조하며, 이는 QwQ-32B의 강화학습 기반 강화에 잘 반영되어 있다. 멀티링구얼 환경과 지역 지식 포함(INCLUDE) 평가를 통해 다국어 및 문화적 맥락 이해 측면에서도 탁월한 성과를 내고 있다. 수학적 추론 한계를 극복하기 위한 DeepSeekMath 연구와 희귀 단어 번역을 위한 서브워드 단위 신경망 기법 등도 Qwen 연구와 연계되어, 언어 모델의 전방위적 성능 개선에 기여한다.\n",
            "\n",
            "종합적으로 Qwen 시리즈는 효율적 학습, 강화학습 적용, 멀티태스크 및 코드 생성 능력, 그리고 지역 및 수학 지식 포함 등 다각적 혁신을 통해 2025년 언어 모델 기술 발전을 선도하고 있다. 이는 OpenAI의 O3, O4-mini 모델 및 다양한 최신 연구들과 함께 인공지능 언어처리 분야의 중요한 이정표를 제시하며, 고도화된 자연어 처리 및 생성 능력 확보에 크게 이바지하고 있다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▋| 53/55 [06:02<00:14,  7.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 52\n",
            "Qwen3 기술 보고서는 다국어 이해 및 수학적 추론 능력을 포함한 최신 인공지능 언어 모델의 성능 평가와 개선 방향을 종합적으로 다룬다. 보고서는 다양한 다국어 벤치마크와 체인 오브 사고(chain-of-thought) 추론 기법을 통해 모델의 복잡한 문제 해결 능력을 검증하며, 희귀 단어 번역을 위한 서브워드 단위 활용과 회전 위치 임베딩(rotary position embedding) 등의 기술적 혁신을 소개한다. 또한, 수학적 추론 능력을 다국어 환경에서 평가하는 PolyMath와 DeepSeekMath 같은 첨단 평가 도구를 활용해 모델의 한계와 가능성을 심층 분석한다.\n",
            "\n",
            "보고서에서 강조하는 주요 이슈는 모델의 일반화 능력과 지역별 지식의 통합, 그리고 테스트 시 스케일링(test-time scaling)의 언어적 일반화 가능성이다. 이러한 연구들은 빅벤치(BIG-Bench)와 MMLU-Pro 같은 강력하고 도전적인 다중 작업 언어 이해 벤치마크를 통해 입증되며, 모델이 다국어 및 수학 문제 해결에서 보여주는 성능 향상을 목표로 한다. 또한, LiveBench와 WritingBench 같은 새로운 벤치마크는 오염 없는 데이터 환경에서 생성형 쓰기 능력을 포함한 다양한 언어 생성 태스크를 평가하는 데 기여한다.\n",
            "\n",
            "종합적으로 Qwen3 보고서는 인공지능 언어 모델이 다언어 환경과 수학적 복잡성을 효과적으로 다룰 수 있도록 설계되고 평가되는 최신 연구 동향을 반영한다. 이는 자연어 처리, 기계 번역, 수학적 추론, 그리고 생성형 AI 분야에서의 기술 발전과 실용적 적용 가능성을 높이는 데 중요한 역할을 한다. 앞으로도 이와 같은 다각도의 평가와 혁신적 기법 도입이 모델의 정확도와 범용성을 강화하는 데 필수적임을 시사한다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 54/55 [06:07<00:06,  6.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 53\n",
            "Qwen3 기술 보고서는 대규모 언어 모델(LLM)의 최신 발전과 성능 향상에 관한 내용을 다루고 있다. 보고서는 다양한 벤치마크와 비교 평가를 통해 Qwen 시리즈 모델의 우수성을 입증하며, 특히 Qwen2, Qwen2.5, 그리고 Qwen2.5-Math 모델들의 기술적 세부사항과 성능 개선 전략을 상세히 설명한다. 이들 모델은 데이터 혼합 최적화, 장기 문맥 처리 능력 향상, 그리고 수학적 문제 해결 능력 강화에 중점을 두고 개발되었다고 밝히고 있다.\n",
            "\n",
            "또한, 보고서는 LiveBench, WritingBench, Berkeley function calling leaderboard 등 최신 LLM 평가 벤치마크와의 연계 연구를 통해 모델의 실제 적용 가능성과 신뢰성을 높이고자 하는 시도를 소개한다. 특히 오염 없는 데이터 셋을 활용한 엄격한 평가와 생성형 글쓰기 능력 검증이 중요한 평가 지표로 다뤄진다. 이와 같이 Qwen3는 다면적 평가를 기반으로 한 실용적이고 확장성 있는 LLM 개발을 목표로 한다.\n",
            "\n",
            "마지막으로, 보고서는 향후 연구 방향으로 추론 에이전트 시대에 적합한 모델 설계, 데이터 전처리 및 혼합 전략의 지속적 최적화, 그리고 전문 영역(예: 수학) 모델의 자기 개선 메커니즘 개발을 제시한다. 이를 통해 Qwen 시리즈는 인공지능 언어 모델 분야에서 경쟁력을 유지하며, 다양한 응용 분야에서의 활용 가능성을 넓히려는 비전을 공유한다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55/55 [06:16<00:00,  6.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 54\n",
            "Qwen3는 최신 대규모 언어 모델(LLM)로, Qwen2.5 시리즈의 발전을 기반으로 설계되었다. 이 모델은 텍스트와 코드, 멀티모달 데이터를 통합하여 다양한 자연어 처리 및 생성 작업에서 뛰어난 성능을 보인다. 특히, 수학적 추론 능력 강화를 위해 Qwen2.5-Math에서 도입된 자기 개선(self-improvement) 기법을 확장 적용하였으며, 이를 통해 전문적인 수학 모델로서의 역량을 크게 향상시켰다. 또한, 다국어 및 다중 작업 평가를 위해 P-MMEval 벤치마크를 활용해 다양한 언어와 작업 간의 일관된 성능 평가를 수행하였다. 이와 함께, Instruction-following 및 논리 추론 능력 평가를 위한 AutoLogi 등 자동화된 평가 도구도 병행하여 개발, 모델의 실용성과 신뢰성을 높였다.\n",
            "\n",
            "Qwen3 기술 보고서는 학습 데이터, 모델 아키텍처, 훈련 방법론, 그리고 평가 결과를 상세히 다룬다. 모델은 대규모 멀티태스크 학습과 고도화된 지시어 이해 능력을 바탕으로 복잡한 지시를 정확히 수행하며, 다양한 도메인에 걸쳐 강력한 일반화 능력을 보인다. 특히, 수학 및 논리 퍼즐과 같은 고난도 문제에서 기존 모델 대비 우수한 성능을 기록하였으며, 이는 모델의 자기 개선 및 멀티모달 학습 전략이 실질적 효과를 발휘했음을 의미한다. 평가 결과는 Qwen3가 현재 공개된 주요 LLM들과 비교해도 경쟁력 있는 수준임을 입증한다. 아울러, 지속적인 연구를 통해 모델의 확장성과 효율성 향상 방안도 제시하고 있다.\n",
            "\n",
            "종합적으로, Qwen3는 대규모 언어 모델 연구에 있어 자기 개선, 멀티태스크 및 멀티모달 학습, 그리고 일관된 평가 체계 구축 등 다각도의 혁신을 포함한다. 이는 자연어 이해, 생성, 수학적 추론, 그리고 다국어 처리 능력에서 새로운 기준을 제시하며, 차세대 인공지능 응용에 중요한 기반 기술로 자리매김하고 있다. 앞으로도 Qwen 시리즈의 발전을 통해 더욱 정교하고 신뢰성 높은 AI 모델 개발이 기대된다.\n",
            "===========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Map 과정 : 각 문서에 대해 요약을 생성합니다.\n",
        "from tqdm import tqdm\n",
        "\n",
        "map_prompt = ChatPromptTemplate([\n",
        "    ('system', '''당신은 인공지능과 거대 언어 모델의 전문가입니다.\n",
        "Qwen3 Techinal Report의 내용을 읽고 요약하세요.\n",
        "요약은 1-3개의 문단과 문단별 5개의 문장으로 작성하세요.\n",
        "'''),\n",
        "    ('user', '''{text}\n",
        "---\n",
        "Summary:\n",
        "''')])\n",
        "\n",
        "raw_summaries = []\n",
        "\n",
        "map_chain  = map_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "for i in tqdm(range(len(token_chunks))):\n",
        "    response = map_chain.invoke(token_chunks[i].page_content)\n",
        "    raw_summaries.append(response)\n",
        "    print('')\n",
        "    print('#',i)\n",
        "    print(response)\n",
        "    print('===========================')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-33gBwzQ-4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "d2df87f6-df78-4634-c09f-311bc4fdc710"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Qwen3는 Qwen 시리즈의 최신 대형 언어 모델(LLM)로, 0.6억에서 2350억 파라미터 규모의 밀집형(Dense)과 전문가 혼합형(MoE) 모델을 포함한다. 이 모델은 ‘사고 모드’와 ‘비사고 모드’를 하나의 프레임워크에서 동적으로 전환할 수 있으며, 계산 자원을 작업 복잡도에 맞게 조절하는 ‘사고 예산’ 메커니즘을 도입해 성능과 지연 시간의 균형을 최적화한다. 119개 언어와 방언, 36조 토큰에 달하는 대규모 다국어 데이터를 활용해 사전학습되었으며, 단계별 학습 전략과 강화학습, 증류 기법을 통해 추론 능력과 다국어 이해력을 크게 향상시켰다. Apache 2.0 라이선스로 공개되어 연구 및 산업계에서 협업과 재현성을 지원한다.\\n\\n아키텍처 측면에서 Qwen3는 최신 Transformer 기법을 적용하고, MoE 모델은 128명의 전문가를 활용해 활성화된 파라미터 수를 줄이면서도 고성능을 구현한다. 대표 모델인 Qwen3-235B-A22B는 2350억 파라미터 중 220억 활성화 파라미터를 사용해 수학, 코딩, 다국어 등 다양한 분야에서 최첨단 성능을 보인다. 토크나이저는 바이트 수준의 Byte-Pair Encoding을 기반으로 약 15만 개 어휘를 포함하며, 최대 128K 토큰의 긴 문맥도 처리 가능해 대규모 문서 이해에 강점을 가진다.\\n\\n사후학습에서는 사고 모드와 비사고 모드의 융합(Thinking Mode Fusion)과 강한 모델에서 약한 모델로 지식을 전이하는 강한-약한 증류(Strong-to-Weak Distillation) 파이프라인을 도입했다. 이를 통해 경량 모델도 고성능과 사고 모드 전환 능력을 갖추면서 훈련 비용과 시간을 크게 줄였다. 강화학습과 감독학습이 결합된 학습 전략은 다양한 작업에서 일관된 성능 향상을 가져왔으며, 사고 예산 조절 기능은 복잡한 문제에서 효율적이고 유연한 추론을 가능하게 한다.\\n\\n성능 평가에서는 Qwen3 시리즈가 다양한 일반, 수학, 코딩, 에이전트, 다국어, 정렬(Alignment) 벤치마크에서 뛰어난 성과를 기록했다. 특히 Qwen3-235B-A22B는 공개 소스 모델 중 최고 수준의 성능을 보이며, OpenAI GPT-4o, Gemini2.5-Pro 등 폐쇄형 최상위 모델과도 대등하거나 경쟁력 있는 결과를 냈다. 32B, 30B, 14B, 8B, 4B, 1.7B, 0.6B 등 다양한 규모 모델이 크기 대비 효율성을 극대화하며 일관된 성능을 보여 실무 및 연구 현장에서 폭넓게 활용 가능함을 입증했다.\\n\\n다국어 평가에서는 80여 개 이상의 언어를 포괄하는 Belebele 벤치마크 등을 통해 광범위한 언어 지원과 우수한 자연어 이해 능력을 입증했다. 사고 모드 활성화 시 성능이 뛰어나며, 다양한 언어와 과제에서 Gemini, Gemma, Qwen2.5 등 경쟁 모델을 능가하거나 대등한 성과를 보였다. 긴 문맥 처리 능력도 개선되어 4K부터 128K 토큰까지 안정적인 정보 이해가 가능하며, 향후 장기 문맥과 다중 모달 통합 연구가 기대된다. 전반적으로 Qwen3는 다국어, 추론, 생성, 코딩 분야에서 최신 AI 연구를 선도하는 혁신적이고 실용적인 대형 언어 모델임을 입증했다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Reduce 과정 : 각 문서의 요약을 하나로 합칩니다.\n",
        "reduce_prompt = ChatPromptTemplate([\n",
        "    ('system', '''당신은 인공지능과 거대 언어 모델의 전문가입니다.\n",
        "Qwen3 Techinal Report에 대한 요약문의 리스트가 주어집니다.\n",
        "이를 읽고, 전체 주제를 포함하는 최종 요약을 작성하세요.\n",
        "요약은 5개의 문단과 문단별 4-8개의 문장으로 작성하세요.\n",
        "'''),\n",
        "    ('user', '''{text}\n",
        "---\n",
        "Summary:\n",
        "''')])\n",
        "\n",
        "reduce_chain = reduce_prompt | llm | StrOutputParser()\n",
        "\n",
        "summary = reduce_chain.invoke('\\n---\\n'.join(raw_summaries))\n",
        "\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33wttmTRB-Ce"
      },
      "source": [
        "## 3. Refine\n",
        "Refine은 청크를 순서대로 참고하며, 매 시점 요약문을 작성합니다.   \n",
        "요약문과 새로운 청크를 비교하여, 요약문을 업데이트합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTsO2OvjB-Ce"
      },
      "outputs": [],
      "source": [
        "# 첫 번째 청크 요약하기\n",
        "\n",
        "first_summarize_prompt = ChatPromptTemplate([\n",
        "    ('system', '''당신은 인공지능과 거대 언어 모델의 전문가입니다.\n",
        "Qwen3 Techinal Report의 내용을 읽고 요약하세요.\n",
        "요약은 1-3개의 문단과 문단별 5개의 문장으로 작성하세요.\n",
        "'''),\n",
        "    ('user', '''{text}\n",
        "---\n",
        "Summary:\n",
        "''')])\n",
        "\n",
        "X = first_summarize_prompt.format_messages(text=token_chunks[0].page_content)\n",
        "\n",
        "summary = llm.invoke(X).content\n",
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6LeNm-FB-Ce"
      },
      "outputs": [],
      "source": [
        "# 두 번째 청크부터, 계속 추가하며 요약하기\n",
        "refine_prompt = ChatPromptTemplate([\n",
        "    ('system', '''당신은 인공지능과 거대 언어 모델의 전문가입니다.\n",
        "Qwen3 Techinal Report의 현재 시점까지의 요약이 주어집니다.\n",
        "이를 읽고, 새롭게 주어지는 내용과 비교하여 요약을 보완하세요.\n",
        "요약은 5개의 문단과 문단별 4-8개의 문장으로 작성하세요.\n",
        "'''),\n",
        "    ('user', '''현재 시점까지의 요약: {previous_summary}\n",
        "---\n",
        "새로운 내용: {new_text}''')])\n",
        "\n",
        "\n",
        "refine_chain = refine_prompt | llm | StrOutputParser()\n",
        "\n",
        "for i in tqdm(range(1, len(token_chunks))):\n",
        "    summary = refine_chain.invoke(\n",
        "        {'previous_summary':summary,\n",
        "         'new_text':token_chunks[i].page_content})\n",
        "    print('')\n",
        "    print(summary)\n",
        "    print('=======================')\n",
        "\n",
        "# 길이를 지정하지 않으면 오래 걸릴 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvvK8mP_B-Ce"
      },
      "outputs": [],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxZDzVPrVdwb"
      },
      "source": [
        "## [실습] 임의의 PDF 다운로드하여 요약하기\n",
        "\n",
        "arxiv 등의 페이지에서 PDF를 다운로드하여 업로드하고,   \n",
        "Stuff/Map-Reduce/Refine 등의 방법을 이용해 전체 PDF를 요약하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOKzQ8n5Vdwb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKjsAmSnVdwb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yQwijcsVdwb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3y-UXsTVVdwb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}